code_text,op_kind,dtype,layout,math_mode,conv_role,correct,pred,proba,code_len
"#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>
#include <vector>

// CUDA kernel with manual loop unrolling applied on grid-stride loop
__global__ void post_process_kernel(
    float* output,
    const float* bias,
    const int total_size,
    const int channels,
    const int height,
    const int width,
    const float scaling_factor
) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    int stride = blockDim.x * gridDim.x;

    // Process multiple elements per thread (unroll factor of 4)
    // Adjust the loop bound to ensure all iterations in the unrolled section are valid
    int max_i = total_size - (stride * 4);
    
    // Main loop with full unrolling
    for (int i = idx; i < max_i; i += stride * 4) {
        #pragma unroll
        for (int j = 0; j < 4; j++) {
            int index = i + j * stride;
            // Identify the channel for bias addition
            int c = (index / (height * width)) % channels;

            // Add bias
            float val = output[index] + bias[c];

            // First clamp
            val = fminf(fmaxf(val, 0.0f), 1.0f);

            // Scale
            val = val * scaling_factor;

            // Second clamp
            val = fminf(fmaxf(val, 0.0f), 1.0f);

            // Divide by scaling factor
            val = val / scaling_factor;

            output[index] = val;
        }
    }
    
    // Handle remaining elements
    for (int i = idx + max_i; i < total_size; i += stride) {
        int c = (i / (height * width)) % channels;
        
        float val = output[i] + bias[c];
        val = fminf(fmaxf(val, 0.0f), 1.0f);
        val = val * scaling_factor;
        val = fminf(fmaxf(val, 0.0f), 1.0f);
        val = val / scaling_factor;
        
        output[i] = val;
    }
    }
}

// The forward function executes conv_transpose2d then launches the CUDA kernel
// for post-processing with loop unrolling improvements.

torch::Tensor forward(
    torch::Tensor x,
    int64_t stride,
    int64_t padding, 
    int64_t output_padding,
    float scaling_factor,
    torch::Tensor conv_transpose,
    torch::Tensor conv_transpose_bias,
    torch::Tensor bias
) {
    // Execute conv_transpose2d using PyTorch's built-in function
    auto output = torch::conv_transpose2d(
        x, conv_transpose, conv_transpose_bias,
        stride, padding, output_padding
    );

    const int batch_size = output.size(0);
    const int channels = output.size(1);
    const int height = output.size(2);
    const int width = output.size(3);
    const int total_size = batch_size * channels * height * width;

    // Launch configuration: using 256 threads per block
    int threads = 256;
    int blocks = (total_size + threads - 1) / threads;

    // Launch the post-processing kernel
    post_process_kernel<<<blocks, threads>>>(
        output.data_ptr<float>(),
        bias.data_ptr<float>(),
        total_size,
        channels,
        height,
        width,
        scaling_factor
    );

    return output;
}

PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
    m.def(""forward"", &forward, ""Transposed conv with post-processing and loop unrolling (CUDA)"");
}
",other,,,,,0,1,0.9229225,3167
"#include <torch/extension.h>
#include <ATen/ATen.h>
#include <cuda.h>
#include <cuda_runtime.h>
#include <vector>

// Device function to compute the minimum across the channel dimension
__device__ float compute_min(const float* input, int C, int H, int W, int n, int h, int w) {
    float min_val = __ldg(&input[((n * C + 0) * H + h) * W + w]);
    #pragma unroll 4
    for (int c = 1; c < C; c++) {
        float val = __ldg(&input[((n * C + c) * H + h) * W + w]);
        min_val = (val < min_val) ? val : min_val;
    }
    return min_val;
}

// Device function to compute the sum along the height dimension
__device__ float compute_sum(const float* input, int H, int W, int n, int w) {
    float sum_val = 0.0f;
    #pragma unroll 4
    for (int h = 0; h < H; h++) {
        sum_val += __ldg(&input[((n * 1 + 0) * H + h) * W + w]);
    }
    return sum_val;
}

// Kernel to compute the minimum across the channel dimension using a flattened grid-stride loop
__global__ void min_dim1_kernel_modular(const float* __restrict__ input,
                                         float* __restrict__ output,
                                         int N, int C, int H, int W) {
    int total = N * H * W;
    for (int idx = blockIdx.x * blockDim.x + threadIdx.x;
         idx < total;
         idx += gridDim.x * blockDim.x) {
        int w = idx % W;
        int nh = idx / W;
        int h = nh % H;
        int n = nh / H;
        output[idx] = compute_min(input, C, H, W, n, h, w);
    }
}

// Kernel to compute the sum along the height dimension using a flattened grid-stride loop
__global__ void sum_dim2_kernel_modular(const float* __restrict__ input,
                                         float* __restrict__ output,
                                         int N, int H, int W) {
    int total = N * W;
    for (int idx = blockIdx.x * blockDim.x + threadIdx.x;
         idx < total;
         idx += gridDim.x * blockDim.x) {
        int w = idx % W;
        int n = idx / W;
        output[idx] = compute_sum(input, H, W, n, w);
    }
}

// Forward function that performs conv_transpose2d, the min and sum reductions, GELU activation and bias addition
torch::Tensor forward(
    torch::Tensor x,
    int64_t stride,
    int64_t padding,
    int64_t output_padding,
    torch::Tensor conv_transpose,
    torch::Tensor conv_transpose_bias,
    torch::Tensor bias
) {
    x = x.contiguous();
    x = at::conv_transpose2d(
        x,
        conv_transpose,
        conv_transpose_bias,
        {stride, stride},
        {padding, padding},
        {output_padding, output_padding},
        1,   // groups
        {1, 1} // dilation
    );

    auto sizes = x.sizes();
    int N = sizes[0];
    int C = sizes[1];
    int H = sizes[2];
    int W = sizes[3];

    auto options = torch::TensorOptions().dtype(x.dtype()).device(x.device());
    auto x_min = torch::zeros({N, 1, H, W}, options);

    int total_min = N * H * W;
    // Optimize thread block size to be multiple of warp size (32)
    int threads = 256;  // 8 warps per block
    int max_blocks = 0;
    cudaGetDeviceProperties(&prop, 0);
    int max_threads_per_sm = prop.maxThreadsPerMultiProcessor;
    int num_sms = prop.multiProcessorCount;
    // Calculate optimal number of blocks
    int blocks = std::min((total_min + threads - 1) / threads,
                         2 * num_sms * (max_threads_per_sm / threads));
    min_dim1_kernel_modular<<<blocks, threads>>>(
        x.data_ptr<float>(),
        x_min.data_ptr<float>(),
        N, C, H, W
    );

    auto x_sum = torch::zeros({N, 1, 1, W}, options);

    int total_sum = N * W;
    blocks = (total_sum + threads - 1) / threads;
    sum_dim2_kernel_modular<<<blocks, threads>>>(
        x_min.data_ptr<float>(),
        x_sum.data_ptr<float>(),
        N, H, W
    );

    x_sum = at::gelu(x_sum);
    x_sum = x_sum + bias;

    return x_sum;
}

PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
    m.def(""forward"", &forward, ""Modular 36_ConvTranspose2d_Min_Sum_GELU_Add kernel"");
}
",other,,,,,0,1,0.9162436,4006
"#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>
#include <cmath>

// This kernel is specialized for 36 features. By fully unrolling the inner loops, we ensure that each thread follows an identical control flow, 
// thereby minimizing warp divergence. This uniformity can help improve performance on the NVIDIA H100 GPU while guaranteeing correct results.

template <typename scalar_t>
__global__ void rms_norm_kernel_no_div(
    const scalar_t* __restrict__ input,
    scalar_t* __restrict__ output,
    const int batch_size,
    const int numel_per_batch,
    const float eps
) {
    // Since this is 36_RMSNorm, we assume num_features is 36
    constexpr int num_features = 36;
    int tid = blockIdx.x * blockDim.x + threadIdx.x;
    int total_threads = gridDim.x * blockDim.x;

    // Each thread processes multiple elements in a grid-stride loop
    for (int index = tid; index < batch_size * numel_per_batch; index += total_threads) {
        int batch_id = index / numel_per_batch;
        int offset = index % numel_per_batch;
        int base = batch_id * num_features * numel_per_batch;

        // Compute sum of squares with fully unrolled loop to avoid conditional divergence
        scalar_t sumsq = 0;
        #pragma unroll
        for (int feat = 0; feat < num_features; feat++) {
            int idx = base + feat * numel_per_batch + offset;
            scalar_t val = input[idx];
            sumsq += val * val;
        }

        // Calculate RMS using epsilon for numerical stability
        scalar_t rms = sqrt(sumsq / num_features + eps);

        // Normalize all features using the precomputed rms
        #pragma unroll
        for (int feat = 0; feat < num_features; feat++) {
            int idx = base + feat * numel_per_batch + offset;
            output[idx] = input[idx] / rms;
        }
    }
}

// CUDA forward function for RMSNorm with minimized warp divergence

torch::Tensor rms_norm_cuda_forward_no_div(torch::Tensor input, float eps) {
    auto output = torch::empty_like(input);
    const int batch_size = input.size(0);
    
    // Calculate number of elements per batch for dimensions beyond the first two
    int numel_per_batch = 1;
    for (int i = 2; i < input.dim(); i++) {
        numel_per_batch *= input.size(i);
    }

    int total_work = batch_size * numel_per_batch;
    const int threads_per_block = 256;
    const int blocks = (total_work + threads_per_block - 1) / threads_per_block;

    AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.scalar_type(), ""rms_norm_cuda_no_div"", ([&] {
        rms_norm_kernel_no_div<scalar_t><<<blocks, threads_per_block>>>(
            input.data_ptr<scalar_t>(),
            output.data_ptr<scalar_t>(),
            batch_size,
            numel_per_batch,
            eps
        );
    }));

    return output;
}

PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
    m.def(""forward"", &rms_norm_cuda_forward_no_div, ""RMS normalization forward with minimized warp divergence (CUDA)"");
}
",other,,,,,0,1,0.9144786,2990
"#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>

#define CHECK_CUDA(x) TORCH_CHECK(x.is_cuda(), #x "" must be a CUDA tensor"")
#define CHECK_CONTIGUOUS(x) TORCH_CHECK(x.is_contiguous(), #x "" must be contiguous"")
#define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)

// Optimized cumulative sum kernel with manual loop unrolling to reduce loop overhead
__global__ void cumsum_kernel_unroll(const float* __restrict__ input, float* __restrict__ output,
                                       int inner_size, int stride) {
    int outer_idx = blockIdx.x;
    int base = outer_idx * stride * inner_size;

    // Each thread processes multiple inner indices in steps of blockDim.x for better occupancy
    for (int inner_idx = threadIdx.x; inner_idx < inner_size; inner_idx += blockDim.x) {
        float sum = 0.0f;
        const int unroll_factor = 4;
        int limit = stride - (stride % unroll_factor);

        #pragma unroll
        for (int i = 0; i < limit; i += unroll_factor) {
            int idx0 = base + (i + 0) * inner_size + inner_idx;
            sum += __ldg(&input[idx0]);
            output[idx0] = sum;

            int idx1 = base + (i + 1) * inner_size + inner_idx;
            sum += __ldg(&input[idx1]);
            output[idx1] = sum;

            int idx2 = base + (i + 2) * inner_size + inner_idx;
            sum += __ldg(&input[idx2]);
            output[idx2] = sum;

            int idx3 = base + (i + 3) * inner_size + inner_idx;
            sum += __ldg(&input[idx3]);
            output[idx3] = sum;
        }

        // Process remaining iterations if stride is not divisible by unroll_factor
        for (int i = limit; i < stride; i++) {
            int idx = base + i * inner_size + inner_idx;
            sum += __ldg(&input[idx]);
            output[idx] = sum;
        }
    }
}

// Forward function: computes cumulative sum along the specified dimension
torch::Tensor forward(torch::Tensor x, int dim) {
    CHECK_INPUT(x);
    auto output = torch::empty_like(x);
    int ndim = x.dim();
    dim = (dim + ndim) % ndim;

    int outer_size = 1;
    for (int i = 0; i < dim; i++) {
        outer_size *= x.size(i);
    }

    int inner_size = 1;
    for (int i = dim + 1; i < ndim; i++) {
        inner_size *= x.size(i);
    }

    int stride = x.size(dim);

    // Launch the kernel: each block processes one outer index, each thread corresponds to an inner index
    cumsum_kernel_unroll<<<outer_size, inner_size>>>(
        x.data_ptr<float>(),
        output.data_ptr<float>(),
        outer_size,
        inner_size,
        stride
    );

    return output;
}

PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
    m.def(""forward"", &forward, ""Optimized CUDA cumulative sum with manual loop unrolling"");
}
",other,,,,,0,1,0.91201925,2771
"#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>
#include <math.h>

// Device function for performing convolution at a specific output location
// This function computes the convolution result for input pixel location (h, w) in batch b and channel c

template <typename scalar_t>
__device__ __forceinline__ float compute_conv(
    const scalar_t* __restrict__ input,
    const scalar_t* __restrict__ weight,
    const scalar_t* __restrict__ bias,
    int b, int c, int h, int w,
    int in_channels, int kernel_size,
    int in_height, int in_width
) {
    // Load bias using __ldg for read-only cache benefit
    float conv_sum = __ldg(&bias[c]);
    
    // Loop over input channels and kernel spatial dimensions
    for (int ic = 0; ic < in_channels; ic++) {
        #pragma unroll
        for (int kh = 0; kh < kernel_size; kh++) {
            #pragma unroll
            for (int kw = 0; kw < kernel_size; kw++) {
                int in_h = h + kh;
                int in_w = w + kw;
                if (in_h < in_height && in_w < in_width) {
                    int in_idx = b * (in_channels * in_height * in_width)
                              + ic * (in_height * in_width)
                              + in_h * in_width + in_w;
                    int w_idx = c * (in_channels * kernel_size * kernel_size)
                              + ic * (kernel_size * kernel_size)
                              + kh * kernel_size + kw;
                    conv_sum += input[in_idx] * weight[w_idx];
                }
            }
        }
    }
    return conv_sum;
}

// Device function for performing pooling on the convolution output
// It iterates over the pooling window and applies: subtract -> tanh -> subtract operations

template <typename scalar_t>
__device__ __forceinline__ float compute_pool(
    const scalar_t* __restrict__ input,
    const scalar_t* __restrict__ weight,
    const scalar_t* __restrict__ bias,
    int b, int c, int pool_h, int pool_w,
    int pool_size, int out_height, int out_width,
    int in_channels, int kernel_size,
    int in_height, int in_width,
    float subtract1, float subtract2
) {
    int h_start = pool_h * pool_size;
    int w_start = pool_w * pool_size;
    float pool_sum = 0.0f;
    int count = 0;

    // Iterate over the pooling window
    for (int ph = 0; ph < pool_size; ph++) {
        for (int pw = 0; pw < pool_size; pw++) {
            int h = h_start + ph;
            int w = w_start + pw;
            if (h < out_height && w < out_width) {
                float conv_val = compute_conv(input, weight, bias,
                                              b, c, h, w,
                                              in_channels, kernel_size,
                                              in_height, in_width);
                // Apply operations: subtract1 then tanh then subtract2
                conv_val = tanhf(conv_val - subtract1) - subtract2;
                pool_sum += conv_val;
                count++;
            }
        }
    }
    return (count > 0 ? pool_sum / count : 0.0f);
}

// Global kernel: Split into batch and channel parallel execution
template <typename scalar_t>
__global__ void process_kernel(
    const scalar_t* __restrict__ input,
    const scalar_t* __restrict__ weight,
    const scalar_t* __restrict__ bias,
    scalar_t* __restrict__ output,
    int batch_size,
    int in_channels,
    int out_channels,
    int in_height,
    int in_width,
    int out_height,
    int out_width,
    int kernel_size,
    int pool_size,
    int pool_out_h,
    int pool_out_w,
    float subtract1,
    float subtract2
) {
    // Use 2D grid for better parallelism across batches and channels
    int batch_channel_idx = blockIdx.x;
    int spatial_idx = blockIdx.y * blockDim.x + threadIdx.x;
    
    int b = batch_channel_idx / out_channels;
    int c = batch_channel_idx % out_channels;
    
    if (b >= batch_size) return;
    
    int total_spatial = pool_out_h * pool_out_w;
    if (spatial_idx >= total_spatial) return;

    // Decode the linear index into b, c, and pooling output spatial location
    int pw = idx % pool_out_w;
    int ph = (idx / pool_out_w) % pool_out_h;
    int c = (idx / (pool_out_w * pool_out_h)) % out_channels;
    int b = idx / (pool_out_w * pool_out_h * out_channels);

    // Use the modular device function to compute the pooling value
    output[idx] = compute_pool(input, weight, bias,
                               b, c, ph, pw,
                               pool_size, out_height, out_width,
                               in_channels, kernel_size,
                               in_height, in_width,
                               subtract1, subtract2);
}

// Host function: sets up tensor dimensions and launches the CUDA kernel

torch::Tensor forward(
    torch::Tensor input,
    int kernel_size_pool,
    torch::Tensor conv_weight,
    torch::Tensor conv_bias,
    float subtract1_value,
    float subtract2_value
) {
    // Dimensions from input and convolution parameters
    int batch_size = input.size(0);
    int in_channels = input.size(1);
    int in_height = input.size(2);
    int in_width = input.size(3);
    int out_channels = conv_weight.size(0);
    int kernel_size = conv_weight.size(2);

    int out_height = in_height - kernel_size + 1;
    int out_width = in_width - kernel_size + 1;
    int pool_out_h = out_height / kernel_size_pool;
    int pool_out_w = out_width / kernel_size_pool;

    auto output = torch::zeros({batch_size, out_channels, pool_out_h, pool_out_w}, input.options());
    int total_elements = batch_size * out_channels * pool_out_h * pool_out_w;
    int threads = 256;
    int blocks = (total_elements + threads - 1) / threads;

    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), ""process_kernel"", ([&] {
        process_kernel<scalar_t><<<blocks, threads>>>(
            input.data<scalar_t>(),
            conv_weight.data<scalar_t>(),
            conv_bias.data<scalar_t>(),
            output.data<scalar_t>(),
            batch_size,
            in_channels,
            out_channels,
            in_height,
            in_width,
            out_height,
            out_width,
            kernel_size,
            kernel_size_pool,
            pool_out_h,
            pool_out_w,
            subtract1_value,
            subtract2_value
        );
    }));

    return output;
}

PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
    m.def(""forward"", &forward, ""Modular device functions for Conv+sub+tanh+sub+pool"");
}
",conv2d,,,,,0,1,0.9117099,6512
"#include <torch/extension.h>
#include <ATen/ATen.h>
#include <cuda.h>
#include <cuda_runtime.h>

// This kernel computes the mean of each (batch, channel) slice in a divergence-minimized manner
// using warp-level reduction with __shfl_down_sync and a branchless determination of the warp leader.

template <unsigned int blockSize>
__global__ void warp_uniform_reduction_mean_kernel(
    const float* __restrict__ input,
    float* __restrict__ global_accum,
    int H,
    int W,
    int C
) {
    const int num_elements = H * W;
    // Each block processes one (batch, channel) slice. Block index = batch * C + channel.
    int bc = blockIdx.x;
    // Compute offset for the (batch, channel) slice. Assumes y is of shape (N, C, H, W) in contiguous layout.
    int input_offset = bc * num_elements;

    float sum = 0.0f;
    // Accumulate elements with a grid-stride loop over the slice
    for (int i = threadIdx.x; i < num_elements; i += blockDim.x) {
        sum += input[input_offset + i];
    }

    // Perform warp-level reduction using shuffle intrinsics for uniform control flow
    int lane = threadIdx.x & 31;
    sum += __shfl_down_sync(0xffffffff, sum, 16);
    sum += __shfl_down_sync(0xffffffff, sum, 8);
    sum += __shfl_down_sync(0xffffffff, sum, 4);
    sum += __shfl_down_sync(0xffffffff, sum, 2);
    sum += __shfl_down_sync(0xffffffff, sum, 1);

    // Use a branchless flag: all threads compute the same flag; only lane 0 in each warp contributes its sum
    int leader_flag = !lane; // Evaluates to 1 if lane==0, 0 otherwise

    // All threads call atomicAdd uniformly; non-leader threads add 0, avoiding divergent branching
    atomicAdd(global_accum, leader_flag * sum);
}

at::Tensor module_fn(
    at::Tensor x,
    int64_t stride,
    int64_t padding,
    int64_t output_padding,
    at::Tensor conv_transpose,
    at::Tensor conv_transpose_bias,
    double multiplier
) {
    // Apply transposed convolution using PyTorch's native function
    at::Tensor y = at::conv_transpose2d(
        x,
        conv_transpose,
        conv_transpose_bias,
        {stride, stride},
        {padding, padding},
        {output_padding, output_padding},
        1,
        {1, 1}
    );

    // Scale the result by the given multiplier
    y = y * multiplier;

    // Get dimensions of y: (N, C, H, W)
    auto dims = y.sizes();
    int N = dims[0];
    int C = dims[1];
    int H = dims[2];
    int W = dims[3];

    // Allocate a scalar accumulator on the device and initialize to zero
    auto options = torch::TensorOptions().device(y.device()).dtype(y.dtype());
    at::Tensor accum = torch::zeros({1}, options);

    // Launch the kernel with one block per (batch, channel) slice
    constexpr int blockSize = 256;
    int numBlocks = N * C;
    // No shared memory is needed due to warp-level reduction
    warp_uniform_reduction_mean_kernel<blockSize><<<numBlocks, blockSize, 0>>>(
        y.data_ptr<float>(),
        accum.data_ptr<float>(),
        H, W, C
    );

    // Final overall mean: each block computed a slice mean; average these over all (batch, channel) pairs
    accum = accum / static_cast<float>(N * C);
    return accum;
}

PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
    m.def(""forward"", &module_fn, ""Module function"");
}
",other,,,,,0,1,0.9082912,3268
"#include <torch/extension.h>
#include <torch/torch.h>
#include <vector>

// Forward function implementation
torch::Tensor forward(
    torch::Tensor x,
    std::vector<torch::Tensor> gru_weights_ih,
    std::vector<torch::Tensor> gru_weights_hh,
    std::vector<torch::Tensor> gru_biases_ih,
    std::vector<torch::Tensor> gru_biases_hh,
    torch::Tensor h0,
    bool is_training) {
  // Ensure h0 is on the same device as x
  h0 = h0.to(x.device());

  // Retrieve dimensions and parameters
  size_t num_layers = gru_weights_ih.size();
  int64_t input_size = x.size(2);
  int64_t hidden_size = gru_weights_hh[0].size(1);
  bool bidirectional = false;
  bool batch_first = false;

  // Create GRU options
  torch::nn::GRUOptions gru_options(input_size, hidden_size);
  gru_options.num_layers(num_layers);
  gru_options.bidirectional(bidirectional);
  gru_options.batch_first(batch_first);

  // Initialize the GRU module
  torch::nn::GRU gru(gru_options);
  gru->to(x.device());
  gru->train(is_training);

  // Access the named parameters of the GRU module
  auto params = gru->named_parameters();

  // Prefetch tensors to cache
  #pragma unroll
  for (size_t l = 0; l < num_layers; ++l) {
    torch::cuda::CUDAGraph::record(gru_weights_ih[l]);
    gru_weights_hh[l].prefetch();
    gru_biases_ih[l].prefetch();
    gru_biases_hh[l].prefetch();
  }

  // Set the weights and biases for each layer with unrolled loop
  #pragma unroll
  for (size_t l = 0; l < num_layers; ++l) {
    std::string layer_str = std::to_string(l);

    // Parameter names
    std::string w_ih_key = ""weight_ih_l"" + layer_str;
    std::string w_hh_key = ""weight_hh_l"" + layer_str;
    std::string b_ih_key = ""bias_ih_l"" + layer_str;
    std::string b_hh_key = ""bias_hh_l"" + layer_str;

    // Copy the provided weights and biases into the GRU module
    params[w_ih_key].copy_(gru_weights_ih[l]);
    params[w_hh_key].copy_(gru_weights_hh[l]);
    params[b_ih_key].copy_(gru_biases_ih[l]);
    params[b_hh_key].copy_(gru_biases_hh[l]);
  }

  // Reshape h0 to match expected dimensions: (num_layers * num_directions, batch, hidden_size)
  h0 = h0.contiguous().view({static_cast<int64_t>(num_layers), x.size(1), hidden_size});

  // Forward pass through the GRU module
  auto result = gru->forward(x, h0);
  torch::Tensor output = std::get<0>(result);

  return output;
}

// Pybind11 module definition
PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
  m.def(""forward"", &forward, ""GRU forward (CUDA)"");
}",other,,,,,0,1,0.9063444,2476
"#include <torch/extension.h>
#include <ATen/ATen.h>
#include <cuda.h>
#include <cuda_runtime.h>
#include <cmath>

// Macros for input checking
#define CHECK_CUDA(x) TORCH_CHECK(x.is_cuda(), #x "" must be a CUDA tensor"")
#define CHECK_CONTIGUOUS(x) TORCH_CHECK(x.is_contiguous(), #x "" must be contiguous"")
#define CHECK_INPUT(x) \
    CHECK_CUDA(x);     \
    CHECK_CONTIGUOUS(x)

// Fused activation function: Swish, division, clamping, tanh, and final clamp
// Note: All arithmetic is performed in the same precision as input
template <typename scalar_t>
__device__ __forceinline__ scalar_t activation_func(scalar_t x) {
    const scalar_t one = static_cast<scalar_t>(1);
    const scalar_t half = static_cast<scalar_t>(0.5);
    scalar_t sigmoid_x = one / (one + exp(-x));
    x = x * sigmoid_x;
    x = x * half;  // Divide by 2
    x = max(min(x, one), -one);  // First clamp
    x = tanh(x);                 
    x = max(min(x, one), -one);  // Final clamp
    return x;
}

// Kernel specialized for float using vectorized access (float4) for coalesced memory operations
__global__ void module_kernel_coalesced_float(const float* __restrict__ x_in,
                                               float* __restrict__ x_out,
                                               const size_t size) {
    const int tid = blockIdx.x * blockDim.x + threadIdx.x;
    const int stride = blockDim.x * gridDim.x;
    const int vec_size = 4;
    // Compute the number of elements that can be processed in vectorized mode
    size_t aligned_size = size - (size % vec_size);

    // Reinterpret pointers for vectorized access
    float4* x_in_vec = (float4*) x_in;
    float4* x_out_vec = (float4*) x_out;
    size_t vec_len = aligned_size / vec_size;

    // Each thread processes a float4, ensuring consecutive threads read consecutive memory
    for (size_t i = tid; i < vec_len; i += stride) {
        float4 data = __ldg(&x_in_vec[i]);
        data.x = activation_func(data.x);
        data.y = activation_func(data.y);
        data.z = activation_func(data.z);
        data.w = activation_func(data.w);
        x_out_vec[i] = data;
    }

    // Process any remaining elements that were not covered in the vectorized loop
    for (size_t i = aligned_size + tid; i < size; i += stride) {
        float x = __ldg(&x_in[i]);
        x = activation_func(x);
        x_out[i] = x;
    }
}

// Generic kernel for non-float types (or when vectorized access is not applicable)
// Each thread reads/writes a single element in a grid-stride loop
template <typename scalar_t>
__global__ void module_kernel_coalesced_generic(const scalar_t* __restrict__ x_in,
                                                  scalar_t* __restrict__ x_out,
                                                  const size_t size) {
    size_t idx = blockIdx.x * blockDim.x + threadIdx.x;
    size_t stride = blockDim.x * gridDim.x;
    for (; idx < size; idx += stride) {
        scalar_t x = __ldg(&x_in[idx]);
        x = activation_func(x);
        x_out[idx] = x;
    }
}

// CUDA forward function
// Performs the linear transformation via torch::addmm and applies the activation kernel
torch::Tensor module_forward_cuda(
    torch::Tensor x,
    torch::Tensor weight,
    torch::Tensor bias) {
    // Compute x_linear = bias + x * weight^T using optimized addmm
    auto x_linear = torch::addmm(bias, x, weight.t());
    auto x_out = torch::empty_like(x_linear);

    size_t size = x_linear.numel();
    const int threads = 256;

    AT_DISPATCH_FLOATING_TYPES_AND_HALF(x_linear.scalar_type(), ""module_forward_cuda"", ([&] {
        if (std::is_same<scalar_t, float>::value) {
            // For float, use vectorized kernel
            const int vec_size = 4;
            size_t aligned_size = size - (size % vec_size);
            int blocks = ((aligned_size / vec_size) + threads - 1) / threads;
            module_kernel_coalesced_float<<<blocks, threads>>>(
                x_linear.data_ptr<scalar_t>(),
                x_out.data_ptr<scalar_t>(),
                size);
        } else {
            // For other types, fall back to the generic kernel
            int blocks = (size + threads - 1) / threads;
            module_kernel_coalesced_generic<scalar_t><<<blocks, threads>>>(
                x_linear.data_ptr<scalar_t>(),
                x_out.data_ptr<scalar_t>(),
                size);
        }
    }));

    return x_out;
}

// C++ interface: wraps the CUDA forward function with input checks
torch::Tensor module_forward(
    torch::Tensor x,
    torch::Tensor weight,
    torch::Tensor bias) {
    CHECK_INPUT(x);
    CHECK_INPUT(weight);
    CHECK_INPUT(bias);
    return module_forward_cuda(x, weight, bias);
}

// PyBind11 module definition
PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
    m.def(""forward"", &module_forward, ""Custom module forward function (CUDA)"");
}
",gemm,,,,,0,1,0.902282,4854
"#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>

__global__ void avg_pool1d_kernel(
    const float *input,
    float *output,
    int kernel_size,
    int stride,
    int padding,
    int input_length,
    int output_length,
    int batch_size,
    int in_channels) {

    int channel = blockIdx.y;
    int batch = blockIdx.z;

    // Use grid-stride loop to cover all output positions
    for (int o = blockIdx.x * blockDim.x + threadIdx.x; o < output_length; o += blockDim.x * gridDim.x) {
        float sum = 0.0f;
        for (int k = 0; k < kernel_size; ++k) {
            int pos_padded = o * stride + k;
            int pos_input = pos_padded - padding;
            if (pos_input >= 0 && pos_input < input_length) {
                int input_idx = batch * in_channels * input_length + channel * input_length + pos_input;
                sum += input[input_idx];
            }
        }
        int output_idx = batch * in_channels * output_length + channel * output_length + o;
        output[output_idx] = sum / kernel_size;
    }
}

torch::Tensor avg_pool1d_forward(
    const torch::Tensor &x,
    int kernel_size,
    int stride,
    int padding) {
    
    TORCH_CHECK(x.is_cuda(), ""x must be a CUDA tensor"");
    TORCH_CHECK(x.dim() == 3, ""x must be 3D"");
    TORCH_CHECK(kernel_size > 0 && stride > 0 && padding >= 0, ""Invalid kernel parameters"");

    int batch_size = x.size(0);
    int in_channels = x.size(1);
    int input_length = x.size(2);
    int output_length = (input_length + 2 * padding - kernel_size) / stride + 1;

    auto output = torch::empty({batch_size, in_channels, output_length}, x.options());

    // Use a fixed number of threads per block with grid-stride looping for large workloads
    dim3 threads(256);
    dim3 grid(
        (output_length + threads.x - 1) / threads.x,
        in_channels,
        (batch_size + gridDim.z - 1) / gridDim.z
    );

    avg_pool1d_kernel<<<grid, threads>>>(
        x.data_ptr<float>(),
        output.data_ptr<float>(),
        kernel_size,
        stride,
        padding,
        input_length,
        output_length,
        batch_size,
        in_channels
    );

    return output;
}

PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
    m.def(""forward"", &avg_pool1d_forward, ""1D Average Pooling forward (CUDA) with stride loops"");
}
",other,,,,,0,1,0.88811487,2336
"#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>

template <typename scalar_t>
__global__ void grid_aligned_div_leaky_relu_kernel(
    scalar_t* __restrict__ x,
    scalar_t divisor,
    scalar_t negative_slope,
    int height,
    int width,
    int channels,
    int batch_size
) {
    // Use 1D thread indexing for better memory coalescing
    const int tid = threadIdx.x;
    const int bid = blockIdx.x;
    
    // Calculate total elements and stride
    const int total_elements = batch_size * channels * height * width;
    const int stride = blockDim.x * gridDim.x;
    
    // Process multiple elements per thread for better utilization
    const scalar_t inv_divisor = static_cast<scalar_t>(1.0) / divisor;  // Precompute reciprocal
    
    for (int idx = bid * blockDim.x + tid; idx < total_elements; idx += stride) {
        scalar_t val = x[idx] * inv_divisor;  // Multiply by reciprocal instead of dividing
        x[idx] = (val >= static_cast<scalar_t>(0)) ? val : val * negative_slope;
    }
}

torch::Tensor grid_aligned_div_leaky_relu(
    torch::Tensor x,
    double divisor,
    double negative_slope
) {
    x = x.contiguous();
    
    // Get tensor dimensions
    int batch_size = x.size(0);
    int channels = x.size(1);
    int height = x.size(2);
    int width = x.size(3);

    if (x.is_cuda()) {
        // Configure 2D block dimensions
        dim3 threads(16, 16);  // 256 threads per block, arranged in 2D
        
        // Calculate grid dimensions to cover the entire image
        dim3 blocks(
            (width + threads.x - 1) / threads.x,
            (height + threads.y - 1) / threads.y,
            batch_size * channels  // Process all batches and channels
        );

        AT_DISPATCH_FLOATING_TYPES_AND_HALF(x.scalar_type(), ""grid_aligned_div_leaky_relu_cuda"", ([&] {
            scalar_t divisor_val = static_cast<scalar_t>(divisor);
            scalar_t negative_slope_val = static_cast<scalar_t>(negative_slope);
            
            grid_aligned_div_leaky_relu_kernel<scalar_t><<<blocks, threads>>>(
                x.data_ptr<scalar_t>(),
                divisor_val,
                negative_slope_val,
                height,
                width,
                channels,
                batch_size
            );
        }));
    }
    return x;
}

torch::Tensor forward(
    torch::Tensor x,
    torch::Tensor conv_weight,
    torch::Tensor conv_bias,
    double divisor
) {
    x = at::conv2d(x, conv_weight, conv_bias);
    x = grid_aligned_div_leaky_relu(x, divisor, 0.01);
    return x;
}

PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
    m.def(""forward"", &forward, ""Convolution, division, and LeakyReLU forward (grid aligned)"");
}",conv2d,,,,,0,1,0.88434535,2722
"#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>
#include <vector>
#include <cmath>

// Device function for Mish activation for arbitrary scalar types.
template <typename scalar_t>
__device__ inline scalar_t mish_activation(scalar_t x) {
    return x * tanh(log1p(exp(x)));
}

// Device function to compute the convolution sum for a given output element.
template <typename scalar_t>
__device__ scalar_t compute_convolution_sum(const scalar_t* __restrict__ input,
                                            const scalar_t* __restrict__ weight,
                                            int batch_idx, int oc, int oh, int ow,
                                            int in_channels, int in_h, int in_w,
                                            int k_h, int k_w) {
    scalar_t result = 0;
    for (int ic = 0; ic < in_channels; ++ic) {
        #pragma unroll
        for (int kh = 0; kh < k_h; ++kh) {
            #pragma unroll
            for (int kw = 0; kw < k_w; ++kw) {
                int in_y = oh + kh - k_h / 2;
                int in_x = ow + kw;
                int input_idx = batch_idx * (in_channels * in_h * in_w) 
                              + ic * (in_h * in_w) 
                              + in_y * in_w 
                              + in_x;
                int weight_idx = oc * (in_channels * k_h * k_w) 
                               + ic * (k_h * k_w) 
                               + kh * k_w 
                               + kw;
                result += input[input_idx] * weight[weight_idx];
            }
        }
    }
    return result;
}

// Templated CUDA kernel that performs 2D convolution (stride=1, no padding)
// followed by two sequential Mish activations.
template <typename scalar_t>
__global__ void conv2d_mish_kernel_modular(
    const scalar_t* __restrict__ input,
    const scalar_t* __restrict__ weight,
    const scalar_t* __restrict__ bias,
    scalar_t* __restrict__ output,
    int batch_size,
    int in_channels,
    int in_h,
    int in_w,
    int out_channels,
    int k_h,
    int k_w,
    int out_h,
    int out_w) {

    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    int total = batch_size * out_channels * out_h * out_w;
    if (idx >= total) return;

    // Compute the output pixel indices from the flattened index.
    int ow = idx % out_w;
    int oh = (idx / out_w) % out_h;
    int oc = (idx / (out_w * out_h)) % out_channels;
    int b = idx / (out_w * out_h * out_channels);

    // Compute convolution sum and add bias.
    scalar_t sum = bias[oc] +
                   compute_convolution_sum(input, weight, b, oc, oh, ow, in_channels, in_h, in_w, k_h, k_w);

    // Apply the first Mish activation.
    scalar_t tmp = mish_activation(sum);
    // Apply the second Mish activation.
    scalar_t out_val = mish_activation(tmp);

    // Write the result to the output tensor.
    output[idx] = out_val;
}

// Host wrapper function for launching the CUDA kernel.
at::Tensor forward(at::Tensor input, at::Tensor conv_weight, at::Tensor conv_bias) {
    // Expected input dimensions: [batch_size, in_channels, in_h, in_w]
    // conv_weight dimensions: [out_channels, in_channels, k_h, k_w]
    // conv_bias dimensions: [out_channels]

    const auto batch_size = input.size(0);
    const auto in_channels = input.size(1);
    const auto in_h = input.size(2);
    const auto in_w = input.size(3);

    const auto out_channels = conv_weight.size(0);
    const auto k_h = conv_weight.size(2);
    const auto k_w = conv_weight.size(3);

    // Compute output spatial dimensions (assuming stride=1 and no padding).
    const auto out_h = in_h - k_h + 1;
    const auto out_w = in_w - k_w + 1;

    // Allocate the output tensor.
    auto output = at::empty({batch_size, out_channels, out_h, out_w}, input.options());

    const int total = batch_size * out_channels * out_h * out_w;
    const int blockSize = 256;
    const int numBlocks = (total + blockSize - 1) / blockSize;

    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), ""conv2d_mish_forward_cuda_modular"", ([&] {
        conv2d_mish_kernel_modular<scalar_t><<<numBlocks, blockSize>>>(
            input.data_ptr<scalar_t>(),
            conv_weight.data_ptr<scalar_t>(),
            conv_bias.data_ptr<scalar_t>(),
            output.data_ptr<scalar_t>(),
            batch_size,
            in_channels,
            in_h,
            in_w,
            out_channels,
            k_h,
            k_w,
            out_h,
            out_w);
    }));

    return output;
}

PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
    m.def(""forward"", &forward, ""2D Convolution with double Mish activation and modular implementation (CUDA)"");
}
",conv2d,,,,,0,1,0.8839256,4695
"#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>
#include <vector>

// Optimized kernel using a grid-stride loop and loop unrolling
// This kernel computes the cumulative product along the specified dimension in a tensor.

template <typename scalar_t>
__global__ void cumprod_kernel_optimized(
    scalar_t* output,
    const scalar_t* input,
    const int64_t total_threads,
    const int64_t dim_size,
    const int64_t stride) {

    // Use grid-stride loop to cover all tasks without warp divergence
    for (int idx = blockIdx.x * blockDim.x + threadIdx.x; idx < total_threads; idx += blockDim.x * gridDim.x) {
        // Determine the batch and inner index
        int batch = idx / stride;
        int in_idx = idx % stride;

        // Base pointer offset for this cumulative product task
        int base = batch * dim_size * stride + in_idx;
        scalar_t prod = input[base];

        // Unroll loop over dim_size to reduce loop overhead
        #pragma unroll
        for (int i = 0; i < dim_size; i++) {
            int offset = base + i * stride;
            prod *= input[offset];
            output[offset] = prod;
        }
    }
}

// Host function to launch the optimized CUDA kernel

torch::Tensor cumprod_cuda_forward_optimized(torch::Tensor input, int64_t dim) {
    auto output = torch::empty_like(input);

    // Retrieve tensor properties
    auto sizes = input.sizes();
    auto strides = input.strides();

    int64_t dim_size = sizes[dim];
    int64_t stride = strides[dim];
    int64_t numel = input.numel();

    // Each cumulative product task processes a complete column along the dim dimension
    int64_t total_threads = numel / dim_size;

    const int threads = 256;
    const int blocks = (total_threads + threads - 1) / threads;

    AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.scalar_type(), ""cumprod_cuda_forward_optimized"", ([&] {
        cumprod_kernel_optimized<scalar_t><<<blocks, threads>>>(
            output.data_ptr<scalar_t>(),
            input.data_ptr<scalar_t>(),
            total_threads,
            dim_size,
            stride
        );
    }));

    return output;
}

PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
    m.def(""forward"", &cumprod_cuda_forward_optimized, ""Optimized Cumulative product forward (CUDA)"");
}
",other,,,,,0,1,0.8826721,2300
"#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>

// Activation functor for ReLU
template <typename T>
struct Relu {
    __device__ T operator()(T x) const {
        return x > T(0) ? x : T(0);
    }
};

// Activation functor for Identity (no activation)
template <typename T>
struct Identity {
    __device__ T operator()(T x) const {
        return x;
    }
};

// Kernel using stride loops to cover all output elements
// Each thread processes multiple output elements, which are flattened from batch and feature dimensions
template <typename T, typename Activation>
__global__ void linear_activation_stride_kernel(
    const T* __restrict__ input, int in_dim,
    const T* __restrict__ weight, const T* __restrict__ bias,
    T* __restrict__ output, int out_dim, int batch_size,
    Activation activation) {

    int total_elements = batch_size * out_dim;
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    int stride = blockDim.x * gridDim.x;

    for (int pos = idx; pos < total_elements; pos += stride) {
        int batch_idx = pos / out_dim;
        int feature = pos % out_dim;

        T sum = T(0);
        int input_offset = batch_idx * in_dim;\n        int weight_offset = feature * in_dim;
        int weight_offset = feature * in_dim;
        
        // Compute dot product
        for (int k = 0; k < in_dim; k++) {
            sum += input[input_offset + k] * weight[weight_offset + k];
        }
        sum += bias[feature];
        output[pos] = activation(sum);
    }
}

// Forward function that applies the MLP layers with stride loop kernels
// Hidden layers use ReLU activation; final layer uses Identity activation
torch::Tensor forward(
    torch::Tensor x,
    std::vector<torch::Tensor> weights,
    std::vector<torch::Tensor> biases) {

    TORCH_CHECK(weights.size() == biases.size(), ""Weights and biases count mismatch"");
    TORCH_CHECK(x.size(1) == weights[0].size(1), ""Input dimension mismatch"");

    torch::Tensor current_input = x;
    const int threads = 256;

    // Process all layers except the last using ReLU activation
    for (size_t i = 0; i < weights.size() - 1; i++) {
        auto weight = weights[i];
        auto bias = biases[i];
        int in_dim = weight.size(1);
        int out_dim = weight.size(0);
        int batch_size = current_input.size(0);

        auto output = torch::zeros({batch_size, out_dim}, 
            torch::device(torch::kCUDA).dtype(current_input.dtype()));

        int total_elements = batch_size * out_dim;
        int blocks = (total_elements + threads - 1) / threads;

        if (current_input.dtype() == torch::kFloat32) {
            linear_activation_stride_kernel<float, Relu<float>><<<blocks, threads>>>(
                current_input.data_ptr<float>(),
                in_dim,
                weight.data_ptr<float>(),
                bias.data_ptr<float>(),
                output.data_ptr<float>(),
                out_dim,
                batch_size,
                Relu<float>()
            );
        } else {
            TORCH_CHECK(false, ""Unsupported dtype"");
        }

        current_input = output;
    }

    // Last layer uses Identity activation
    {
        auto weight = weights.back();
        auto bias = biases.back();
        int in_dim = weight.size(1);
        int out_dim = weight.size(0);
        int batch_size = current_input.size(0);

        auto output = torch::zeros({batch_size, out_dim}, 
            torch::device(torch::kCUDA).dtype(current_input.dtype()));

        int total_elements = batch_size * out_dim;
        int blocks = (total_elements + threads - 1) / threads;
        
        if (current_input.dtype() == torch::kFloat32) {
            linear_activation_stride_kernel<float, Identity<float>><<<blocks, threads>>>(
                current_input.data_ptr<float>(),
                in_dim,
                weight.data_ptr<float>(),
                bias.data_ptr<float>(),
                output.data_ptr<float>(),
                out_dim,
                batch_size,
                Identity<float>()
            );
        } else {
            TORCH_CHECK(false, ""Unsupported dtype"");
        }
        
        current_input = output;
    }

    return current_input;
}

PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
    m.def(""forward"", &forward, ""MLP forward (CUDA) with stride loops"");
}
",other,,,,,0,1,0.8797974,4360
"#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>

// Kernel using warp shuffle reduction with loop unrolling for performance
__global__ void custom_kernel(
    const float *x,
    const float *weight,
    float *output,
    float scaling_factor,
    int input_size,
    int hidden_size,
    int batch_size) {

    int batch_idx = blockIdx.x;
    if (batch_idx >= batch_size) return;

    int tid = threadIdx.x;

    extern __shared__ float x_shared[];
    // Load the input vector for current batch into shared memory using a uniform strided loop
    for (int k = tid; k < input_size; k += blockDim.x) {
        x_shared[k] = x[batch_idx * input_size + k];
    }
    __syncthreads();

    float thread_sum = 0.0f;
    // Uniformly distribute work over hidden_size using a stride loop to avoid divergent conditional bounds
    for (int j = tid; j < hidden_size; j += blockDim.x) {
        const float *weight_row = weight + j * input_size;
        float dot = 0.0f;
        // Unroll loop for dot product computation
        #pragma unroll 4
        for (int k = 0; k < input_size; ++k) {
            dot += x_shared[k] * weight_row[k];
        }
        thread_sum += dot;
    }

    // Perform warp-level reduction using shuffle intrinsics to minimize divergence
    unsigned int mask = 0xFFFFFFFF;
    for (int offset = warpSize / 2; offset > 0; offset /= 2) {
        thread_sum += __shfl_down_sync(mask, thread_sum, offset);
    }

    // Each warp's lane 0 writes its sum to shared memory
    __shared__ float warp_sums[blockDim.x/32];
    int warp_id = tid / warpSize;
    if ((tid & (warpSize - 1)) == 0) {
        warp_sums[warp_id] = thread_sum;
    }
    __syncthreads();

    // Final reduction performed by the first warp
    if (tid < (blockDim.x / 32)) {
        float warp_total = warp_sums[tid];
        for (int offset = (blockDim.x / 32) / 2; offset > 0; offset /= 2) {
            warp_total += __shfl_down_sync(mask, warp_total, offset);
        }
        if (tid == 0) {
            // Final scaling: first divide by 2 then multiply by scaling_factor
            output[batch_idx] = (warp_total / 2.0f) * scaling_factor;
        }
    }
}

// Interface function to launch the kernel
torch::Tensor forward_cuda(
    torch::Tensor x,
    float scaling_factor,
    torch::Tensor weight) {

    int batch_size = x.size(0);
    int input_size = x.size(1);
    int hidden_size = weight.size(0);

    auto output = torch::zeros({batch_size, 1}, x.options());

    const int BLOCK_THREADS = 256;
    dim3 grid(batch_size);
    dim3 block(BLOCK_THREADS);
    size_t shared_mem = input_size * sizeof(float);

    custom_kernel<<<grid, block, shared_mem>>>(
        x.data_ptr<float>(),
        weight.data_ptr<float>(),
        output.data_ptr<float>(),
        scaling_factor,
        input_size,
        hidden_size,
        batch_size);

    cudaError_t err = cudaGetLastError();
    if (err != cudaSuccess) {
        printf(""CUDA Error: %s\n"", cudaGetErrorString(err));
    }

    return output;
}

PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
    m.def(""forward"", &forward_cuda, ""Custom forward CUDA function"");
}",gemm,,,,,0,1,0.8769189,3151
"#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>

// Kernel that performs reduction with loop unrolling for better performance
template <typename scalar_t>
__global__ void sum_reduce_unrolled_kernel(
    const scalar_t* __restrict__ input,
    scalar_t* __restrict__ output,
    int64_t reduce_size,
    int64_t inner_size) {

    // Each block is responsible for one output element
    int out_idx = blockIdx.x; // flattened index over outer * inner
    int outer_idx = out_idx / inner_size;
    int inner_idx = out_idx % inner_size;
    int64_t base = outer_idx * reduce_size * inner_size + inner_idx;

    // Each thread accumulates a partial sum over the reduction dimension
    scalar_t sum = 0;
    int i = threadIdx.x;

    // Unroll loop for better performance
    for (; i + 4 <= reduce_size; i += 4 * blockDim.x) {
        sum += input[base + i * inner_size] +
               input[base + (i + blockDim.x) * inner_size] +
               input[base + (i + 2 * blockDim.x) * inner_size] +
               input[base + (i + 3 * blockDim.x) * inner_size];
    }

    for (; i < reduce_size; i += blockDim.x) {
        sum += input[base + i * inner_size];
    }

    // Intra-warp reduction using warp shuffle
    unsigned int mask = 0xffffffff;
    for (int offset = warpSize/2; offset > 0; offset /= 2) {
        sum += __shfl_down_sync(mask, sum, offset);
    }

    // Use shared memory to accumulate results from different warps
    __shared__ scalar_t warp_sum[32]; // Enough space for up to 32 warps per block
    int warp_id = threadIdx.x / warpSize;
    if ((threadIdx.x & (warpSize - 1)) == 0) {
        warp_sum[warp_id] = sum;
    }
    __syncthreads();

    // First warp now reduces the partial sums from all warps
    int num_warps = (blockDim.x + warpSize - 1) / warpSize;
    if (threadIdx.x < num_warps) {
        sum = warp_sum[threadIdx.x];
        for (int offset = warpSize/2; offset > 0; offset /= 2) {
            sum += __shfl_down_sync(mask, sum, offset);
        }
        if (threadIdx.x == 0) {
            output[out_idx] = sum;
        }
    }
}

// Host function that prepares tensor dimensions and launches the unrolled reduction kernel

torch::Tensor sum_reduce_unrolled_cuda(torch::Tensor input, int64_t dim) {
    // Handle negative dimensions
    if (dim < 0) dim += input.dim();

    // Compute sizes: outer, reduce, and inner
    auto sizes = input.sizes().vec();
    int64_t reduce_size = sizes[dim];
    int64_t outer_size = 1;
    for (int i = 0; i < dim; i++) {
        outer_size *= sizes[i];
    }
    int64_t inner_size = 1;
    for (int i = dim + 1; i < sizes.size(); i++) {
        inner_size *= sizes[i];
    }

    // Prepare output tensor shape (with dimension 'dim' collapsed to 1)
    sizes[dim] = 1;
    auto output = torch::empty(sizes, input.options());

    // Launch kernel: one block per output element
    int64_t num_output = outer_size * inner_size;
    int threads = 256; // Ensure threads is a multiple of 32 for warp efficiency
    int blocks = num_output;

    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), ""sum_reduce_unrolled_cuda"", ([&] {
        sum_reduce_unrolled_kernel<scalar_t><<<blocks, threads>>>(
            input.data_ptr<scalar_t>(),
            output.data_ptr<scalar_t>(),
            reduce_size,
            inner_size
        );
    }));

    return output;
}

PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
    m.def(""forward"", &sum_reduce_unrolled_cuda, ""Sum reduction with unrolled loops (CUDA)"");
}",other,,,,,0,1,0.86843914,3514
"#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>

// Device function performing clamp and division with pre-cast constants
template <typename scalar_t>
__device__ __forceinline__ scalar_t clamp_and_divide_op(scalar_t x, const scalar_t min_val, const scalar_t inv_divisor) {
    return (x < min_val ? min_val : x) * inv_divisor;
}

// Kernel with grid-stride loop and loop unrolling, using an experimental block size
template <typename scalar_t>
__global__ void clamp_and_divide_kernel(scalar_t* __restrict__ output,
                                           const int64_t numel,
                                           const scalar_t min_val,
                                           const scalar_t divisor) {
    int tid = blockIdx.x * blockDim.x + threadIdx.x;
    int stride = blockDim.x * gridDim.x;
    
    #pragma unroll 4
    for (int i = tid; i < numel; i += stride) {
        output[i] = clamp_and_divide_op(output[i], min_val, divisor);
    }
}

// Forward function: perform 3D transposed convolution then apply the clamp and divide kernel
torch::Tensor forward(torch::Tensor input,
                      int stride,
                      int padding,
                      float min_value,
                      float divisor,
                      torch::Tensor weight,
                      torch::Tensor bias) {
    // Execute 3D transposed convolution using PyTorch
    auto output = torch::conv_transpose3d(input, weight, bias, stride, padding);

    // Experiment with block sizes: here we use 512 threads per block as an optimization
    const int block_size = 512;
    int blocks = (output.numel() + block_size - 1) / block_size;
    if (blocks > 65535) {
        blocks = 65535;
    }

    AT_DISPATCH_FLOATING_TYPES(output.scalar_type(), ""optimized_blocksize_divide"", ([&] {
        const auto min_val_cast = static_cast<scalar_t>(min_value);
        const auto divisor_cast = static_cast<scalar_t>(divisor);
        clamp_and_divide_kernel<scalar_t><<<blocks, block_size>>>(
            output.data_ptr<scalar_t>(),
            output.numel(),
            min_val_cast,
            divisor_cast
        );
    }));

    return output;
}

PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
    m.def(""forward"", &forward, ""3D Transposed convolution with clamp and divide (CUDA) using optimized block size"");
}
",other,,,,,0,1,0.8672904,2353
"#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>

constexpr int WARP_SIZE = 32;
constexpr int ELEMENTS_PER_THREAD = 8;

__global__ void optimized_grid_stride_kernel(
    const float* __restrict__ log_predictions,
    const float* __restrict__ targets,
    float* __restrict__ output,
    const int n) {
    
    const int global_tid = blockIdx.x * blockDim.x + threadIdx.x;
    const int total_threads = gridDim.x * blockDim.x;
    const int elements_per_cycle = total_threads * ELEMENTS_PER_THREAD;
    
    float thread_sum = 0.0f;

    // Grid-stride loop with predefined element count per iteration
    for (int base = global_tid * ELEMENTS_PER_THREAD; 
         base < n; 
         base += elements_per_cycle) {
        
        #pragma unroll
        for (int i = 0; i < ELEMENTS_PER_THREAD; ++i) {
            const int idx = base + i * total_threads;
            if (idx < n) {
                const float log_pred = __ldg(log_predictions + idx);
                const float target = __ldg(targets + idx);
                thread_sum += expf(log_pred) - target * log_pred;
            }
        }
    }

    // Warp-level reduction using butterfly pattern
    for (int offset = WARP_SIZE/2; offset >= 1; offset >>= 1) {
        thread_sum += __shfl_down_sync(0xffffffff, thread_sum, offset);
    }

    // Block-level reduction
    extern __shared__ float warp_sums[];
    const int warp_id = threadIdx.x / WARP_SIZE;
    if (threadIdx.x % WARP_SIZE == 0) {
        warp_sums[warp_id] = thread_sum;
    }
    __syncthreads();

    // Final reduction by first warp only
    if (warp_id == 0) {
        thread_sum = (threadIdx.x < blockDim.x / WARP_SIZE) ? warp_sums[threadIdx.x] : 0.0f;
        for (int offset = WARP_SIZE/2; offset >= 1; offset >>= 1) {
            thread_sum += __shfl_down_sync(0xffffffff, thread_sum, offset);
        }
        
        if (threadIdx.x == 0) {
            atomicAdd(output, thread_sum);
        }
    }
}

torch::Tensor optimized_grid_stride_forward(
    torch::Tensor log_predictions,
    torch::Tensor targets) {
    
    const int n = log_predictions.numel();
    auto output = torch::zeros({1}, log_predictions.options());
    
    const int threads = 256;
    const int elements_per_block = threads * ELEMENTS_PER_THREAD;
    int blocks = (n + elements_per_block - 1) / elements_per_block;
    blocks = min(blocks, 256);
    
    optimized_grid_stride_kernel<<<blocks, threads, (threads/WARP_SIZE)*sizeof(float)>>>(
        log_predictions.data_ptr<float>(),
        targets.data_ptr<float>(),
        output.data_ptr<float>(),
        n
    );
    
    return output / static_cast<float>(n);
}

PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
    m.def(""forward"", &optimized_grid_stride_forward, ""Optimized grid-stride KL divergence (CUDA)"");
}
",other,,,,,0,1,0.86708385,2814
"#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>
#include <math.h>
#include <vector>

#define CHECK_CUDA(x) TORCH_CHECK(x.is_cuda(), #x "" must be a CUDA tensor"")
#define CHECK_CONTIGUOUS(x) TORCH_CHECK(x.is_contiguous(), #x "" must be contiguous"")
#define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)

// This kernel computes the ELU activation elementwise and also accumulates a reduction (the sum of all activated values)
// It uses a grid-stride loop for processing, shared memory for intra-block reductions,
// and warp-level primitives (__shfl_down_sync) for efficient reduction within a warp.

__global__ void elu_activation_reduction_kernel(const float* __restrict__ x,
                                                  float* __restrict__ out,
                                                  float* __restrict__ global_sum,
                                                  float alpha,
                                                  int n) {
    float local_sum = 0.0f;
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    int stride = blockDim.x * gridDim.x;

    // Process multiple elements per thread with grid-stride loop
    for (int i = idx; i < n; i += stride) {
        float val = x[i];
        float activated = (val > 0.f) ? val : alpha * (expf(val) - 1.f);
        out[i] = activated;
        local_sum += activated;
    }

    // Perform warp-level reduction on local_sum
    unsigned int mask = 0xffffffff;
    int lane = threadIdx.x & 31; // lane index in the warp
    // Reduce within warp using shuffle
    for (int offset = 16; offset > 0; offset /= 2) {
        local_sum += __shfl_down_sync(mask, local_sum, offset);
    }

    // Each warp's lane 0 writes its sum to shared memory
    __shared__ float shared[32];  // enough for up to 32 warps per block
    int warpId = threadIdx.x / 32;
    if (lane == 0) {
        shared[warpId] = local_sum;
    }
    __syncthreads();

    // First warp of the block reduces the partial sums from each warp
    if (threadIdx.x < 32) {
        int numWarps = (blockDim.x + 31) / 32;
        float block_sum = (threadIdx.x < numWarps) ? shared[threadIdx.x] : 0.0f;
        for (int offset = 16; offset > 0; offset /= 2) {
            block_sum += __shfl_down_sync(mask, block_sum, offset);
        }
        if (threadIdx.x == 0) {
            atomicAdd(global_sum, block_sum);
        }
    }
}

// The wrapper function allocates the output tensor and a scalar tensor to hold the reduction sum.
// It launches the kernel and returns both the ELU-activated tensor and the sum of its elements.

std::vector<torch::Tensor> elu_activation_reduction_cuda(torch::Tensor x, float alpha) {
    CHECK_INPUT(x);
    auto out = torch::empty_like(x);
    auto reduction = torch::zeros({1}, x.options());

    int n = x.numel();
    const int threads = 512;
    const int blocks = (n + threads - 1) / threads;

    // Launch kernel: shared memory is statically allocated, so no dynamic shared mem size needed
    elu_activation_reduction_kernel<<<blocks, threads>>>(
        x.data_ptr<float>(),
        out.data_ptr<float>(),
        reduction.data_ptr<float>(),
        alpha,
        n
    );

    return {out, reduction.item<float>()};
}

PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
    m.def(""forward"", &elu_activation_reduction_cuda, ""ELU activation with reduction optimization (CUDA)"");
}
",other,,,,,0,1,0.8646818,3387
"#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>
#include <vector>

#define CHECK_CUDA(x) TORCH_CHECK(x.is_cuda(), #x "" must be a CUDA tensor"")
#define CHECK_CONTIGUOUS(x) TORCH_CHECK(x.is_contiguous(), #x "" must be contiguous"")
#define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)
#define BLOCK_SIZE 256

__global__ void prod_reduce_kernel(const float* __restrict__ input, 
                                 float* __restrict__ output, 
                                 const int stride, 
                                 const int num_elements) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    float product = 1.0f;

    // Loop over the entire workload, blocking by blockDim.x and thread count
    for (int i = idx; i < num_elements; i += blockDim.x * gridDim.x * 50) {
        float local_product = 1.0f;
        int local_offset = i;
        #pragma unroll
        for (int j = 0; j < 50 && (local_offset + j * stride) < num_elements; ++j) {
            local_product *= input[local_offset + j * stride];
        }
        product *= local_product;
    }

    if (idx < num_elements) {
        output[blockIdx.x] = product;
    }
}

torch::Tensor forward(torch::Tensor x, int dim) {
    CHECK_INPUT(x);

    auto sizes = x.sizes().vec();
    int dim_size = sizes[dim];
    sizes.erase(sizes.begin() + dim);
    torch::Tensor output = torch::empty(sizes, x.options());

    int num_elements = output.numel();
    int stride = x.stride(dim);

    const float* input_ptr = x.data_ptr<float>();
    float* output_ptr = output.data_ptr<float>();

    int blocks = (num_elements + BLOCK_SIZE - 1) / BLOCK_SIZE;

    prod_reduce_kernel<<<blocks, BLOCK_SIZE>>>(input_ptr, output_ptr, stride, num_elements);

    return output;
}

PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
    m.def(""forward"", &forward, ""Product reduction over a dimension (CUDA)"");
}
",other,,,,,0,1,0.8547736,1887
"#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>

#define TILE_DIM 16
#define UNROLL_FACTOR 4

// Optimized tiled kernel using shared memory and loop unrolling for better performance
template <typename scalar_t>
__global__ void unrolled_optimized_tiled_kernel(
    const scalar_t* __restrict__ input,    // [batch_size, in_features]
    const scalar_t* __restrict__ weight,   // [out_features, in_features]
    const scalar_t* __restrict__ bias,     // [out_features]
    scalar_t* __restrict__ output,         // [batch_size, out_features]
    const int batch_size,                  // M: number of input rows
    const int in_features,                 // K: inner dimension
    const int out_features,                // N: number of output features
    const float subtract_value,
    const float multiply_value) {

    // Allocate shared memory for input and weight tiles
    __shared__ scalar_t shared_input[TILE_DIM][TILE_DIM];
    __shared__ scalar_t shared_weight[TILE_DIM][TILE_DIM];

    // Compute global row and column indices
    int row = blockIdx.x * TILE_DIM + threadIdx.x;  // batch index
    int col = blockIdx.y * TILE_DIM + threadIdx.y;  // output feature index

    scalar_t sum = 0;

    // Loop over tiles along the k dimension
    for (int t = 0; t < (in_features + TILE_DIM - 1) / TILE_DIM; t++) {
        // Load a tile of the input matrix from global memory into shared memory
        int tiled_col = t * TILE_DIM + threadIdx.y;
        if (row < batch_size && tiled_col < in_features)
            shared_input[threadIdx.x][threadIdx.y] = input[row * in_features + tiled_col];
        else
            shared_input[threadIdx.x][threadIdx.y] = 0;

        // Load a tile of the weight matrix into shared memory
        // Note: weight is stored as [out_features, in_features]
        // We are effectively accessing the transposed view to perform standard GEMM
        int tiled_row = t * TILE_DIM + threadIdx.x;
        if (col < out_features && tiled_row < in_features)
            shared_weight[threadIdx.x][threadIdx.y] = weight[col * in_features + tiled_row];
        else
            shared_weight[threadIdx.x][threadIdx.y] = 0;

        __syncthreads();

        // Multiply the tiles and accumulate
        #pragma unroll UNROLL_FACTOR
        for (int k = 0; k < TILE_DIM; k++) {
            sum += shared_input[threadIdx.x][k] * shared_weight[k][threadIdx.y];
        }
        __syncthreads();
    }

    // After accumulating the dot product, add bias and apply subtract, multiply and ReLU
    if (row < batch_size && col < out_features) {
        sum += bias[col];
        sum = (sum - subtract_value) * multiply_value;
        sum = sum > 0 ? sum : 0;
        output[row * out_features + col] = sum;
    }
}

torch::Tensor forward(
    torch::Tensor input,
    torch::Tensor weight,
    torch::Tensor bias,
    float subtract_value,
    float multiply_value) {

    auto batch_size = input.size(0);
    auto in_features = input.size(1);
    auto out_features = weight.size(0);

    auto output = torch::empty({batch_size, out_features}, input.options());

    const dim3 block(TILE_DIM, TILE_DIM);
    const dim3 grid((batch_size + TILE_DIM - 1) / TILE_DIM, (out_features + TILE_DIM - 1) / TILE_DIM);

    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), ""unrolled_optimized_tiled_kernel"", ([&] {
        unrolled_optimized_tiled_kernel<scalar_t><<<grid, block>>>(
            input.data_ptr<scalar_t>(),
            weight.data_ptr<scalar_t>(),
            bias.data_ptr<scalar_t>(),
            output.data_ptr<scalar_t>(),
            batch_size,
            in_features,
            out_features,
            subtract_value,
            multiply_value
        );
    }));

    return output;
}

PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
    m.def(""forward"", &forward, ""Unrolled optimized linear transformation with subtract, multiply, and ReLU"");
}
",other,,,,,0,1,0.8489601,3916
"#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>
#include <stdexcept>

#define TILE_SIZE 16

// Modular device function to compute convolution for a single output pixel
__device__ __forceinline__ float compute_depthwise_conv(
    const float* __restrict__ input,
    const float* __restrict__ weight,
    int b,
    int c,
    int oh,
    int ow,
    int in_h,
    int in_w,
    int channels,
    int kernel_h,
    int stride,
    int padding,
    int dilation) {
  float sum = 0.0f;
  #pragma unroll
  for (int kh = 0; kh < kernel_h; ++kh) {
    int ih = oh * stride - padding + kh * dilation;
    int iw = ow * stride - padding + kh * dilation;
    if (ih >= 0 && ih < in_h && iw >= 0 && iw < in_w) {
      // Compute index for the input tensor
      int input_idx = ((b * channels + c) * in_h + ih) * in_w + iw;
      int weight_idx = c * kernel_h + kh;
      sum += input[input_idx] * weight[weight_idx];
    }
  }
  return sum;
}

// Tiled kernel that processes a tile of the output using the modular device function
__global__ void depthwise_conv2d_modular_kernel(
    const float* __restrict__ input,
    const float* __restrict__ weight,
    const float* __restrict__ bias,
    float* __restrict__ output,
    int batch,
    int channels,
    int in_h,
    int in_w,
    int out_h,
    int out_w,
    int kernel_h,
    int stride,
    int padding,
    int dilation) {
  // Determine the tile position
  int tile_row = blockIdx.y * TILE_SIZE;
  int tile_col = blockIdx.x * TILE_SIZE;
  
  // Each block in grid.z corresponds to a unique (batch, channel) slice
  int slice = blockIdx.z; // slice index = b * channels + c
  int b = slice / channels;
  int c = slice % channels;
  
  // Each thread processes one output pixel in the tile
  int tx = threadIdx.x % TILE_SIZE;
  int ty = threadIdx.x / TILE_SIZE;
  
  int oh = tile_row + ty;
  int ow = tile_col + tx;

  if (b < batch && oh < out_h && ow < out_w) {
    float sum = compute_depthwise_conv(input, weight, b, c, oh, ow, in_h, in_w, channels, kernel_h, stride, padding, dilation);
    sum += bias[c];
    int output_idx = ((b * channels + c) * out_h + oh) * out_w + ow;
    output[output_idx] = sum;
  }
}

at::Tensor forward(
    at::Tensor x,
    at::Tensor weight,
    c10::optional<at::Tensor> bias,
    int stride,
    int padding,
    int dilation,
    int groups) {
  // Ensure input tensors are contiguous
  x = x.contiguous();
  weight = weight.contiguous();

  int batch = x.size(0);
  int channels = x.size(1);
  int in_h = x.size(2);
  int in_w = x.size(3);
  int kernel_h = weight.size(2); // weight shape: (channels, 1, kernel_h, 1)

  if (groups != channels) {
    throw std::invalid_argument(""Depthwise convolution requires groups == number of input channels."");
  }

  at::Tensor bias_val;
  if (bias.has_value() && bias.value().defined()) {
    bias_val = bias.value().contiguous();
  } else {
    bias_val = at::zeros({channels}, x.options());
  }

  int out_h = (in_h + 2 * padding - dilation * (kernel_h - 1) - 1) / stride + 1;
  int out_w = (in_w + 2 * padding - 1) / stride + 1;

  auto output = at::empty({batch, channels, out_h, out_w}, x.options());

  // Define grid dimensions
  dim3 grid(
      (out_w + TILE_SIZE - 1) / TILE_SIZE,
      (out_h + TILE_SIZE - 1) / TILE_SIZE,
      batch * channels);
  // Each block handles a TILE_SIZE x TILE_SIZE tile
  dim3 block(TILE_SIZE * TILE_SIZE);

  depthwise_conv2d_modular_kernel<<<grid, block>>>(
      x.data_ptr<float>(),
      weight.data_ptr<float>(),
      bias_val.data_ptr<float>(),
      output.data_ptr<float>(),
      batch,
      channels,
      in_h,
      in_w,
      out_h,
      out_w,
      kernel_h,
      stride,
      padding,
      dilation);

  cudaError_t err = cudaGetLastError();
  if (err != cudaSuccess) {
    throw std::runtime_error(cudaGetErrorString(err));
  }

  return output;
}

PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
  m.def(""forward"", &forward, ""Depthwise 2D Convolution forward (CUDA)"",
        py::arg(""x""),
        py::arg(""weight""),
        py::arg(""bias"") = c10::nullopt,
        py::arg(""stride""),
        py::arg(""padding""),
        py::arg(""dilation""),
        py::arg(""groups""));
}
",conv2d,,,,,0,1,0.84496063,4193
"#include <torch/extension.h>
#include <ATen/ATen.h>
#include <cuda.h>
#include <cuda_runtime.h>

// Macros for input checking
#define CHECK_CUDA(x) TORCH_CHECK(x.is_cuda(), #x "" must be a CUDA tensor"")
#define CHECK_CONTIGUOUS(x) TORCH_CHECK(x.is_contiguous(), #x "" must be contiguous"")
#define CHECK_INPUT(x) \
    CHECK_CUDA(x);     \
    CHECK_CONTIGUOUS(x)

// CUDA kernel with grid-stride loop and minimized atomic operations
template <typename scalar_t>
__global__ void module_kernel(
    const scalar_t* __restrict__ x_in,
    scalar_t* __restrict__ x_out,
    size_t size) {

    // Grid-stride loop to distribute work evenly across threads and blocks
    for (size_t idx = blockIdx.x * blockDim.x + threadIdx.x;
         idx < size;
         idx += blockDim.x * gridDim.x) {
        
        scalar_t x = x_in[idx];

        // Swish activation: x * sigmoid(x)
        scalar_t sigmoid_x = static_cast<scalar_t>(1) / (static_cast<scalar_t>(1) + exp(-x));
        x = x * sigmoid_x;

        // Divide by 2
        x = x / static_cast<scalar_t>(2);

        // Clamp between -1 and 1
        x = max(min(x, static_cast<scalar_t>(1)), static_cast<scalar_t>(-1));

        // Tanh activation
        x = tanh(x);

        // Clamp again between -1 and 1
        x = max(min(x, static_cast<scalar_t>(1)), static_cast<scalar_t>(-1));

        // Only use atomic operations if necessary for global memory updates
        atomicExch(&x_out[idx], x);
    }
}

// CUDA forward function
torch::Tensor module_forward_cuda(
    torch::Tensor x,
    torch::Tensor weight,
    torch::Tensor bias) {
    // Perform linear operation: x_linear = F.linear(x, weight, bias)
    auto x_linear = torch::addmm(bias, x, weight.t());

    // Allocate output tensor
    auto x_out = torch::empty_like(x_linear);

    // Define block and grid sizes
    const int threads = 1024;
    const int blocks = (x_linear.numel() + threads - 1) / threads;

    // Launch CUDA kernel with grid-stride loop
    AT_DISPATCH_FLOATING_TYPES_AND_HALF(x_linear.scalar_type(), ""module_forward_cuda"", ([&] {
        module_kernel<scalar_t><<<blocks, threads>>>(
            x_linear.data_ptr<scalar_t>(),
            x_out.data_ptr<scalar_t>(),
            x_linear.numel());
    }));

    return x_out;
}

// C++ interface
torch::Tensor module_forward(
    torch::Tensor x,
    torch::Tensor weight,
    torch::Tensor bias) {
    CHECK_INPUT(x);
    CHECK_INPUT(weight);
    CHECK_INPUT(bias);
    return module_forward_cuda(x, weight, bias);
}

// PyBind11 module definition
PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
    m.def(""forward"", &module_forward, ""Custom module forward function (CUDA)"");
}
",gemm,,,,,0,1,0.8417369,2666
"#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>

// This kernel assigns one warp per output pixel. Each warp's threads cooperatively compute the convolution sum
// by distributing the kernel window multiplications among warp lanes and then using warp-level reduction (__shfl_down_sync) to combine the partial sums.

__global__ void depthwise_conv2d_warp_reduced_kernel(
    const float* input,
    const float* weight,
    const float* bias,
    float* output,
    int batch_size,
    int in_channels,
    int input_h,
    int input_w,
    int out_channels,
    int output_h,
    int output_w,
    int kernel_size,
    int stride,
    int padding,
    int channels_per_group
) {
    // Each warp computes one output element
    int warp_id = (blockIdx.x * (blockDim.x / 32)) + (threadIdx.x / 32);
    int lane = threadIdx.x % 32;

    int total_outputs = batch_size * out_channels * output_h * output_w;
    if (warp_id >= total_outputs) return;

    // Decode the flat warp_id into output tensor coordinates (b, oc, h_out, w_out)
    int tmp = warp_id;
    int w_out = tmp % output_w;
    tmp /= output_w;
    int h_out = tmp % output_h;
    tmp /= output_h;
    int oc = tmp % out_channels;
    int b = tmp / out_channels;

    int in_ch = oc / channels_per_group;
    int weight_ch = oc % channels_per_group;
    int kernel_area = kernel_size * kernel_size;

    // Compute the top-left corner of the receptive field in the input
    int h_in_start = h_out * stride - padding;
    int w_in_start = w_out * stride - padding;

    float sum = 0.0f;

    // Distribute the kernel window iterations among the warp lanes
    for (int i = lane; i < kernel_area; i += 32) {
        int kh = i / kernel_size;
        int kw = i % kernel_size;
        int h_in = h_in_start + kh;
        int w_in = w_in_start + kw;
        
        // Check validity of the input coordinates
        if (h_in >= 0 && h_in < input_h && w_in >= 0 && w_in < input_w) sum += input[input_idx] * weight[weight_idx]; else sum += 0; // Avoid branch divergence
            int input_idx = b * (in_channels * input_h * input_w) +
                            in_ch * (input_h * input_w) +
                            h_in * input_w +
                            w_in;
            int weight_idx = in_ch * (channels_per_group * kernel_area) +
                             weight_ch * kernel_area +
                             i;
            sum += input[input_idx] * weight[weight_idx];
        }
    }

    // Warp-level reduction using __shfl_down_sync
    unsigned int mask = 0xffffffff;
    for (int offset = 16; offset > 0; offset /= 2) {
        sum += __shfl_down_sync(mask, sum, offset);
    }

    // The first lane in the warp writes the accumulated value to the output
    if (lane == 0) {
        if (bias != nullptr) {
            sum += bias[oc];
        }
        int output_idx = b * (out_channels * output_h * output_w) +
                         oc * (output_h * output_w) +
                         h_out * output_w +
                         w_out;
        output[output_idx] = sum;
    }
}

// The forward function prepares the launch configuration and calls the kernel

torch::Tensor forward(
    torch::Tensor input,
    torch::Tensor weight,
    torch::optional<torch::Tensor> bias,
    int stride,
    int padding
) {
    TORCH_CHECK(input.is_cuda() && weight.is_cuda(), ""Inputs must be CUDA tensors"");
    if (bias.has_value()) {
        TORCH_CHECK(bias->is_cuda(), ""Bias must be a CUDA tensor"");
    }
    TORCH_CHECK(input.is_contiguous() && weight.is_contiguous(), ""Input and weight must be contiguous"");
    if (bias.has_value()) {
        TORCH_CHECK(bias->is_contiguous(), ""Bias must be contiguous"");
    }
    TORCH_CHECK(weight.dim() == 4, ""Weight must be a 4D tensor"");

    int batch_size = input.size(0);
    int in_channels = input.size(1);
    int input_h = input.size(2);
    int input_w = input.size(3);
    int kernel_size = weight.size(2);
    int channels_per_group = weight.size(1);
    int out_channels = in_channels * channels_per_group;
    
    if (bias.has_value()) {
        TORCH_CHECK(bias->size(0) == out_channels, ""Bias size mismatch"");
    }

    int output_h = (input_h + 2 * padding - kernel_size) / stride + 1;
    int output_w = (input_w + 2 * padding - kernel_size) / stride + 1;

    auto output = torch::empty({batch_size, out_channels, output_h, output_w}, input.options());

    int total_outputs = batch_size * out_channels * output_h * output_w;
    // Each warp (32 threads) computes one output element
    int warps_per_block = 256 / 32;  // using 256 threads per block
    int num_blocks = (total_outputs + warps_per_block - 1) / warps_per_block;

    const float* bias_ptr = bias ? bias->data_ptr<float>() : nullptr;

    depthwise_conv2d_warp_reduced_kernel<<<num_blocks, 256>>>(
        input.data_ptr<float>(),
        weight.data_ptr<float>(),
        bias_ptr,
        output.data_ptr<float>(),
        batch_size,
        in_channels,
        input_h,
        input_w,
        out_channels,
        output_h,
        output_w,
        kernel_size,
        stride,
        padding,
        channels_per_group
    );

    return output;
}

PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
    m.def(""forward"", &forward, ""Depthwise 2D Convolution with Warp Reduction (CUDA)"",
          py::arg(""input""), py::arg(""weight""), py::arg(""bias"") = py::none(), py::arg(""stride""), py::arg(""padding""));
}
",conv2d,,,,,0,1,0.84133154,5456
"#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>
#include <vector>

#define CHECK_CUDA(x) TORCH_CHECK(x.is_cuda(), #x "" must be a CUDA tensor"")
#define CHECK_CONTIGUOUS(x) TORCH_CHECK(x.is_contiguous(), #x "" must be contiguous"")
#define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)

// Use constant memory to store precomputed offsets (i * stride) for reduction dimension
__constant__ int d_offsets[50];

__global__ void prod_reduce_kernel(const float* input, float* output, int num_elements) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < num_elements) {
        float product = 1.0f;
        #pragma unroll
        for (int i = 0; i < 50; ++i) {
            product *= input[idx + d_offsets[i]];
        }
        output[idx] = product;
    }
}

torch::Tensor forward(torch::Tensor x, int dim) {
    CHECK_INPUT(x);

    auto sizes = x.sizes().vec();
    int dim_size = sizes[dim];
    TORCH_CHECK(dim_size == 50, ""Dimension size must be 50 for this kernel"");
    sizes.erase(sizes.begin() + dim);
    torch::Tensor output = torch::empty(sizes, x.options());

    int num_elements = output.numel();
    int stride = x.stride(dim);

    // Precompute offsets for the reduction dimension and copy to constant memory
    int offsets_host[50];
    for (int i = 0; i < 50; ++i) {
        offsets_host[i] = i * stride;
    }
    cudaMemcpyToSymbol(d_offsets, offsets_host, 50 * sizeof(int), 0, cudaMemcpyHostToDevice);

    int threads = 1024;
    int blocks = (num_elements + threads - 1) / threads;

    const float* input_ptr = x.data_ptr<float>();
    float* output_ptr = output.data_ptr<float>();

    prod_reduce_kernel<<<blocks, threads>>>(input_ptr, output_ptr, num_elements);

    return output;
}

PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
    m.def(""forward"", &forward, ""Constant Memory Unrolled 50-Product Reduction over a dimension (CUDA)"");
}
",other,,,,,0,1,0.84069824,1905
"#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>
#include <vector>
#include <cmath>

// Define the warp size (typically 32 threads)
#define WARP_SIZE 32

// Fused kernel using warp-level primitives for reduction
// Each block handles one (batch_row, group) pair and uses one warp (32 threads) to perform reduction
__global__ void fused_warp_gn_lrelu_sum_kernel(
    float* x,               // Tensor after linear layer (shape: [batch_size, num_channels])
    int batch_size,
    int num_channels,
    int channels_per_group,
    int num_groups,
    float eps,
    float negative_slope,
    const float* __restrict__ gn_weight,
    const float* __restrict__ gn_bias) {

    // Determine the sample (row) and group this block is responsible for
    int row = blockIdx.x;      // Batch index
    int group = blockIdx.y;    // Group index
    int group_start = group * channels_per_group;

    // Each thread processes a subset of channels in the group
    float sum = 0.0f;
    float sumsq = 0.0f;

    // Loop over the channels in this group with stride equal to warp size
    for (int i = threadIdx.x; i < channels_per_group; i += WARP_SIZE) {
        int idx = row * num_channels + group_start + i;
        float val = x[idx];
        sum += val;
        sumsq += val * val;
    }

    // Perform warp-level reduction using __shfl_down_sync
    unsigned int mask = 0xffffffff;  // Full warp
    for (int offset = WARP_SIZE / 2; offset > 0; offset /= 2) {
        sum += __shfl_down_sync(mask, sum, offset);
        sumsq += __shfl_down_sync(mask, sumsq, offset);
    }

    // Broadcast the reduced results from lane 0 to all lanes
    float total_sum = __shfl_sync(mask, sum, 0);
    float total_sumsq = __shfl_sync(mask, sumsq, 0);

    // Compute mean and inverse standard deviation for the group
    float mean = total_sum / channels_per_group;
    float var = total_sumsq / channels_per_group - mean * mean;
    float inv_std = rsqrtf(var + eps);

    // Normalize, apply affine transformation, Leaky ReLU, and element-wise sum (doubling the value)
    for (int i = threadIdx.x; i < channels_per_group; i += WARP_SIZE) {
        int idx = row * num_channels + group_start + i;
        float val = x[idx];
        float norm = (val - mean) * inv_std;
        norm = norm * gn_weight[idx] + gn_bias[idx];
        norm = (norm < 0.0f) ? norm * negative_slope : norm;
        x[idx] = norm + norm;
    }
}

// Forward function: fuses the linear layer with group normalization, LeakyReLU, and element-wise sum
// 1. Performs the linear transformation using torch::addmm
// 2. Launches the fused kernel that applies group norm using warp-level reduction to speed up small reductions

torch::Tensor forward(
    torch::Tensor x,
    double eps,
    double negative_slope,
    torch::Tensor fc_weight,
    torch::Tensor fc_bias,
    torch::Tensor gn_weight,
    torch::Tensor gn_bias,
    int64_t num_groups) {

    // Ensure all tensors are CUDA tensors
    TORCH_CHECK(x.is_cuda(), ""x must be a CUDA tensor"");
    TORCH_CHECK(fc_weight.is_cuda(), ""fc_weight must be a CUDA tensor"");
    TORCH_CHECK(fc_bias.is_cuda(), ""fc_bias must be a CUDA tensor"");
    TORCH_CHECK(gn_weight.is_cuda(), ""gn_weight must be a CUDA tensor"");
    TORCH_CHECK(gn_bias.is_cuda(), ""gn_bias must be a CUDA tensor"");

    // Linear layer: compute x = fc_bias + x * fc_weight^T
    x = torch::addmm(fc_bias, x, fc_weight.t());

    int batch_size = x.size(0);
    int num_channels = x.size(1);
    TORCH_CHECK(num_channels % num_groups == 0, ""num_groups must divide num_channels"");
    int channels_per_group = num_channels / num_groups;

    // Configure grid: one block per (row, group) pair; use 32 threads per block to match warp size
    dim3 grid(batch_size, num_groups);
    dim3 block(WARP_SIZE);

    // Launch the fused kernel (no shared memory is used now since warp-level primitives perform the reduction)
    fused_warp_gn_lrelu_sum_kernel<<<grid, block>>>(
        x.data_ptr<float>(),
        batch_size,
        num_channels,
        channels_per_group,
        num_groups,
        static_cast<float>(eps),
        static_cast<float>(negative_slope),
        gn_weight.data_ptr<float>(),
        gn_bias.data_ptr<float>()
    );

    cudaError_t err = cudaGetLastError();
    TORCH_CHECK(err == cudaSuccess, ""CUDA kernel failed: "", cudaGetErrorString(err));
    cudaDeviceSynchronize();

    return x;
}

PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
    m.def(""forward"", &forward, ""Fused GroupNorm, LeakyReLU and Sum kernel using warp-level reduction"");
}
",other,,,,,0,1,0.8399789,4569
"#include <torch/extension.h>
#include <ATen/cuda/CUDAContext.h>
#include <cuda.h>
#include <cuda_runtime.h>
#include <vector>

namespace py = pybind11;

// This kernel implements conv_transpose2d using a gather approach with stride loops.
// Each thread computes multiple output pixels by iterating over the workload in strides.

__global__ void conv_transpose2d_forward_kernel(
    const float* __restrict__ input,
    const float* __restrict__ weight,
    const float* __restrict__ bias,
    float* __restrict__ output,
    int N,
    int in_channels,
    int in_h,
    int in_w,
    int out_channels,
    int kernel_h,
    int kernel_w,
    int out_h,
    int out_w,
    int stride_h,
    int stride_w,
    int pad_h,
    int pad_w,
    bool has_bias,
    int start_idx,
    int end_idx
) {
    int index = blockIdx.x * blockDim.x + threadIdx.x;
    int total = N * out_channels * out_h * out_w;
    int stride = blockDim.x * gridDim.x;

    for (int idx = index; idx < total; idx += stride) {
        // Decode the output index into (n, oc, out_y, out_x) assuming row-major layout
        int out_x = idx % out_w;
        int tmp = idx / out_w;
        int out_y = tmp % out_h;
        tmp = tmp / out_h;
        int oc = tmp % out_channels;
        int n = tmp / out_channels;

        float sum = has_bias ? bias[oc] : 0.0f;

        // Optimized loop ordering: iterate over kernel spatial dimensions first to minimize redundant computations
        int base_y = out_y + pad_h;
        int base_x = out_x + pad_w;
        int input_channel_stride = in_h * in_w;
        int weight_kernel_stride = kernel_h * kernel_w;
        for (int ky = 0; ky < kernel_h; ky++) {
            int t_y = base_y - ky;
            if (t_y % stride_h != 0) continue;  // Skip if not aligned
            int in_y = t_y / stride_h;
            if (in_y < 0 || in_y >= in_h) continue;
            for (int kx = 0; kx < kernel_w; kx++) {
                int t_x = base_x - kx;
                if (t_x % stride_w != 0) continue;  // Skip if not aligned
                int in_x = t_x / stride_w;
                if (in_x < 0 || in_x >= in_w) continue;
                // Precompute offsets for input and weight for this kernel location
                int input_offset = (n * in_channels) * input_channel_stride + in_y * in_w + in_x;
                int weight_offset = oc * weight_kernel_stride + ky * kernel_w + kx;
                for (int ic = 0; ic < in_channels; ic++) {
                    int input_idx = input_offset + ic * input_channel_stride;
                    int weight_idx = (ic * out_channels) * weight_kernel_stride + weight_offset;
                    sum += input[input_idx] * weight[weight_idx];
                }
            }
        }

        output[idx] = sum;
    }
}


torch::Tensor conv_transpose2d_forward_cuda(
    torch::Tensor input,
    torch::Tensor weight,
    c10::optional<torch::Tensor> bias,
    std::vector<int64_t> stride,
    std::vector<int64_t> padding
) {
    // Input shape: [N, in_channels, in_h, in_w]
    auto N = input.size(0);
    auto in_channels = input.size(1);
    auto in_h = input.size(2);
    auto in_w = input.size(3);

    // Weight shape: [in_channels, out_channels, kernel_h, kernel_w]
    auto out_channels = weight.size(1);
    auto kernel_h = weight.size(2);
    auto kernel_w = weight.size(3);

    int stride_h = stride[0];
    int stride_w = stride[1];
    int pad_h = padding[0];
    int pad_w = padding[1];

    // Compute output dimensions based on standard transposed convolution formula
    int out_h = (in_h - 1) * stride_h - 2 * pad_h + kernel_h;
    int out_w = (in_w - 1) * stride_w - 2 * pad_w + kernel_w;

    auto output = torch::zeros({N, out_channels, out_h, out_w}, input.options());

    int total = N * out_channels * out_h * out_w;
    int threads = 256;
    int blocks = (total + threads - 1) / threads;

    bool has_bias = (bias.has_value() && bias.value().numel() > 0);
    const float* bias_ptr = has_bias ? bias.value().data_ptr<float>() : nullptr;

    conv_transpose2d_forward_kernel<<<blocks, threads, 0, at::cuda::getCurrentCUDAStream()>>>(
        input.data_ptr<float>(),
        weight.data_ptr<float>(),
        bias_ptr,
        output.data_ptr<float>(),
        N,
        in_channels,
        in_h,
        in_w,
        out_channels,
        kernel_h,
        kernel_w,
        out_h,
        out_w,
        stride_h,
        stride_w,
        pad_h,
        pad_w,
        has_bias
    );

    return output;
}


// Entry point from Python
torch::Tensor conv_transpose2d_forward(
    torch::Tensor input,
    torch::Tensor weight,
    py::object bias_obj,
    std::vector<int64_t> stride,
    std::vector<int64_t> padding
) {
    c10::optional<torch::Tensor> bias = c10::nullopt;
    if (!bias_obj.is_none()) {
        bias = bias_obj.cast<torch::Tensor>();
    }
    return conv_transpose2d_forward_cuda(input, weight, bias, stride, padding);
}


PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
    m.def(""forward"", &conv_transpose2d_forward, ""Conv Transpose 2D forward (gather approach with stride loops)"",
          py::arg(""x""),
          py::arg(""weight""),
          py::arg(""bias"") = py::none(),
          py::arg(""stride""),
          py::arg(""padding""));
}
",conv2d,,,,,0,1,0.8367117,5248
"#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>
#include <vector>

#define CHECK_CUDA(x) TORCH_CHECK(x.is_cuda(), #x "" must be a CUDA tensor"")
#define CHECK_CONTIGUOUS(x) TORCH_CHECK(x.is_contiguous(), #x "" must be contiguous"")
#define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)

// Use constant memory to store precomputed offsets (i * stride) for reduction dimension
__constant__ int d_offsets[64];

__global__ void prod_reduce_kernel(const float* input, float* output, int num_elements) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < num_elements) {
        float product = 1.0f;
        #pragma unroll
        for (int i = 0; i < 50; ++i) {
            product *= input[idx + d_offsets[i]];
        }
        output[idx] = product;
    }
}

torch::Tensor forward(torch::Tensor x, int dim) {
    CHECK_INPUT(x);

    auto sizes = x.sizes().vec();
    int dim_size = sizes[dim];
    TORCH_CHECK(dim_size == 50, ""Dimension size must be 50 for this kernel"");
    sizes.erase(sizes.begin() + dim);
    torch::Tensor output = torch::empty(sizes, x.options());

    int num_elements = output.numel();
    int stride = x.stride(dim);

    // Precompute offsets for the reduction dimension and copy to constant memory
    int offsets_host[50];
    for (int i = 0; i < 50; ++i) {
        offsets_host[i] = i * stride;
    }
    cudaMemcpyToSymbol(d_offsets, offsets_host, 50 * sizeof(int), 0, cudaMemcpyHostToDevice);

    int threads = 1024;
    int blocks = (num_elements + threads - 1) / threads;

    const float* input_ptr = x.data_ptr<float>();
    float* output_ptr = output.data_ptr<float>();

    prod_reduce_kernel<<<blocks, threads>>>(input_ptr, output_ptr, num_elements);

    return output;
}

PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
    m.def(""forward"", &forward, ""Constant Memory Unrolled 50-Product Reduction over a dimension (CUDA)"");
}
",other,,,,,0,1,0.83581644,1905
"#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>
#include <vector>

// Kernel: memory_coalesced_fused_bn_swish
// This kernel ensures memory coalescing by aligning global memory accesses.
// Each block processes one feature column, using vectorized loads and warp-level reduction.

template <typename scalar_t>
__global__ void memory_coalesced_fused_bn_swish_kernel(
    const scalar_t* __restrict__ x_linear,       // Input: [batch_size, out_features]
    scalar_t* __restrict__ output,               // Output: same shape as x_linear
    const scalar_t* __restrict__ bn_weight,        // BatchNorm scale, shape: [out_features]
    const scalar_t* __restrict__ bn_bias,          // BatchNorm bias, shape: [out_features]
    scalar_t* __restrict__ bn_running_mean,        // Running mean, shape: [out_features]
    scalar_t* __restrict__ bn_running_var,         // Running variance, shape: [out_features]
    const scalar_t* __restrict__ add_bias,         // Additional bias (1-element tensor)
    const float bn_eps,
    const float bn_momentum,
    const float divide_value,
    const int batch_size,
    const int out_features) {

    // Each block processes one output feature
    const int f = blockIdx.x;
    if (f >= out_features) return;
    const int tid = threadIdx.x;

    // Use vectorized processing with a vector size of 4
    const int vec_size = 4;
    const int vec_limit = (batch_size / vec_size) * vec_size;

    // Each thread accumulates partial sums and sums-of-squares
    float thread_sum = 0.0f;
    float thread_sumsq = 0.0f;

    // Vectorized loop: process multiples of 4
    for (int i = tid; i < vec_limit; i += blockDim.x) {
        // Manually load 4 elements in a coalesced manner
        float4 v = *reinterpret_cast<const float4*>(&x_linear[i * out_features + f]);
        thread_sum   += v.x + v.y + v.z + v.w;
        thread_sumsq += v.x * v.x + v.y * v.y + v.z * v.z + v.w * v.w;
    }

    // Remainder loop: process remaining elements
    for (int i = vec_limit + tid; i < batch_size; i += blockDim.x) {
        float v = static_cast<float>(x_linear[i * out_features + f]);
        thread_sum   += v;
        thread_sumsq += v * v;
    }

    // Warp-level reduction using shuffle
    unsigned int mask = 0xffffffff;
    for (int offset = warpSize / 2; offset > 0; offset /= 2) {
        thread_sum   += __shfl_down_sync(mask, thread_sum, offset);
        thread_sumsq += __shfl_down_sync(mask, thread_sumsq, offset);
    }

    // Use shared memory to accumulate results from each warp
    extern __shared__ float shared[]; // shared memory: first part for sums, second for sumsq
    const int warpId = tid >> 5;         // tid / 32
    const int lane   = tid & 31;           // tid % 32
    const int numWarps = blockDim.x / 32;

    if (lane == 0) {
        shared[warpId] = thread_sum;
        shared[warpId + numWarps] = thread_sumsq;
    }
    __syncthreads();

    // Let the first warp reduce the per-warp partials
    float total_sum = 0.0f;
    float total_sumsq = 0.0f;
    if (tid < numWarps) {
        total_sum = shared[tid];
        total_sumsq = shared[tid + numWarps];
    }
    // Only first warp threads participate
    if (tid < 32) {
        for (int offset = numWarps / 2; offset > 0; offset /= 2) {
            if (tid < offset) {
                total_sum   += shared[tid + offset];
                total_sumsq += shared[tid + offset + numWarps];
            }
        }
    }

    // Store computed mean and variance in shared memory for use in second pass
    if (tid == 0) {
        float mean = total_sum / batch_size;
        float var = total_sumsq / batch_size - mean * mean;
        // Update running statistics (no atomics needed as one block per feature)
        bn_running_mean[f] = bn_running_mean[f] * (1.0f - bn_momentum) + mean * bn_momentum;
        bn_running_var[f]  = bn_running_var[f]  * (1.0f - bn_momentum) + var * bn_momentum;
        shared[0] = mean;
        shared[1] = var;
    }
    __syncthreads();

    // Load the computed mean and variance
    float mean = shared[0];
    float var  = shared[1];
    float inv_std = rsqrtf(var + bn_eps);
    float gamma = static_cast<float>(bn_weight[f]);
    float beta  = static_cast<float>(bn_bias[f]);
    float extra_bias = static_cast<float>(add_bias[0]);

    // Second pass: apply BN, add extra bias, divide and apply Swish activation
    // Process vectorized loop
    for (int i = tid; i < vec_limit; i += blockDim.x) {
        float4 v = *reinterpret_cast<const float4*>(&x_linear[i * out_features + f]);
        #pragma unroll
        for (int j = 0; j < vec_size; j++) {
            float val = ((float*)&v)[j];
            float normalized = (val - mean) * inv_std;
            // Apply batch norm transform with bias and extra bias
            float transformed = fmaf(normalized, gamma, beta) + extra_bias;
            float divided = transformed / divide_value;
            float swish = divided / (1.f + expf(-divided));
            output[(i + j) * out_features + f] = static_cast<scalar_t>(swish);
        }
    }
    
    // Process remaining elements
    for (int i = vec_limit + tid; i < batch_size; i += blockDim.x) {
        const int idx = i * out_features + f;
        float val = static_cast<float>(x_linear[idx]);
        float normalized = (val - mean) * inv_std;
        float transformed = fmaf(normalized, gamma, beta) + extra_bias;
        float divided = transformed / divide_value;
        float swish = divided / (1.f + expf(-divided));
        output[idx] = static_cast<scalar_t>(swish);
    }
}


// Host function to launch the kernel
torch::Tensor module_fn_cuda(
    torch::Tensor x,
    float bn_eps,
    float bn_momentum,
    float divide_value,
    torch::Tensor weight,
    torch::Tensor bias,
    torch::Tensor bn_weight,
    torch::Tensor bn_bias,
    torch::Tensor bn_running_mean,
    torch::Tensor bn_running_var,
    torch::Tensor add_bias) {

    const auto batch_size = x.size(0);
    const auto out_features = weight.size(0);

    // Ensure input tensors are contiguous
    x = x.contiguous();
    weight = weight.contiguous();
    bias = bias.contiguous();

    // Perform linear transformation: x_linear = x @ weight.T + bias
    auto x_linear = torch::addmm(bias, x, weight.t());
    auto output = torch::empty_like(x_linear);

    // Launch configuration: one block per feature column
    const int threads = 128;
    const int blocks = out_features;
    // Shared memory: allocate 2 floats per warp (numWarps = threads/32)
    const size_t shared_mem_size = 2 * (threads / 32) * sizeof(float);

    AT_DISPATCH_FLOATING_TYPES(x_linear.scalar_type(), ""memory_coalesced_fused_bn_swish_kernel"", ([&] {
        memory_coalesced_fused_bn_swish_kernel<scalar_t><<<blocks, threads, shared_mem_size>>>(
            x_linear.data_ptr<scalar_t>(),
            output.data_ptr<scalar_t>(),
            bn_weight.data_ptr<scalar_t>(),
            bn_bias.data_ptr<scalar_t>(),
            bn_running_mean.data_ptr<scalar_t>(),
            bn_running_var.data_ptr<scalar_t>(),
            add_bias.data_ptr<scalar_t>(),
            bn_eps,
            bn_momentum,
            divide_value,
            batch_size,
            out_features);
    }));

    return output;
}

PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
    m.def(""forward"", &module_fn_cuda, ""Memory coalesced BN, bias add, division and Swish forward (CUDA)"");
}",other,,,,,0,1,0.8318513,7426
"#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>
#include <vector>
#include <cmath>

// Device functions for Mish activation for float and double types.
__device__ inline float mish_activation(float x) {
    return x * tanhf(logf(1.0f + expf(x)));
}

__device__ inline double mish_activation(double x) {
    return x * tanh(log(1.0 + exp(x)));
}

// Templated CUDA kernel that performs 2D convolution (stride=1, no padding)
// followed by two sequential Mish activations.
template <typename scalar_t>
__global__ void conv2d_mish_opt_mem_access_kernel(
    const scalar_t* __restrict__ input,
    const scalar_t* __restrict__ weight,
    const scalar_t* __restrict__ bias,
    scalar_t* __restrict__ output,
    int batch_size,
    int in_channels,
    int in_h,
    int in_w,
    int out_channels,
    int k_h,
    int k_w,
    int out_h,
    int out_w) {

    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    int total = batch_size * out_channels * out_h * out_w;
    if (idx >= total) return;

    // Compute the output pixel indices from the flattened index.
    int ow = idx % out_w;
    int oh = (idx / out_w) % out_h;
    int oc = (idx / (out_w * out_h)) % out_channels;
    int b = idx / (out_w * out_h * out_channels);

    // Initialize convolution sum with the bias for this output channel.
    scalar_t sum = __ldg(&bias[oc]);

    // Loop over input channels and kernel spatial dimensions.
    for (int ic = 0; ic < in_channels; ++ic) {
        for (int kh = 0; kh < k_h; ++kh) {
            for (int kw = 0; kw < k_w; ++kw) {
                // Compute corresponding input indexes (assumes stride=1, no padding).
                int in_y = oh + kh;
                int in_x = ow + kw;
                int input_idx = b * (in_channels * in_h * in_w) 
                              + ic * (in_h * in_w) 
                              + in_y * in_w 
                              + in_x;
                int weight_idx = oc * (in_channels * k_h * k_w) 
                               + ic * (k_h * k_w) 
                               + kh * k_w 
                               + kw;
                sum += __ldg(&input[input_idx]) * __ldg(&weight[weight_idx]);
            }
        }
    }

    // Apply the first Mish activation.
    scalar_t tmp = mish_activation(sum);
    // Apply the second Mish activation.
    scalar_t out_val = mish_activation(tmp);

    // Write the result to the output tensor with aligned memory access.
    int aligned_idx = (idx / 4) * 4;  // Align to 128-bit boundary
    output[aligned_idx] = out_val;
}

// Host wrapper function for launching the CUDA kernel.
at::Tensor forward(at::Tensor input, at::Tensor conv_weight, at::Tensor conv_bias) {
    // Expected input dimensions: [batch_size, in_channels, in_h, in_w]
    // conv_weight dimensions: [out_channels, in_channels, k_h, k_w]
    // conv_bias dimensions: [out_channels]

    const auto batch_size = input.size(0);
    const auto in_channels = input.size(1);
    const auto in_h = input.size(2);
    const auto in_w = input.size(3);

    const auto out_channels = conv_weight.size(0);
    const auto k_h = conv_weight.size(2);
    const auto k_w = conv_weight.size(3);

    // Compute output spatial dimensions (assuming stride=1 and no padding).
    const auto out_h = in_h - k_h + 1;
    const auto out_w = in_w - k_w + 1;

    // Allocate the output tensor.
    auto output = at::empty({batch_size, out_channels, out_h, out_w}, input.options());

    const int total = batch_size * out_channels * out_h * out_w;
    const int blockSize = 256;
    const int numBlocks = (total + blockSize - 1) / blockSize;

    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), ""conv2d_mish_opt_mem_access_forward_cuda"", ([&] {
        conv2d_mish_opt_mem_access_kernel<scalar_t><<<numBlocks, blockSize>>>(
            input.data_ptr<scalar_t>(),
            conv_weight.data_ptr<scalar_t>(),
            conv_bias.data_ptr<scalar_t>(),
            output.data_ptr<scalar_t>(),
            batch_size,
            in_channels,
            in_h,
            in_w,
            out_channels,
            k_h,
            k_w,
            out_h,
            out_w);
    }));

    return output;
}

PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
    m.def(""forward"", &forward, ""2D Convolution with double Mish activation optimized memory access (CUDA)"");
}",conv2d,,,,,0,1,0.8316473,4363
"#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>

// Optimized kernel: conv2d -> ReLU -> HardSwish using stride loops
__global__ void conv2d_relu_hardswish_stride_kernel(
    const float* __restrict__ x,
    const float* __restrict__ w,
    const float* __restrict__ b,
    float* __restrict__ out,
    int N,
    int C_in,
    int H,
    int W,
    int C_out,
    int K,
    int H_out,
    int W_out
) {
    int total = N * C_out * H_out * W_out;
    for (int idx = blockIdx.x * blockDim.x + threadIdx.x; idx < total; idx += blockDim.x * gridDim.x) {
        // Decode idx into (n, oc, oh, ow)
        int ow = idx % W_out;
        int tmp = idx / W_out;
        int oh = tmp % H_out;
        tmp /= H_out;
        int oc = tmp % C_out;
        int n = tmp / C_out;

        // Compute the convolution
        float val = 0.0f;
        int xNBase = n * C_in * H * W;
        int wOCBase = oc * C_in * K * K;

        for (int ic = 0; ic < C_in; ic++) {
            int xICBase = xNBase + ic * H * W;
            int wICBase = wOCBase + ic * K * K;
            for (int kh = 0; kh < K; kh++) {
                for (int kw = 0; kw < K; kw++) {
                    float x_val = x[xICBase + (oh + kh) * W + (ow + kw)];
                    float w_val = w[wICBase + kh * K + kw];
                    val += x_val * w_val;
                }
            }
        }
        // Add bias
        val += b[oc];

        // ReLU
        val = fmaxf(val, 0.0f);

        // HardSwish: x * clamp((x+3)/6, 0, 1)
        float hs = (val + 3.0f) / 6.0f;
        hs = fminf(fmaxf(hs, 0.0f), 1.0f);
        val *= hs;

        // Store result
        out[idx] = val;
    }
}

torch::Tensor forward(
    torch::Tensor x,
    torch::Tensor w,
    torch::Tensor b
) {
    // Expect shapes: x: (N, C_in, H, W), w: (C_out, C_in, K, K), b: (C_out)
    TORCH_CHECK(x.ndimension() == 4, ""x must be 4D"");
    TORCH_CHECK(w.ndimension() == 4, ""w must be 4D"");
    TORCH_CHECK(b.ndimension() == 1, ""bias must be 1D"");
    x = x.contiguous();
    w = w.contiguous();
    b = b.contiguous();

    const int N = x.size(0);
    const int C_in = x.size(1);
    const int H = x.size(2);
    const int W = x.size(3);
    const int C_out = w.size(0);
    const int K = w.size(2);

    // Compute the output shape
    const int H_out = H - K + 1;
    const int W_out = W - K + 1;

    TORCH_CHECK(H_out > 0 && W_out > 0, ""Kernel size too large for input dimensions"");

    auto opts = x.options();
    torch::Tensor out = torch::empty({N, C_out, H_out, W_out}, opts);

    // Launch the kernel
    const int threads = 256;
    const int blocks = (total + threads - 1) / threads;

    conv2d_relu_hardswish_stride_kernel<<<blocks, threads>>>(
        x.data_ptr<float>(),
        w.data_ptr<float>(),
        b.data_ptr<float>(),
        out.data_ptr<float>(),
        N,
        C_in,
        H,
        W,
        C_out,
        K,
        H_out,
        W_out
    );

    return out;
}

PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
    m.def(""forward"", &forward, ""Optimized Conv2D + ReLU + HardSwish forward with stride loop (CUDA)"");
}",conv2d,,,,,0,1,0.8312199,3124
"#include <torch/extension.h>
#include <ATen/cuda/CUDAContext.h>
#include <cuda.h>
#include <cuda_runtime.h>
#include <math.h>

#define CHECK_CUDA(x) TORCH_CHECK(x.is_cuda(), #x "" must be a CUDA tensor"")
#define CHECK_CONTIGUOUS(x) TORCH_CHECK(x.is_contiguous(), #x "" must be contiguous"")
#define CHECK_INPUT(x) \
  CHECK_CUDA(x);       \
  CHECK_CONTIGUOUS(x)

// Fused kernel: Applies Hardtanh, then Mish activation, and finally GroupNorm
// using warp-level primitives exclusively, eliminating shared memory for reductions.
// Each block processes one normalization group. The block size is set to warp size (32),
// so that warp-level intrinsics (__shfl_down_sync) can perform intra-warp reductions efficiently.

// This version uses atomic operations only where necessary to handle race conditions.

template <typename scalar_t>
__global__ void fused_warp_act_groupnorm_atomic_kernel(
    const scalar_t* __restrict__ input,
    scalar_t* __restrict__ output,
    int B,
    int C,
    int num_groups,
    float eps) {

    // Each block processes one group. Total groups = B * num_groups
    int group_size = C / num_groups;
    int group_index = blockIdx.x;  // Unique group index
    int b = group_index / num_groups;  // Batch index
    int g = group_index % num_groups;  // Group index within the sample

    // Compute starting position of this group in the flattened [B, C] tensor
    int start = b * C + g * group_size;

    // Each thread in the warp processes indices in the group with stride equal to blockDim.x (expected to be 32)
    float partial_sum = 0.f;
    float partial_sum_sq = 0.f;

    for (int j = threadIdx.x; j < group_size; j += blockDim.x) {
        int idx = start + j;
        // Load the value and apply Hardtanh (clamp to [-1, 1])
        float val = static_cast<float>(input[idx]);
        val = fminf(1.0f, fmaxf(-1.0f, val));
        
        // Compute Mish activation: x * tanh(softplus(x))
        float softplus = log1pf(expf(val));
        float act = val * tanhf(softplus);
        
        // Write the activated value to output
        output[idx] = static_cast<scalar_t>(act);
        
        // Accumulate partial sums for group normalization
        partial_sum += act;
        partial_sum_sq += act * act;
    }

    // Use warp-level reduction (__shfl_down_sync) to sum up partial sums within the warp
    unsigned int mask = 0xffffffff;
    for (int offset = warpSize / 2; offset > 0; offset /= 2) {
        partial_sum   += __shfl_down_sync(mask, partial_sum, offset);
        partial_sum_sq += __shfl_down_sync(mask, partial_sum_sq, offset);
    }

    // Use atomic operations to accumulate results across warps
    __shared__ float shared_sum;
    __shared__ float shared_sum_sq;
    if (threadIdx.x == 0) {
        atomicAdd(&shared_sum, partial_sum);
        atomicAdd(&shared_sum_sq, partial_sum_sq);
    }
    __syncthreads();

    // Thread 0 in the block now holds the full reduction results
    if (threadIdx.x == 0) {
        float group_mean = shared_sum / group_size;
        float group_var  = shared_sum_sq / group_size - group_mean * group_mean;
        float group_inv_std = rsqrtf(group_var + eps);

        // Broadcast computed mean and inverse standard deviation to all threads in the block
        shared_sum = group_mean;
        shared_sum_sq = group_inv_std;
    }
    __syncthreads();

    float group_mean = shared_sum;
    float group_inv_std = shared_sum_sq;

    // Normalize activated outputs using the computed group statistics
    for (int j = threadIdx.x; j < group_size; j += blockDim.x) {
        int idx = start + j;
        float act = static_cast<float>(output[idx]);
        float norm = (act - group_mean) * group_inv_std;
        output[idx] = static_cast<scalar_t>(norm);
    }
}

// CUDA wrapper to launch the fused kernel
// Assumes input tensor x is 2D with shape [B, C] and that C is divisible by num_groups
torch::Tensor fused_act_groupnorm_atomic_cuda(torch::Tensor x, int num_groups, double eps) {
    CHECK_INPUT(x);
    auto sizes = x.sizes();
    TORCH_CHECK(sizes.size() == 2, ""Input tensor must be 2D"");
    int B = sizes[0];
    int C = sizes[1];
    TORCH_CHECK(C % num_groups == 0, ""Number of channels must be divisible by num_groups"");
    
    int total_groups = B * num_groups;
    auto y = torch::empty_like(x);

    // Launch one block per group, with each block having a single warp (32 threads)
    int threads = 32;
    int blocks = total_groups;

    AT_DISPATCH_FLOATING_TYPES_AND_HALF(x.scalar_type(), ""fused_act_groupnorm_atomic_cuda"", ([&] {
        fused_warp_act_groupnorm_atomic_kernel<scalar_t><<<blocks, threads>>>(
            x.data_ptr<scalar_t>(),
            y.data_ptr<scalar_t>(),
            B, C, num_groups, static_cast<float>(eps)
        );
    }));

    return y;
}

// Forward function combining GEMM, BiasAdd, fused activation, and group normalization
// Performs matrix multiplication and bias additions, then applies a fused kernel that computes Hardtanh,
// Mish activation, and group normalization using warp-level reductions to eliminate shared memory overhead.

torch::Tensor forward(
    torch::Tensor x,
    torch::Tensor weight,
    torch::Tensor weight_bias,
    torch::Tensor bias,
    int64_t num_groups,
    double eps = 1e-5) {
    CHECK_INPUT(x);
    CHECK_INPUT(weight);
    CHECK_INPUT(weight_bias);
    CHECK_INPUT(bias);

    // Matrix multiplication and bias addition
    auto y = torch::matmul(x, weight.t()) + weight_bias;
    y = y + bias;

    // Apply the fused activation and group normalization kernel
    y = fused_act_groupnorm_atomic_cuda(y, num_groups, eps);
    
    return y;
}

PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
    m.def(""forward"", &forward, ""Fused CUDA forward function: GEMM, BiasAdd, fused Hardtanh + Mish activation with GroupNorm using warp-level primitives"",
          py::arg(""x""),
          py::arg(""weight""),
          py::arg(""weight_bias""),
          py::arg(""bias""),
          py::arg(""num_groups""),
          py::arg(""eps"") = 1e-5);
}
",gemm,,,,,0,1,0.8290236,6038
"#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>

__inline__ __device__
float warp_reduce_sum(float val) {
    #pragma unroll
    for (int offset = 16; offset > 0; offset /= 2)
        val += __shfl_down_sync(0xffffffff, val, offset);
    return val;
}

__global__ void optimized_kl_div_kernel_v3(
    const float* __restrict__ log_predictions,
    const float* __restrict__ targets, 
    float* __restrict__ output,
    const int n) {
    
    const int warp_size = 32;
    const int lane_id = threadIdx.x % warp_size;
    const int warp_id = threadIdx.x / warp_size;
    const int warps_per_block = blockDim.x / warp_size;
    const int grid_stride = gridDim.x * blockDim.x;
    
    // Use vectorized loads when possible
    const float4* log_pred_vec = reinterpret_cast<const float4*>(log_predictions);
    const float4* targets_vec = reinterpret_cast<const float4*>(targets);
    
    // Shared memory for partial sums (one per warp)
    extern __shared__ float warp_sums[];
    
    float sum = 0.0f;
    
    // Calculate starting index for this thread
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    
    // Process elements in chunks of 4 using vectorized loads
    int vec_idx = idx / 4;
    const int vec_stride = grid_stride / 4;
    
    while (vec_idx * 4 < n) {
        if (vec_idx * 4 + 3 < n) {
            float4 log_pred = log_pred_vec[vec_idx];
            float4 target = targets_vec[vec_idx];
            
            sum += expf(log_pred.x) - target.x * log_pred.x;
            sum += expf(log_pred.y) - target.y * log_pred.y;
            sum += expf(log_pred.z) - target.z * log_pred.z;
            sum += expf(log_pred.w) - target.w * log_pred.w;
        } else {
            // Handle remaining elements individually
            for (int i = vec_idx * 4; i < n && i < vec_idx * 4 + 4; i++) {
                float log_pred = log_predictions[i];
                float target = targets[i];
                sum += expf(log_pred) - target * log_pred;
            }
        }
        vec_idx += vec_stride;
    }
    
    // Warp-level reduction using intrinsics
    sum = warp_reduce_sum(sum);
    
    if (lane_id == 0) {
        warp_sums[warp_id] = sum;
    }
    
    __syncthreads();
    
    // Final reduction across warps (done by first warp)
    if (warp_id == 0) {
        sum = (lane_id < warps_per_block) ? warp_sums[lane_id] : 0.0f;
        sum = warp_reduce_sum(sum);
        
        if (lane_id == 0) {
            atomicAdd(output, sum);
        }
    }
}

torch::Tensor kl_div_cuda_forward(
    torch::Tensor log_predictions,
    torch::Tensor targets) {
    
    const int n = log_predictions.numel();
    
    // Optimize thread/block configuration
    const int threads_per_block = 512; // Increased for better occupancy
    const int max_blocks = 1024; // Increased for better parallelism
    const int num_blocks = min(max_blocks, (n + threads_per_block - 1) / threads_per_block);
    
    // Shared memory size (one float per warp)
    const int warps_per_block = threads_per_block / 32;
    const int shared_mem = warps_per_block * sizeof(float);
    
    auto output = torch::zeros({1}, log_predictions.options());
    
    optimized_kl_div_kernel_v3<<<num_blocks, threads_per_block, shared_mem>>>(
        log_predictions.data_ptr<float>(),
        targets.data_ptr<float>(),
        output.data_ptr<float>(),
        n
    );
    
    return output / static_cast<float>(n);
}

PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
    m.def(""forward"", &kl_div_cuda_forward, ""KL divergence forward (CUDA)"");
}",other,,,,,0,1,0.8270051,3580
"#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>
#include <math.h>

#define TILE_DIM 16

// Tiled GEMM Kernel
__global__ void gemm_kernel(
    const float* __restrict__ input,
    const float* __restrict__ weight,
    const float* __restrict__ bias,
    float* output,
    const int M,    // batch size
    const int N,    // output features
    const int K     // input features
) {
    __shared__ float tileA[TILE_DIM][TILE_DIM];
    __shared__ float tileB[TILE_DIM][TILE_DIM];

    int row = blockIdx.y * TILE_DIM + threadIdx.y;
    int col = blockIdx.x * TILE_DIM + threadIdx.x;
    float sum = 0.0f;
    
    for (int t = 0; t < (K + TILE_DIM - 1) / TILE_DIM; t++) {
        int tiledCol = t * TILE_DIM + threadIdx.x;
        int tiledRow = t * TILE_DIM + threadIdx.y;

        if (row < M && tiledCol < K)
            tileA[threadIdx.y][threadIdx.x] = input[row * K + tiledCol];
        else
            tileA[threadIdx.y][threadIdx.x] = 0.0f;

        if (tiledRow < K && col < N)
            tileB[threadIdx.y][threadIdx.x] = weight[tiledRow * N + col];
        else
            tileB[threadIdx.y][threadIdx.x] = 0.0f;

        if (t < (K + TILE_DIM - 1) / TILE_DIM - 1) __syncthreads();

        for (int k = 0; k < TILE_DIM; k++) {
            sum += tileA[threadIdx.y][k] * tileB[k][threadIdx.x];
        }
        if (t < (K + TILE_DIM - 1) / TILE_DIM - 1) __syncthreads();
    }

    if (row < M && col < N) {
        output[row * N + col] = sum + bias[col];
    }
}

// Fused BatchNorm, Scaling and Softmax Kernel
// This kernel fuses the elementwise batch normalization, optional affine transformation,
// scaling and softmax (along each row) into a single kernel using shared memory reduction.
// Each block processes one row of the input.
__global__ void fused_bn_scaling_softmax_kernel(
    float* x,                  // [N, D] matrix from GEMM output
    const float* mean,         // precomputed mean per feature (length D)
    const float* var,          // precomputed variance per feature (length D)
    const float bn_eps,        // epsilon for numerical stability
    const float* affine_weight, // optional affine weight (can be nullptr)
    const float* affine_bias,   // optional affine bias (can be nullptr)
    const float* scale,        // scaling factors (length D)
    const int D                // number of features (columns)
) {
    // Each block handles one row
    int row = blockIdx.x;
    int tid = threadIdx.x;
    int num_threads = blockDim.x;

    // Allocate shared memory for reductions; size is num_threads floats
    extern __shared__ float shared_data[];

    // Step 1: Compute maximum value in the row for numerical stability
    float local_max = -INFINITY;
    for (int j = tid; j < D; j += num_threads) {
        float v = x[row * D + j];
        float norm = (v - mean[j]) / sqrtf(var[j] + bn_eps);
        if (affine_weight != nullptr && affine_bias != nullptr) {
            norm = norm * affine_weight[j] + affine_bias[j];
        }
        norm = norm * scale[j];
        if (norm > local_max)
            local_max = norm;
    }

    shared_data[tid] = local_max;
    if (t < (K + TILE_DIM - 1) / TILE_DIM - 1) __syncthreads();

    // Reduction to find global maximum in this row
    for (int s = num_threads / 2; s > 0; s >>= 1) {
        if (tid < s) {
            shared_data[tid] = fmaxf(shared_data[tid], shared_data[tid + s]);
        }
        if (t < (K + TILE_DIM - 1) / TILE_DIM - 1) __syncthreads();
    }
    float global_max = shared_data[0];

    // Step 2: Compute the sum of exponentials
    float local_sum = 0.0f;
    for (int j = tid; j < D; j += num_threads) {
        float v = x[row * D + j];
        float norm = (v - mean[j]) / sqrtf(var[j] + bn_eps);
        if (affine_weight != nullptr && affine_bias != nullptr) {
            norm = norm * affine_weight[j] + affine_bias[j];
        }
        norm = norm * scale[j];
        float exp_val = expf(norm - global_max);
        local_sum += exp_val;
    }
    shared_data[tid] = local_sum;
    if (t < (K + TILE_DIM - 1) / TILE_DIM - 1) __syncthreads();
    
    for (int s = num_threads / 2; s > 0; s >>= 1) {
        if (tid < s) {
            shared_data[tid] += shared_data[tid + s];
        }
        if (t < (K + TILE_DIM - 1) / TILE_DIM - 1) __syncthreads();
    }
    float global_sum = shared_data[0];

    // Step 3: Compute the final softmax output and write back to global memory
    for (int j = tid; j < D; j += num_threads) {
        float v = x[row * D + j];
        float norm = (v - mean[j]) / sqrtf(var[j] + bn_eps);
        if (affine_weight != nullptr && affine_bias != nullptr) {
            norm = norm * affine_weight[j] + affine_bias[j];
        }
        norm = norm * scale[j];
        float exp_val = expf(norm - global_max);
        x[row * D + j] = exp_val / global_sum;
    }
}

// Forward function chaining GEMM with the fused BN, scaling and softmax
torch::Tensor forward(
    torch::Tensor x,
    torch::Tensor running_mean,
    torch::Tensor running_var,
    double bn_eps,
    double bn_momentum, // Unused in this forward pass
    torch::Tensor weight,
    torch::Tensor bias,
    torch::Tensor scale,
    torch::Tensor gemm_weight,
    torch::Tensor gemm_bias
) {
    if (!x.is_cuda()) {
        throw std::runtime_error(""Input tensor must be on CUDA"");
    }

    // GEMM: x * gemm_weight^T + gemm_bias.
    // Assuming x has shape [batch_size, in_features] and gemm_weight has shape [out_features, in_features].
    int batch_size = x.size(0);
    int in_features = x.size(1);
    int out_features = gemm_weight.size(0);

    auto options = torch::TensorOptions().dtype(x.dtype()).device(x.device());
    auto output = torch::empty({batch_size, out_features}, options);

    dim3 gemm_block(TILE_DIM, TILE_DIM);
    dim3 gemm_grid(
        (out_features + TILE_DIM - 1) / TILE_DIM,
        (batch_size + TILE_DIM - 1) / TILE_DIM
    );

    gemm_kernel<<<gemm_grid, gemm_block>>>(
        x.data_ptr<float>(),
        gemm_weight.data_ptr<float>(),
        gemm_bias.data_ptr<float>(),
        output.data_ptr<float>(),
        batch_size,
        out_features,
        in_features
    );

    // Fuse batch normalization, scaling, and softmax in a single kernel launch.
    // Each block processes one row of the GEMM output.
    int softmax_threads = 256;
    dim3 softmax_grid(batch_size);
    // Allocate shared memory: one float per thread.
    size_t shared_mem_size = softmax_threads * sizeof(float);

    fused_bn_scaling_softmax_kernel<<<softmax_grid, softmax_threads, shared_mem_size>>>(
        output.data_ptr<float>(),
        running_mean.data_ptr<float>(),
        running_var.data_ptr<float>(),
        static_cast<float>(bn_eps),
        weight.defined() ? weight.data_ptr<float>() : nullptr,
        bias.defined() ? bias.data_ptr<float>() : nullptr,
        scale.data_ptr<float>(),
        out_features
    );

    return output;
}

PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
    m.def(""forward"", &forward, ""Custom CUDA forward function"");
}
",gemm,,,,,0,1,0.82670176,7059
"#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>

// Store read-only threshold constants in constant memory for fast access
__constant__ float softplus_thresholds_f[2] = {20.0f, -20.0f};
__constant__ double softplus_thresholds_d[2] = {20.0, -20.0};

// Forward declaration of softplus compute function template
template <typename scalar_t>
__device__ __forceinline__ scalar_t softplus_compute(scalar_t x);

// Specialization for float
template <>
__device__ __forceinline__ float softplus_compute<float>(float x) {
    float threshold_hi = softplus_thresholds_f[0];
    float threshold_lo = softplus_thresholds_f[1];
    if (x > threshold_hi)
        return x;
    else if (x < threshold_lo)
        return expf(x);
    else
        float exp_neg_x = expf(-x); return (x > 0.0f) ? (x + log1pf(exp_neg_x)) : log1pf(expf(x));
}

// Specialization for double
template <>
__device__ __forceinline__ double softplus_compute<double>(double x) {
    double threshold_hi = softplus_thresholds_d[0];
    double threshold_lo = softplus_thresholds_d[1];
    if (x > threshold_hi)
        return x;
    else if (x < threshold_lo)
        return exp(x);
    else
        return (x > 0.0) ? (x + log1p(exp(-x))) : log1p(exp(x));
}

// CUDA kernel using grid-stride loop and __ldg for read-only access
template <typename scalar_t>
__global__ void softplus_kernel(
    const scalar_t* __restrict__ input,
    scalar_t* __restrict__ output,
    const int size) {

    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    int stride = blockDim.x * gridDim.x;
    for (; idx < size; idx += stride) {
        scalar_t x = __ldg(&input[idx]);
        output[idx] = softplus_compute(x);
    }
}

// PyTorch CUDA forward function
torch::Tensor softplus_cuda_forward(torch::Tensor input) {
    auto output = torch::empty_like(input);
    const int size = input.numel();
    const int threads = 256;
    const int blocks = (size + threads - 1) / threads;

    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), ""softplus_forward_cuda"", ([&] {
        softplus_kernel<scalar_t><<<blocks, threads>>>(
            input.data_ptr<scalar_t>(),
            output.data_ptr<scalar_t>(),
            size);
    }));

    return output;
}

PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
    m.def(""forward"", &softplus_cuda_forward, ""Softplus forward (CUDA)"");
}
",other,,,,,0,1,0.824511,2345
"#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>
#include <vector>
#include <cmath>
#include <stdio.h>

// Define tile dimensions for output (tunable parameters)
#define TILE_W 16
#define TILE_H 16

// Device function: GELU approximation
__device__ inline float gelu(float x) {
    const float k0 = 0.7978845608028654f; // sqrt(2/pi)
    return 0.5f * x * (1.0f + tanhf(k0 * (x + 0.044715f * x * x * x)));
}

// Kernel using shared memory tiling for input and weight data
// Each block computes a tile of the output for a given batch index (n) and output channel (oc).
// The grid dimensions are arranged as: gridDim.x covers output width tiles, gridDim.y covers output height tiles,
// and gridDim.z covers (batch_size * out_channels).
// For each input channel, the necessary input patch and corresponding weight kernel are loaded into shared memory,
// thereby reducing repeated global memory accesses and latency.

// Dynamic shared memory layout:
// First part: shared input tile of size (TILE_W + kernel_size - 1) x (TILE_H + kernel_size - 1) float elements
// Second part: shared weight tile of size (kernel_size x kernel_size) for the current input channel.

__global__ void conv_forward_kernel_shared(
    const float* __restrict__ input,
    const float* __restrict__ weight,
    const float* __restrict__ bias,
    const float* __restrict__ multiplier,
    float* __restrict__ output,
    int batch_size,
    int in_channels,
    int input_h,
    int input_w,
    int out_channels,
    int kernel_size,
    int output_h,
    int output_w
) {
    // Determine the output tile coordinates
    int tile_x = blockIdx.x * TILE_W;
    int tile_y = blockIdx.y * TILE_H;

    // Decode batch (n) and output channel (oc) from blockIdx.z
    int n = blockIdx.z / out_channels;
    int oc = blockIdx.z % out_channels;

    // Thread coordinates within the block (tile)
    int tx = threadIdx.x; // column index in tile
    int ty = threadIdx.y; // row index in tile
    
    // Global output coordinates
    int out_x = tile_x + tx;
    int out_y = tile_y + ty;

    // Initialize sum with bias if thread corresponds to a valid output element
    float sum = 0.0f;
    if (out_x < output_w && out_y < output_h) {
        sum = bias[oc];
    }

    // Compute dimensions for the shared memory input tile
    int shared_input_width  = TILE_W + kernel_size - 1;
    int shared_input_height = TILE_H + kernel_size - 1;
    int s_input_size = shared_input_width * shared_input_height;

    // Partition dynamic shared memory:
    // First s_input_size floats for input tile, then kernel_size*kernel_size floats for weight tile
    extern __shared__ float shmem[];
    float* s_input  = shmem;
    float* s_weight = shmem + s_input_size;

    // Loop over input channels
    for (int ic = 0; ic < in_channels; ic++) {
        // -----------------------
        // Load weight tile for current input channel
        // Weight tensor shape: [out_channels, in_channels, kernel_size, kernel_size]
        int weight_elements = kernel_size * kernel_size;
        // Use a 1D loop over block threads to cooperatively load the weight tile
        int threadId = threadIdx.y * blockDim.x + threadIdx.x;
        for (int i = threadId; i < weight_elements; i += blockDim.x * blockDim.y) {
            int ky = i / kernel_size;
            int kx = i % kernel_size;
            int w_index = ((oc * in_channels + ic) * kernel_size + ky) * kernel_size + kx;
            s_weight[i] = weight[w_index];
        }

        // -----------------------
        // Load input tile for current input channel
        // The input patch corresponds to the region needed to compute the output tile
        int input_tile_width = shared_input_width;
        int input_tile_height = shared_input_height;
        int tile_size = input_tile_width * input_tile_height;
        
        // Each thread may load multiple elements from the input tile
        for (int i = threadId; i < tile_size; i += blockDim.x * blockDim.y) {
            int sy = i / input_tile_width;
            int sx = i % input_tile_width;
            int in_y = tile_y + sy; 
            int in_x = tile_x + sx; 
            // Check bounds for the input; though for valid convolution these should be in-bound
            if (in_y < input_h && in_x < input_w) {
                int in_index = ((n * in_channels + ic) * input_h + in_y) * input_w + in_x;
                s_input[i] = input[in_index];
            } else {
                s_input[i] = 0.0f;
            }
        }

        __syncthreads(); // Ensure shared memory is loaded for current input channel

        // -----------------------
        // Each thread computes the convolution for its output pixel from the loaded shared memory
        if (out_x < output_w && out_y < output_h) {
            float sum_local = 0.0f;
            for (int ky = 0; ky < kernel_size; ky++) {
                for (int kx = 0; kx < kernel_size; kx++) {
                    int s_index = (ty + ky) * input_tile_width + (tx + kx);
                    float in_val = s_input[s_index];
                    float w_val = s_weight[ky * kernel_size + kx];
                    sum_local += in_val * w_val;
                }
            }
            sum += sum_local;
        }

        __syncthreads(); // Prepare for next input channel iteration
    }

    // After accumulating over all input channels, apply multiplier, activation, then GELU
    if (out_x < output_w && out_y < output_h) {
        sum *= multiplier[oc];
        // Leaky ReLU with negative slope 0.01 (non-branching form using ternary operator)
        sum = (sum > 0.0f) ? sum : 0.01f * sum;
        float out_val = gelu(sum);
        // Write output to global memory; output shape: [batch_size, out_channels, output_h, output_w]
        int out_index = ((n * out_channels + oc) * output_h + out_y) * output_w + out_x;
        output[out_index] = out_val;
    }
}

// C++ interface (to be called from Python)

torch::Tensor forward_cuda(
    torch::Tensor input,
    torch::Tensor conv_weight,
    torch::Tensor conv_bias,
    torch::Tensor multiplier
) {
    // Get tensor dimensions
    const auto batch_size   = input.size(0);
    const auto in_channels  = input.size(1);
    const auto input_h      = input.size(2);
    const auto input_w      = input.size(3);
    
    const auto out_channels = conv_weight.size(0);
    const auto kernel_size  = conv_weight.size(2); // assuming square kernels
    const auto output_h     = input_h - kernel_size + 1;
    const auto output_w     = input_w - kernel_size + 1;
    
    auto output = torch::empty({batch_size, out_channels, output_h, output_w},
                                 torch::dtype(input.dtype()).device(input.device()));
    
    // Define grid and block dimensions
    // Each block computes a TILE_H x TILE_W output tile for a given (n, oc)
    dim3 blockDim(TILE_W, TILE_H);
    
    // Calculate exact number of blocks needed without oversubscription
    // Use ceiling division to ensure coverage of non-aligned dimensions
    int grid_x = (output_w + TILE_W - 1) / TILE_W;
    int grid_y = (output_h + TILE_H - 1) / TILE_H;
    
    // Ensure we don't launch unnecessary blocks at the boundaries
    grid_x = min(grid_x, (output_w + TILE_W - 1) / TILE_W);
    grid_y = min(grid_y, (output_h + TILE_H - 1) / TILE_H);
    
    int grid_z = batch_size * out_channels;
    dim3 gridDim(grid_x, grid_y, grid_z);
    
    // Calculate dynamic shared memory size in bytes
    int shared_input_size = (TILE_W + kernel_size - 1) * (TILE_H + kernel_size - 1);
    int shared_weight_size = kernel_size * kernel_size;
    int shared_mem_bytes = (shared_input_size + shared_weight_size) * sizeof(float);
    
    conv_forward_kernel_shared<<<gridDim, blockDim, shared_mem_bytes>>>(
        input.data_ptr<float>(),
        conv_weight.data_ptr<float>(),
        conv_bias.data_ptr<float>(),
        multiplier.data_ptr<float>(),
        output.data_ptr<float>(),
        batch_size,
        in_channels,
        input_h,
        input_w,
        out_channels,
        kernel_size,
        output_h,
        output_w
    );
    
    cudaError_t err = cudaGetLastError();
    if (err != cudaSuccess) {
        printf(""CUDA kernel failed: %s\n"", cudaGetErrorString(err));
    }
    
    return output;
}

PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
    m.def(""forward"", &forward_cuda, ""Optimized convolution, scalar multiplication, LeakyReLU and GELU using shared memory tiling (CUDA)"");
}
",conv2d,,,,,0,1,0.8242061,8517
"#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>

// Device function to handle shared memory initialization
__device__ __forceinline__ void init_shared_bias(
    float* shared_bias,
    const float* __restrict__ element_bias,
    const int channels,
    const int tid
) {
    for (int i = tid; i < channels; i += blockDim.x) {
        shared_bias[i * 2] = __ldg(&element_bias[i]);
    }
    __syncthreads();
}

// Device function to process a vector of 4 elements
__device__ __forceinline__ float4 process_vector(
    const float4& in_vec,
    const float* shared_bias,
    const int base_idx,
    const int spatial_size,
    const int channels
) {
    float4 out_vec;
    #pragma unroll
    for (int j = 0; j < 4; j++) {
        const int curr_idx = base_idx + j;
        const int c = (curr_idx / spatial_size) % channels;
        const float orig = ((float*)&in_vec)[j];
        const float bias_val = shared_bias[c];
        ((float*)&out_vec)[j] = orig * (2.0f * orig + bias_val + 1.0f);
    }
    return out_vec;
}

// Device function to process a single element
__device__ __forceinline__ float process_scalar(
    const float original,
    const float bias_val
) {
    return original * (2.0f * original + bias_val + 1.0f);
}

__global__ void modular_coalesced_kernel(
    const float* __restrict__ conv_output,
    const float* __restrict__ element_bias,
    float* output,
    const int num_elements,
    const int channels,
    const int spatial_size
) {
    extern __shared__ float shared_bias[];
    
    const int tid = threadIdx.x;
    const int gid = blockIdx.x * blockDim.x + tid;
    const int stride = blockDim.x * gridDim.x;

    // Initialize shared memory
    init_shared_bias(shared_bias, element_bias, channels, tid);

    // Process vectorized elements
    const int vec_elements = num_elements / 4;
    for (int vec_idx = gid; vec_idx < vec_elements; vec_idx += stride) {
        const float4 in_vec = reinterpret_cast<const float4*>(conv_output)[vec_idx];
        const int base_idx = vec_idx * 4;
        
        // Process vector of 4 elements
        const float4 out_vec = process_vector(in_vec, shared_bias, base_idx, spatial_size, channels);
        
        // Store result
        reinterpret_cast<float4*>(output)[vec_idx] = out_vec;
    }

    // Handle remaining elements
    const int remaining_start = vec_elements * 4;
    for (int idx = remaining_start + gid; idx < num_elements; idx += stride) {
        const int c = (idx / spatial_size) % channels;
        const float original = __ldg(&conv_output[idx]);
        const float bias_val = shared_bias[c];
        
        // Process single element
        output[idx] = process_scalar(original, bias_val);
    }
}

torch::Tensor forward(
    torch::Tensor x,
    int stride,
    int padding,
    int output_padding,
    torch::Tensor conv_transpose,
    torch::Tensor conv_transpose_bias,
    torch::Tensor bias
) {
    auto conv_result = torch::conv_transpose3d(
        x, conv_transpose, conv_transpose_bias,
        stride, padding, output_padding
    );

    auto sizes = conv_result.sizes();
    const int channels = sizes[1];
    const int spatial_size = sizes[2] * sizes[3] * sizes[4];
    const int num_elements = conv_result.numel();

    auto output = torch::empty_like(conv_result);

    // Optimize grid size based on vector processing
    const int threads_per_block = 256;
    const int vec_elements = num_elements / 4;
    const int num_blocks = min((vec_elements + threads_per_block - 1) / threads_per_block, 1024);

    const size_t shared_mem_size = channels * sizeof(float);

    modular_coalesced_kernel<<<num_blocks, threads_per_block, shared_mem_size>>>(
        conv_result.data_ptr<float>(),
        bias.data_ptr<float>(),
        output.data_ptr<float>(),
        num_elements,
        channels,
        spatial_size
    );

    return output;
}

PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
    m.def(""forward"", &forward, ""Modular Coalesced ConvTranspose3D with Channel-wise Bias"");
}",other,,,,,0,1,0.822948,4028
"#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>

void gelu_scaling_forward_cuda(
    at::Tensor& x,
    at::Tensor& out,
    double scaling_factor
);

__global__ void gelu_scaling_kernel(
    const float* __restrict__ x,
    float* __restrict__ out,
    int64_t numel,
    float scaling_factor
) {
    int idx = blockDim.x * blockIdx.x + threadIdx.x;
    if (idx < numel) {
        float y = x[idx];

        // Apply GELU activation
        float y_cubed = y * y * y;
        float cdf = 0.5f * (1.0f + tanhf(0.797885f * (y + 0.044715f * y_cubed));
        y = y * cdf;

        // Apply scaling factor
        y = y * scaling_factor;

        out[idx] = y;
    }
}

void gelu_scaling_forward_cuda(
    at::Tensor& x,
    at::Tensor& out,
    double scaling_factor
) {
    AT_ASSERTM(x.device().type() == at::kCUDA, ""x must be a CUDA tensor"");
    AT_ASSERTM(out.device().type() == at::kCUDA, ""out must be a CUDA tensor"");

    int64_t numel = x.numel();

    int threads = 512;  // Increased thread count per block
    int blocks = (numel + threads - 1) / threads;

    gelu_scaling_kernel<<<blocks, threads>>>(
        x.data_ptr<float>(),
        out.data_ptr<float>(),
        numel,
        static_cast<float>(scaling_factor)
    );

    cudaError_t err = cudaGetLastError();
    if (err != cudaSuccess) {
        printf(""Error in gelu_scaling_kernel: %s\n"", cudaGetErrorString(err));
    }
}

at::Tensor forward(
    at::Tensor x,
    int64_t stride,
    int64_t padding,
    double eps,
    double scaling_factor,
    at::Tensor conv_transpose_weight,
    at::Tensor conv_transpose_bias,
    at::Tensor layer_norm_weight,
    at::Tensor layer_norm_bias
) {
    // Perform conv_transpose3d
    at::Tensor x_conv = at::conv_transpose3d(
        x,
        conv_transpose_weight,
        conv_transpose_bias,
        {stride, stride, stride},
        {padding, padding, padding}
    );

    // Get out_channels
    int64_t out_channels = x_conv.size(1);

    // Perform layer_norm
    at::Tensor x_norm = at::layer_norm(
        x_conv,
        {out_channels},
        layer_norm_weight,
        layer_norm_bias,
        eps
    );

    // Apply GELU and scaling via custom CUDA kernel
    at::Tensor x_scaled = at::empty_like(x_norm);

    gelu_scaling_forward_cuda(x_norm, x_scaled, scaling_factor);

    return x_scaled;
}

PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
    m.def(""forward"", &forward, ""Custom module forward"");
}",other,,,,,0,1,0.82274014,2458
"#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>

// Vector type for coalesced memory access
typedef float4 vec4;

// Device function for vectorized loading
__device__ __forceinline__ vec4 load_vec4(const float* ptr) {
    return *reinterpret_cast<const vec4*>(ptr);
}

// Device function for vectorized storing
__device__ __forceinline__ void store_vec4(float* ptr, vec4 val) {
    *reinterpret_cast<vec4*>(ptr) = val;
}

// Device function for concatenation with vectorized memory operations
__device__ __forceinline__ void concat_vec4(
    const float* __restrict__ x,
    const float* __restrict__ hidden,
    float* __restrict__ combined,
    const int row,
    const int x_size,
    const int hidden_size
) {
    const int combined_width = x_size + hidden_size;
    
    // Process x data with vector loads where possible
    for (int i = threadIdx.x * 4; i < x_size; i += blockDim.x * 4) {
        if (i + 4 <= x_size) {
            vec4 x_data = load_vec4(x + row * x_size + i);
            store_vec4(combined + row * combined_width + i, x_data);
        } else {
            // Handle remaining elements
            for (int j = i; j < min(i + 4, x_size); j++) {
                combined[row * combined_width + j] = x[row * x_size + j];
            }
        }
    }
    
    // Process hidden data with vector loads where possible
    for (int i = threadIdx.x * 4; i < hidden_size; i += blockDim.x * 4) {
        if (i + 4 <= hidden_size) {
            vec4 h_data = load_vec4(hidden + row * hidden_size + i);
            store_vec4(combined + row * combined_width + x_size + i, h_data);
        } else {
            // Handle remaining elements
            for (int j = i; j < min(i + 4, hidden_size); j++) {
                combined[row * combined_width + x_size + j] = hidden[row * hidden_size + j];
            }
        }
    }
}

// New kernel for copying x component
__global__ void concat_x_kernel(
    const float* __restrict__ x,
    float* __restrict__ combined,
    const int batch_size,
    const int x_size,
    const int hidden_size
) {
    const int row = blockIdx.x;
    if (row >= batch_size) return;
    const int combined_width = x_size + hidden_size;
    // Process x data with vector loads where possible
    for (int i = threadIdx.x * 4; i < x_size; i += blockDim.x * 4) {
        if (i + 4 <= x_size) {
            vec4 x_data = load_vec4(x + row * x_size + i);
            store_vec4(combined + row * combined_width + i, x_data);
        } else {
            for (int j = i; j < min(i + 4, x_size); j++) {
                combined[row * combined_width + j] = x[row * x_size + j];
            }
        }
    }
}

// New kernel for copying hidden component
__global__ void concat_hidden_kernel(
    const float* __restrict__ hidden,
    float* __restrict__ combined,
    const int batch_size,
    const int x_size,
    const int hidden_size
) {
    const int row = blockIdx.x;
    if (row >= batch_size) return;
    const int combined_width = x_size + hidden_size;
    // Process hidden data with vector loads where possible
    for (int i = threadIdx.x * 4; i < hidden_size; i += blockDim.x * 4) {
        if (i + 4 <= hidden_size) {
            vec4 h_data = load_vec4(hidden + row * hidden_size + i);
            store_vec4(combined + row * combined_width + x_size + i, h_data);
        } else {
            for (int j = i; j < min(i + 4, hidden_size); j++) {
                combined[row * combined_width + x_size + j] = hidden[row * hidden_size + j];
            }
        }
    }
}

torch::Tensor module_fn_cuda(
    torch::Tensor x,
    torch::Tensor i2h_weight,
    torch::Tensor i2h_bias,
    torch::Tensor h2o_weight,
    torch::Tensor h2o_bias,
    torch::Tensor hidden
) {
    x = x.contiguous().cuda();
    i2h_weight = i2h_weight.contiguous().cuda();
    i2h_bias = i2h_bias.contiguous().cuda();
    h2o_weight = h2o_weight.contiguous().cuda();
    h2o_bias = h2o_bias.contiguous().cuda();
    hidden = hidden.contiguous().cuda();

    const int batch_size = x.size(0);
    const int x_size = x.size(1);
    const int hidden_size = hidden.size(1);

    auto combined = torch::empty({batch_size, x_size + hidden_size}, x.options());

    // Configure kernel launch parameters
    const int threads = 256;
    const int blocks = batch_size;
    
    concat_kernel<<<blocks, threads>>>(
        x.data_ptr<float>(),
        hidden.data_ptr<float>(),
        combined.data_ptr<float>(),
        batch_size,
        x_size,
        hidden_size
    );

    auto hidden_new = torch::tanh(torch::addmm(i2h_bias, combined, i2h_weight.t()));
    return torch::addmm(h2o_bias, hidden_new, h2o_weight.t());
}

PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
    m.def(""forward"", &module_fn_cuda, ""Module forward (CUDA)"");
}",other,,,,,0,1,0.82056504,4779
"#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>

template<int KERNEL_SIZE>
__global__ void depthwise_3dgrid_unrolled_kernel(
    const float* __restrict__ input,
    const float* __restrict__ weight,
    const float* __restrict__ bias,
    float* __restrict__ output,
    const int batch_size,
    const int in_channels,
    const int input_h,
    const int input_w,
    const int out_channels,
    const int output_h,
    const int output_w,
    const int stride,
    const int padding,
    const int channels_per_group,
    int grid_offset
) {
    const int oc_batch = blockIdx.z;
    const int b = oc_batch / out_channels;
    const int oc = oc_batch % out_channels;
    
    const int w_out = blockIdx.x * blockDim.x + threadIdx.x;
    const int h_out = blockIdx.y * blockDim.y + threadIdx.y;
    
    if (w_out >= output_w || h_out >= output_h) return;

    const int in_ch = oc / channels_per_group;
    const int weight_ch = oc % channels_per_group;

    // Precompute base indices
    const int input_batch_offset = b * (in_channels * input_h * input_w);
    const int input_channel_offset = in_ch * (input_h * input_w);
    const int weight_offset = in_ch * (channels_per_group * KERNEL_SIZE * KERNEL_SIZE) +
                             weight_ch * (KERNEL_SIZE * KERNEL_SIZE);

    float sum = 0.0f;

    // Manual unroll for KERNEL_SIZE x KERNEL_SIZE
    #pragma unroll
    for (int kh = 0; kh < KERNEL_SIZE; ++kh) {
        const int h_in = h_out * stride + kh - padding;
        
        if (h_in >= 0 && h_in < input_h) {
            const int input_h_offset = input_batch_offset + input_channel_offset + h_in * input_w;
            const int weight_h_offset = weight_offset + kh * KERNEL_SIZE;
            
            #pragma unroll
            for (int kw = 0; kw < KERNEL_SIZE; ++kw) {
                const int w_in = w_out * stride + kw - padding;
                
                if (w_in >= 0 && w_in < input_w) {
                    sum += __ldg(&input[input_h_offset + w_in]) * 
                           __ldg(&weight[weight_h_offset + kw]);
                }
            }
        }
    }

    if (bias != nullptr) {
        sum += __ldg(&bias[oc]);
    }

    output[b * (out_channels * output_h * output_w) +
           oc * (output_h * output_w) +
           h_out * output_w + 
           w_out] = sum;
}

torch::Tensor forward(
    torch::Tensor input,
    torch::Tensor weight,
    torch::optional<torch::Tensor> bias,
    int stride,
    int padding
) {
    TORCH_CHECK(input.is_cuda() && weight.is_cuda(), ""Inputs must be CUDA tensors"");
    if (bias.has_value()) {
        TORCH_CHECK(bias->is_cuda(), ""Bias must be a CUDA tensor"");
    }
    TORCH_CHECK(input.is_contiguous() && weight.is_contiguous(), ""Input and weight must be contiguous"");
    
    const int batch_size = input.size(0);
    const int in_channels = input.size(1);
    const int input_h = input.size(2);
    const int input_w = input.size(3);
    const int kernel_size = weight.size(2);
    const int channels_per_group = weight.size(1);
    const int out_channels = in_channels * channels_per_group;

    const int output_h = (input_h + 2 * padding - kernel_size) / stride + 1;
    const int output_w = (input_w + 2 * padding - kernel_size) / stride + 1;

    auto output = torch::empty({batch_size, out_channels, output_h, output_w}, input.options());

    // Optimize block dimensions for H100
    const int TILE_W = 16;
    const int TILE_H = 16;
    dim3 block(TILE_W, TILE_H);
    dim3 grid((output_w + TILE_W - 1) / TILE_W,
              (output_h + TILE_H - 1) / TILE_H,
              batch_size * out_channels);

    // Template specialization for common kernel sizes
    if (kernel_size == 3) {
        depthwise_3dgrid_unrolled_kernel<3><<<grid, block>>>(
            input.data_ptr<float>(),
            weight.data_ptr<float>(),
            bias.has_value() ? bias->data_ptr<float>() : nullptr,
            output.data_ptr<float>(),
            batch_size,
            in_channels,
            input_h,
            input_w,
            out_channels,
            output_h,
            output_w,
            stride,
            padding,
            channels_per_group
        );
    } else if (kernel_size == 5) {
        depthwise_3dgrid_unrolled_kernel<5><<<grid, block>>>(
            input.data_ptr<float>(),
            weight.data_ptr<float>(),
            bias.has_value() ? bias->data_ptr<float>() : nullptr,
            output.data_ptr<float>(),
            batch_size,
            in_channels,
            input_h,
            input_w,
            out_channels,
            output_h,
            output_w,
            stride,
            padding,
            channels_per_group
        );
    } else {
        // Fallback for other kernel sizes
        depthwise_3dgrid_unrolled_kernel<7><<<grid, block>>>(
            input.data_ptr<float>(),
            weight.data_ptr<float>(),
            bias.has_value() ? bias->data_ptr<float>() : nullptr,
            output.data_ptr<float>(),
            batch_size,
            in_channels,
            input_h,
            input_w,
            out_channels,
            output_h,
            output_w,
            stride,
            padding,
            channels_per_group
        );
    }

    return output;
}

PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
    m.def(""forward"", &forward, ""Depthwise 2D Convolution with unrolled 3D grid mapping"",
          py::arg(""input""), py::arg(""weight""), py::arg(""bias"") = py::none(), py::arg(""stride""), py::arg(""padding""));
}",conv2d,,,,,0,1,0.81399316,5561
"#include <torch/extension.h>
#include <pybind11/pybind11.h>
#include <ATen/cuda/CUDAContext.h>
#include <c10/cuda/CUDAGuard.h>
#include <cuda.h>
#include <cuda_runtime.h>

namespace py = pybind11;

constexpr int BLOCK_SIZE_X = 16;
constexpr int BLOCK_SIZE_Y = 16;

__global__ void fused_add_relu_kernel(float* __restrict__ output,
                                       const float* __restrict__ identity,
                                       int width, int height, int channels) {
    int batch = blockIdx.z;
    int channel = blockIdx.y;
    int x = threadIdx.x + blockIdx.x * blockDim.x;
    int y = threadIdx.y + blockIdx.y * blockDim.y;

    if (x < width && y < height) {
        int idx = batch * (channels * width * height) + channel * (width * height) + y * width + x;
        float val = output[idx] + identity[idx];
        output[idx] = fmaxf(val, 0.0f);
    }
}

void perform_fused_add_relu(torch::Tensor& x, const torch::Tensor& identity) {
    int batch_size = x.size(0);
    int channels = x.size(1);
    int height = x.size(2);
    int width = x.size(3);

    dim3 block(BLOCK_SIZE_X, BLOCK_SIZE_Y);
    dim3 grid((width + BLOCK_SIZE_X - 1) / BLOCK_SIZE_X, (height + BLOCK_SIZE_Y - 1) / BLOCK_SIZE_Y, batch_size * channels);

    fused_add_relu_kernel<<<grid, block, 0, at::cuda::getCurrentCUDAStream()>>>(
        x.data_ptr<float>(), identity.data_ptr<float>(), width, height, channels);
}

torch::Tensor basic_block_fn(
    torch::Tensor x,
    torch::Tensor conv1_w,
    torch::Tensor bn1_w,
    torch::Tensor bn1_b,
    torch::Tensor bn1_rm,
    torch::Tensor bn1_rv,
    torch::Tensor conv2_w,
    torch::Tensor bn2_w,
    torch::Tensor bn2_b,
    torch::Tensor bn2_rm,
    torch::Tensor bn2_rv,
    torch::Tensor downsample_conv_w,
    torch::Tensor downsample_bn_w,
    torch::Tensor downsample_bn_b,
    torch::Tensor downsample_bn_rm,
    torch::Tensor downsample_bn_rv,
    int64_t stride,
    bool is_training
) {
    const at::cuda::CUDAGuard device_guard(x.device());
    torch::Tensor identity = x;

    // Main branch
    x = torch::conv2d(x, conv1_w, /*bias=*/{}, {stride, stride}, {1, 1});
    x = torch::batch_norm(x, bn1_w, bn1_b, bn1_rm, bn1_rv, is_training, 0.0, 1e-5, true);
    x = torch::relu(x);
    
    x = torch::conv2d(x, conv2_w, /*bias=*/{}, {1, 1}, {1, 1});
    x = torch::batch_norm(x, bn2_w, bn2_b, bn2_rm, bn2_rv, is_training, 0.0, 1e-5, true);

    // Downsample path
    if (downsample_conv_w.defined()) {
        identity = torch::conv2d(identity, downsample_conv_w, /*bias=*/{}, {stride, stride});
        identity = torch::batch_norm(identity, downsample_bn_w, downsample_bn_b,
                                   downsample_bn_rm, downsample_bn_rv,
                                   is_training, 0.0, 1e-5, true);
    }

    // Apply fused residual addition and ReLU
    perform_fused_add_relu(x, identity);
    return x;
}

torch::Tensor module_fn(torch::Tensor x, py::object params_py, bool is_training) {
    const at::cuda::CUDAGuard device_guard(x.device());
    
    auto get_param = [&](const std::string& key) -> torch::Tensor {
        return params_py.attr(""__getitem__"")(key.c_str()).cast<torch::Tensor>();
    };

    // Initial layers
    auto conv1_weight = get_param(""conv1_weight"");
    auto bn1_weight = get_param(""bn1_weight"");
    auto bn1_bias = get_param(""bn1_bias"");
    auto bn1_running_mean = get_param(""bn1_running_mean"");
    auto bn1_running_var = get_param(""bn1_running_var"");

    x = torch::conv2d(x, conv1_weight, /*bias=*/{}, {2, 2}, {3, 3});
    x = torch::batch_norm(x, bn1_weight, bn1_bias, bn1_running_mean, bn1_running_var,
                         is_training, 0.0, 1e-5, true);
    x = torch::relu(x);
    x = torch::max_pool2d(x, {3, 3}, {2, 2}, {1, 1});

    // ResNet layers
    for (int i = 1; i <= 4; ++i) {
        std::string layer_name = ""layer"" + std::to_string(i);
        for (int j = 0; j < 2; ++j) {
            std::string block_name = layer_name + ""_"" + std::to_string(j);
            int64_t stride = (i > 1 && j == 0) ? 2 : 1;

            auto conv1_w = get_param(block_name + ""_conv1_weight"");
            auto bn1_w = get_param(block_name + ""_bn1_weight"");
            auto bn1_b = get_param(block_name + ""_bn1_bias"");
            auto bn1_rm = get_param(block_name + ""_bn1_running_mean"");
            auto bn1_rv = get_param(block_name + ""_bn1_running_var"");

            auto conv2_w = get_param(block_name + ""_conv2_weight"");
            auto bn2_w = get_param(block_name + ""_bn2_weight"");
            auto bn2_b = get_param(block_name + ""_bn2_bias"");
            auto bn2_rm = get_param(block_name + ""_bn2_running_mean"");
            auto bn2_rv = get_param(block_name + ""_bn2_running_var"");

            std::string downsample_key = block_name + ""_downsample_0_weight"";
            bool has_downsample = PyMapping_HasKeyString(params_py.ptr(), downsample_key.c_str()) == 1;

            torch::Tensor downsample_conv_w, downsample_bn_w, downsample_bn_b,
                         downsample_bn_rm, downsample_bn_rv;

            if (has_downsample) {
                downsample_conv_w = get_param(block_name + ""_downsample_0_weight"");
                downsample_bn_w = get_param(block_name + ""_downsample_1_weight"");
                downsample_bn_b = get_param(block_name + ""_downsample_1_bias"");
                downsample_bn_rm = get_param(block_name + ""_downsample_1_running_mean"");
                downsample_bn_rv = get_param(block_name + ""_downsample_1_running_var"");
            }

            x = basic_block_fn(x, conv1_w, bn1_w, bn1_b, bn1_rm, bn1_rv,
                             conv2_w, bn2_w, bn2_b, bn2_rm, bn2_rv,
                             downsample_conv_w, downsample_bn_w, downsample_bn_b,
                             downsample_bn_rm, downsample_bn_rv,
                             stride, is_training);
        }
    }

    x = torch::adaptive_avg_pool2d(x, {1, 1});
    x = x.view({x.size(0), -1});
    auto fc_weight = get_param(""fc_weight"");
    auto fc_bias = get_param(""fc_bias"");
    x = torch::linear(x, fc_weight, fc_bias);
    return x;
}

PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
    m.def(""forward"", &module_fn, ""ResNet18 forward function with optimized indexing (CUDA)"");
}",other,,,,,0,1,0.8110157,6236
"#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>
#include <cmath>

__global__ void conv_min_tanh_forward_kernel_3d(
    const float* __restrict__ x,
    const float* __restrict__ weight,
    const float* __restrict__ bias,
    float* __restrict__ output,
    const int batch,
    const int in_channels,
    const int in_height,
    const int in_width,
    const int out_channels,
    const int kernel_size,
    const int out_height,
    const int out_width)
{
    // Direct 3D thread mapping with larger block size
    const int out_x = blockIdx.x * blockDim.x + threadIdx.x;
    const int out_y = blockIdx.y * blockDim.y + threadIdx.y;
    const int b = blockIdx.z;

    if (out_x >= out_width || out_y >= out_height || b >= batch) return;

    // Pre-compute strides for efficient indexing
    const int in_channel_stride = in_height * in_width;
    const int batch_stride = in_channels * in_channel_stride;
    const int weight_channel_stride = in_channels * kernel_size * kernel_size;
    
    // Base indices for current thread
    const int b_offset = b * batch_stride;
    const int out_offset = b * out_channels * out_height * out_width + out_y * out_width + out_x;

    float min_val = 1e20f;
    
    #pragma unroll 1
    for (int oc = 0; oc < out_channels; ++oc) {
        float conv_sum = bias[oc];
        const int weight_oc_offset = oc * weight_channel_stride;
        
        #pragma unroll 1
        for (int ic = 0; ic < in_channels; ++ic) {
            const int ic_offset = ic * in_channel_stride;
            const int weight_ic_offset = weight_oc_offset + ic * kernel_size * kernel_size;
            
            #pragma unroll
            for (int ky = 0; ky < kernel_size; ++ky) {
                const int in_y = out_y + ky;
                const int y_offset = in_y * in_width;
                const int weight_y_offset = weight_ic_offset + ky * kernel_size;
                
                #pragma unroll
                for (int kx = 0; kx < kernel_size; ++kx) {
                    const int in_x = out_x + kx;
                    const int x_idx = b_offset + ic_offset + y_offset + in_x;
                    const int w_idx = weight_y_offset + kx;
                    conv_sum += x[x_idx] * weight[w_idx];
                }
            }
        }
        min_val = min(min_val, conv_sum);
    }

    output[out_offset] = tanhf(tanhf(min_val));
}

void conv_min_tanh_forward_cuda(
    at::Tensor x,
    at::Tensor conv_weight,
    at::Tensor conv_bias,
    at::Tensor output)
{
    const int batch = x.size(0);
    const int in_channels = x.size(1);
    const int in_height = x.size(2);
    const int in_width = x.size(3);
    const int out_channels = conv_weight.size(0);
    const int kernel_size = conv_weight.size(2);
    const int out_height = in_height - kernel_size + 1;
    const int out_width = in_width - kernel_size + 1;

    // Optimize block dimensions to 512 threads (32x16)
    dim3 block(32, 16, 1);
    dim3 grid(
        (out_width + block.x - 1) / block.x,
        (out_height + block.y - 1) / block.y,
        batch
    );

    conv_min_tanh_forward_kernel_3d<<<grid, block>>>(
        x.data_ptr<float>(),
        conv_weight.data_ptr<float>(),
        conv_bias.data_ptr<float>(),
        output.data_ptr<float>(),
        batch,
        in_channels,
        in_height,
        in_width,
        out_channels,
        kernel_size,
        out_height,
        out_width
    );
}

at::Tensor forward(
    at::Tensor x,
    at::Tensor conv_weight,
    at::Tensor conv_bias)
{
    TORCH_CHECK(x.is_cuda(), ""x must be a CUDA tensor"");
    TORCH_CHECK(conv_weight.is_cuda(), ""conv_weight must be a CUDA tensor"");
    TORCH_CHECK(conv_bias.is_cuda(), ""conv_bias must be a CUDA tensor"");

    const int batch = x.size(0);
    const int kernel_size = conv_weight.size(2);
    const int out_height = x.size(2) - kernel_size + 1;
    const int out_width = x.size(3) - kernel_size + 1;

    auto output = at::empty({batch, 1, out_height, out_width}, x.options());
    conv_min_tanh_forward_cuda(x, conv_weight, conv_bias, output);
    return output;
}

PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
    m.def(""forward"", &forward, ""Convolution, min (over channels), and double tanh activation (CUDA)"");
}",conv2d,,,,,0,1,0.80622166,4266
"#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>

// Optimized kernel with coalesced memory access for improved performance.

template <typename scalar_t>
__global__ void coalesced_memory_kernel(
    const scalar_t* __restrict__ input,
    const scalar_t* __restrict__ weight,
    const scalar_t* __restrict__ bias,
    scalar_t* __restrict__ output,
    const int batch_size,
    const int in_channels,
    const int out_channels,
    const int in_height,
    const int in_width,
    const int out_height,
    const int out_width,
    const int kernel_size,
    const int pool_size,
    const int pool_out_h,
    const int pool_out_w,
    const float subtract1,
    const float subtract2
) {
    // Calculate output position
    const int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx >= batch_size * out_channels * pool_out_h * pool_out_w) return;

    const int pw = idx % pool_out_w;
    const int ph = (idx / pool_out_w) % pool_out_h;
    const int c = (idx / (pool_out_w * pool_out_h)) % out_channels;
    const int b = idx / (pool_out_w * pool_out_h * out_channels);

    // Calculate pooling window start position
    const int h_start = ph * pool_size;
    const int w_start = pw * pool_size;
    const int valid_pool_h = (pool_size < (out_height - h_start)) ? pool_size : (out_height - h_start);
    const int valid_pool_w = (pool_size < (out_width - w_start)) ? pool_size : (out_width - w_start);

    float pool_sum = 0.0f;
    // Iterate over valid pooling window
    #pragma unroll
    for (int ph_offset = 0; ph_offset < valid_pool_h; ph_offset++) {
        #pragma unroll
        for (int pw_offset = 0; pw_offset < pool_size; pw_offset++) {
            const int h = h_start + ph_offset;
            const int w = w_start + pw_offset;
            
            if (h >= out_height || w >= out_width) continue;

            // Compute convolution for this position
            float conv_result = bias[c];
            
            #pragma unroll
            for (int ic = 0; ic < in_channels; ic++) {
                #pragma unroll
                for (int kh = 0; kh < kernel_size; kh++) {
                    #pragma unroll
                    for (int kw = 0; kw < kernel_size; kw++) {
                        const int in_h = h + kh;
                        const int in_w = w + kw;
                        
                        if (in_h < in_height && in_w < in_width) {
                            const int in_idx = b * (in_channels * in_height * in_width) +
                                             ic * (in_height * in_width) +
                                             in_h * in_width + in_w;
                            const int w_idx = c * (in_channels * kernel_size * kernel_size) +
                                            ic * (kernel_size * kernel_size) +
                                            kh * kernel_size + kw;
                            conv_result += input[in_idx] * weight[w_idx];
                        }
                    }
                }
            }

            // Apply first subtraction and tanh
            conv_result = tanhf(conv_result - subtract1);
            
            // Apply second subtraction
            conv_result = conv_result - subtract2;
            
            pool_sum += conv_result;
            pool_count++;
        }
    }

    // Write final result
    if (pool_count > 0) {
        output[idx] = pool_sum / pool_count;
    }
}

torch::Tensor forward(
    torch::Tensor input,
    int kernel_size_pool,
    torch::Tensor conv_weight,
    torch::Tensor conv_bias,
    float subtract1_value,
    float subtract2_value
) {
    const int batch_size = input.size(0);
    const int in_channels = input.size(1);
    const int in_height = input.size(2);
    const int in_width = input.size(3);
    const int out_channels = conv_weight.size(0);
    const int kernel_size = conv_weight.size(2);
    
    const int out_height = in_height - kernel_size + 1;
    const int out_width = in_width - kernel_size + 1;
    const int pool_out_h = out_height / kernel_size_pool;
    const int pool_out_w = out_width / kernel_size_pool;

    auto output = torch::zeros({batch_size, out_channels, pool_out_h, pool_out_w},
                             input.options());

    const int total_elements = batch_size * out_channels * pool_out_h * pool_out_w;
    const int threads = 256;
    const int blocks = (total_elements + threads - 1) / threads;

    AT_DISPATCH_FLOATING_TYPES(input.type(), ""coalesced_memory_kernel"", ([&] {
        coalesced_memory_kernel<scalar_t><<<blocks, threads>>>(
            input.data<scalar_t>(),
            conv_weight.data<scalar_t>(),
            conv_bias.data<scalar_t>(),
            output.data<scalar_t>(),
            batch_size,
            in_channels,
            out_channels,
            in_height,
            in_width,
            out_height,
            out_width,
            kernel_size,
            kernel_size_pool,
            pool_out_h,
            pool_out_w,
            subtract1_value,
            subtract2_value
        );
    }));

    return output;
}

PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
    m.def(""forward"", &forward, ""Conv+sub+tanh+sub+pool forward"");
}",conv2d,,,,,0,1,0.805601,5242
"#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>

// Kernel optimized for memory coalescing and instruction-level parallelism
// Use warp-stride loops to ensure threads in a warp access consecutive memory locations
// This improves performance by better utilizing memory bandwidth

const int WARP_SIZE = 32;

__inline__ __device__ int get_warp_id() {
    return threadIdx.x / WARP_SIZE;
}

__inline__ __device__ int get_lane_id() {
    return threadIdx.x % WARP_SIZE;
}

struct CoalescedArrayAccessor {
    float *data;
    size_t num_elements;

    __device__ float& operator[](size_t idx) {
        return data[idx];
    }
};

// The kernel now uses float4 vectorized data types for memory coalescing
template <typename scalar_t>
__global__ void fused_add_hardswish_kernel_coalesced(
    const scalar_t* __restrict__ x_conv,
    const scalar_t* __restrict__ add_input,
    scalar_t* __restrict__ output,
    const size_t num_elements) {

    int lane_id = get_lane_id();
    int warp_id = get_warp_id();
    const size_t warp_stride = gridDim.x * blockDim.x / WARP_SIZE;
    
    for (int i = lane_id + warp_id * WARP_SIZE; i < num_elements; i += warp_stride) {
        scalar_t temp = x_conv[i] + add_input[i];
        scalar_t hs_act = (min(max(temp + static_cast<scalar_t>(3.0), 
                                     static_cast<scalar_t>(0.0)), 
                                 static_cast<scalar_t>(6.0))) / static_cast<scalar_t>(6.0);
        output[i] = temp * hs_act * temp;
    }
}

torch::Tensor forward(
    torch::Tensor x,
    torch::Tensor add_input,
    int64_t stride,
    int64_t padding,
    int64_t output_padding,
    torch::Tensor conv_transpose,
    torch::Tensor conv_transpose_bias) {

    auto x_conv = torch::conv_transpose3d(
        x, conv_transpose, conv_transpose_bias,
        stride, padding, output_padding);

    TORCH_CHECK(x_conv.sizes() == add_input.sizes(), ""add_input must match conv output shape"");

    auto output = torch::empty_like(x_conv);
    const size_t num_elements = x_conv.numel();

    // Optimize block and grid size for H100
    const int threads = 256;
    const int blocks = std::min(65535, (int)((num_elements + threads - 1) / threads));

    AT_DISPATCH_FLOATING_TYPES(x_conv.scalar_type(), ""fused_add_hardswish_coalesced"", ([&] {
        fused_add_hardswish_kernel_coalesced<scalar_t><<<blocks, threads>>>(
            x_conv.data_ptr<scalar_t>(),
            add_input.data_ptr<scalar_t>(),
            output.data_ptr<scalar_t>(),
            num_elements);
    }));

    return output;
}

PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
    m.def(""forward"", &forward, ""Fused ConvTranspose3D+Add+HardSwish forward with warp memory coalescing"");
",other,,,,,0,1,0.80523884,2729
"#include <pybind11/pybind11.h>
#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>

static const int BLOCK_SIZE = 256;
static const int WARP_SIZE = 32;
static const int ELEMENTS_PER_THREAD = 4;

template <typename scalar_t>
__global__ void mse_forward_kernel(
    const scalar_t* __restrict__ preds,
    const scalar_t* __restrict__ tgts,
    double* __restrict__ sum_out,
    const int64_t num_elements
) {
    __shared__ double shm[WARP_SIZE];
    const unsigned int tid = threadIdx.x;
    const unsigned int wid = tid / WARP_SIZE;
    const unsigned int lane = tid % WARP_SIZE;
    const unsigned int warp_count = BLOCK_SIZE / WARP_SIZE;
    
    // Calculate base index aligned to warp size
    unsigned int base_idx = (blockIdx.x * BLOCK_SIZE + tid) * ELEMENTS_PER_THREAD;
    const unsigned int stride = blockDim.x * gridDim.x * ELEMENTS_PER_THREAD;
    
    double thread_sum = 0.0;
    
    // Process multiple elements per thread
    #pragma unroll
    for (int i = 0; i < ELEMENTS_PER_THREAD; i++) {
        unsigned int idx = base_idx + i;
        if (idx < num_elements) {
            double diff = static_cast<double>(preds[idx]) - static_cast<double>(tgts[idx]);
            thread_sum += diff * diff;
        }
        base_idx += stride;
    }
    
    // Warp-level reduction using shuffle
    #pragma unroll
    for (int offset = WARP_SIZE/2; offset > 0; offset >>= 1) {
        thread_sum += __shfl_down_sync(0xffffffff, thread_sum, offset);
    }
    
    // First thread in each warp writes to shared memory
    if (lane == 0) {
        shm[wid] = thread_sum;
    }
    __syncthreads();
    
    // First warp reduces results from all warps
    if (wid == 0) {
        thread_sum = (lane < warp_count) ? shm[lane] : 0.0;
        
        // Final warp-level reduction
        #pragma unroll
        for (int offset = WARP_SIZE/2; offset > 0; offset >>= 1) {
            thread_sum += __shfl_down_sync(0xffffffff, thread_sum, offset);
        }
        
        if (lane == 0) {
            atomicAdd(sum_out, thread_sum);
        }
    }
}

torch::Tensor forward(torch::Tensor predictions, torch::Tensor targets) {
    TORCH_CHECK(predictions.is_cuda(), ""predictions must be a CUDA tensor"");
    TORCH_CHECK(targets.is_cuda(), ""targets must be a CUDA tensor"");
    TORCH_CHECK(predictions.numel() == targets.numel(),
                ""predictions and targets must have the same number of elements"");

    const int64_t num_elements = predictions.numel();
    auto accumulator = torch::zeros({1}, predictions.options().dtype(at::kDouble));

    // Calculate grid size ensuring multiple of warp size
    const int grid_size = ((num_elements / ELEMENTS_PER_THREAD + BLOCK_SIZE - 1) / BLOCK_SIZE + WARP_SIZE - 1) & ~(WARP_SIZE - 1);

    AT_DISPATCH_FLOATING_TYPES(predictions.scalar_type(), ""mse_forward_cuda"", [&] {
        mse_forward_kernel<scalar_t><<<grid_size, BLOCK_SIZE>>>(
            predictions.data_ptr<scalar_t>(),
            targets.data_ptr<scalar_t>(),
            accumulator.data_ptr<double>(),
            num_elements
        );
    });

    auto result = accumulator.div_(static_cast<double>(num_elements));
    return result.to(predictions.dtype());
}

PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
    m.def(""forward"", &forward, ""Mean Squared Error (MSE) forward (CUDA)"");
}",other,,,,,0,1,0.8051161,3333
"#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>
#include <pybind11/pybind11.h>

namespace py = pybind11;

// Custom fully-connected layer kernel with manual loop unrolling
__global__ void fc_forward_unrolled_kernel(const float* __restrict__ input,
                                            const float* __restrict__ weight,
                                            const float* __restrict__ bias,
                                            float* __restrict__ output,
                                            int in_features,
                                            int out_features,
                                            int batch_size) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    int total = batch_size * out_features;
    if (idx < total) {
        int b = idx / out_features;
        int out_idx = idx % out_features;
        const float* in_ptr = input + b * in_features;
        const float* w_ptr = weight + out_idx * in_features;
        float sum = bias[out_idx];
        #pragma unroll
        for (int j = 0; j < unroll_factor; j++) {
            sum += in_ptr[j] * w_ptr[j];
        }

        int i = 0;
        int unroll_factor = 4;
        int limit = in_features - (in_features % unroll_factor);
        #pragma unroll
        for (i = 0; i < limit; i += unroll_factor) {
            sum += in_ptr[i]     * w_ptr[i]     +
                   in_ptr[i + 1] * w_ptr[i + 1] +
                   in_ptr[i + 2] * w_ptr[i + 2] +
                   in_ptr[i + 3] * w_ptr[i + 3];
        }
        // Handle remaining elements
        for (; i < in_features; i++) {
            sum += in_ptr[i] * w_ptr[i];
        }
        output[idx] = sum;
    }
}

// Helper function to launch our custom FC kernel
torch::Tensor fc_layer_cuda(torch::Tensor input, torch::Tensor weight, torch::Tensor bias) {
    int batch_size = input.size(0);
    int in_features = input.size(1);
    int out_features = weight.size(0);

    auto output = torch::empty({batch_size, out_features}, input.options());
    int total_elements = batch_size * out_features;
    int threads = 256;
    int blocks = (total_elements + threads - 1) / threads;

    fc_forward_unrolled_kernel<<<blocks, threads>>>(
        input.data_ptr<float>(),
        weight.data_ptr<float>(),
        bias.data_ptr<float>(),
        output.data_ptr<float>(),
        in_features,
        out_features,
        batch_size
    );
    
    return output;
}

// Forward function implementing AlexNet architecture
// Uses cuDNN-optimized conv2d and a custom fully-connected kernel with manual loop unrolling

torch::Tensor forward(torch::Tensor x, py::object params, bool is_training) {
    auto conv1_weight = params.attr(""get"")(""conv1_weight"").cast<torch::Tensor>();
    auto conv1_bias   = params.attr(""get"")(""conv1_bias"").cast<torch::Tensor>();
    auto conv2_weight = params.attr(""get"")(""conv2_weight"").cast<torch::Tensor>();
    auto conv2_bias   = params.attr(""get"")(""conv2_bias"").cast<torch::Tensor>();
    auto conv3_weight = params.attr(""get"")(""conv3_weight"").cast<torch::Tensor>();
    auto conv3_bias   = params.attr(""get"")(""conv3_bias"").cast<torch::Tensor>();
    auto conv4_weight = params.attr(""get"")(""conv4_weight"").cast<torch::Tensor>();
    auto conv4_bias   = params.attr(""get"")(""conv4_bias"").cast<torch::Tensor>();
    auto conv5_weight = params.attr(""get"")(""conv5_weight"").cast<torch::Tensor>();
    auto conv5_bias   = params.attr(""get"")(""conv5_bias"").cast<torch::Tensor>();
    
    auto fc1_weight   = params.attr(""get"")(""fc1_weight"").cast<torch::Tensor>();
    auto fc1_bias     = params.attr(""get"")(""fc1_bias"").cast<torch::Tensor>();
    auto fc2_weight   = params.attr(""get"")(""fc2_weight"").cast<torch::Tensor>();
    auto fc2_bias     = params.attr(""get"")(""fc2_bias"").cast<torch::Tensor>();
    auto fc3_weight   = params.attr(""get"")(""fc3_weight"").cast<torch::Tensor>();
    auto fc3_bias     = params.attr(""get"")(""fc3_bias"").cast<torch::Tensor>();

    // Convolution and pooling layers (using cuDNN optimized torch ops)
    x = torch::conv2d(x, conv1_weight, conv1_bias, {4, 4}, {2, 2});
    x = torch::relu(x);
    x = torch::max_pool2d(x, {3, 3}, {2, 2});

    x = torch::conv2d(x, conv2_weight, conv2_bias, {1, 1}, {2, 2});
    x = torch::relu(x);
    x = torch::max_pool2d(x, {3, 3}, {2, 2});

    x = torch::conv2d(x, conv3_weight, conv3_bias, {1, 1}, {1, 1});
    x = torch::relu(x);

    x = torch::conv2d(x, conv4_weight, conv4_bias, {1, 1}, {1, 1});
    x = torch::relu(x);

    x = torch::conv2d(x, conv5_weight, conv5_bias, {1, 1}, {1, 1});
    x = torch::relu(x);
    x = torch::max_pool2d(x, {3, 3}, {2, 2});

    x = x.flatten(1);

    // Fully-connected layers using custom kernel with manual loop unrolling
    x = fc_layer_cuda(x, fc1_weight, fc1_bias);
    x = torch::relu(x);
    x = torch::dropout(x, 0.0, is_training);

    x = fc_layer_cuda(x, fc2_weight, fc2_bias);
    x = torch::relu(x);
    x = torch::dropout(x, 0.0, is_training);

    x = fc_layer_cuda(x, fc3_weight, fc3_bias);

    return x;
}

PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
    m.def(""forward"", &forward, ""AlexNet forward with unrolled loops in FC layers"");
}
",other,,,,,0,1,0.80485153,5193
"#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>

// Kernel that uses warp-level primitives to perform reduction over the specified dimension.

template <typename scalar_t>
__global__ void warp_sum_reduce_kernel(
    const scalar_t* __restrict__ input,
    scalar_t* __restrict__ output,
    int64_t reduce_size,
    int64_t inner_size,
    int64_t total_output) {

    // Each block handles one output element (one (outer, inner) pair).
    int idx = blockIdx.x;  // index for output element
    if (idx >= total_output) return;

    // Determine corresponding outer and inner indices
    int outer_idx = idx / inner_size;
    int inner_idx = idx % inner_size;

    scalar_t sum = 0;
    // Use lane id of the warp; assume blockDim.x == warpSize (32 threads)
    int lane = threadIdx.x;

    // Pre-calculate base offset to avoid redundant computations
    const int64_t base_offset = outer_idx * reduce_size * inner_size + inner_idx;
    
    // Each thread in the warp sums elements from the reduction dim in a strided manner
    for (int i = lane; i < reduce_size; i += warpSize) {
        int64_t offset = base_offset + i * inner_size;
        // Use __ldg() for read-only access to global memory
        sum += __ldg(&input[offset]);
    }

    // Use warp-level shuffle to reduce the partial sums within the warp
    // Cache the current partial sum in register
    scalar_t partial = sum;
    const unsigned int mask = 0xffffffff;
    
    #pragma unroll
    for (int offset = warpSize / 2; offset > 0; offset /= 2) {
        const scalar_t n = __shfl_down_sync(mask, partial, offset);
        partial += n;
    }
    sum = partial;

    // The first lane writes the result
    if (lane == 0) {
        // Ensure memory access is aligned
        atomicAdd(&output[idx], sum);
    }
}

// Host function wrapping the kernel launch

torch::Tensor sum_reduce_cuda(torch::Tensor input, int64_t dim) {
    // Adjust negative dimensions
    if (dim < 0) dim += input.dim();
    auto sizes = input.sizes().vec();
    int64_t reduce_size = sizes[dim];

    // Compute outer_size: product of dimensions before the reduction dim
    int64_t outer_size = 1;
    for (int i = 0; i < dim; i++) {
        outer_size *= sizes[i];
    }

    // Compute inner_size: product of dimensions after the reduction dim
    int64_t inner_size = 1;
    for (int i = dim + 1; i < sizes.size(); i++) {
        inner_size *= sizes[i];
    }

    // Prepare output tensor with reduce dimension set to 1
    sizes[dim] = 1;
    auto output = torch::empty(sizes, input.options());

    // Total number of output elements
    int64_t total_output = outer_size * inner_size;

    // Calculate optimal grid size
    const int threads = 32;  // warp size
    const int max_blocks = 65535;  // Maximum blocks per grid dimension
    const int min_blocks = (total_output + threads - 1) / threads;
    const int blocks = min(min_blocks, max_blocks);

    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), ""sum_reduce_cuda"", ([&] {
        warp_sum_reduce_kernel<scalar_t><<<blocks, threads>>>(
            input.data_ptr<scalar_t>(),
            output.data_ptr<scalar_t>(),
            reduce_size,
            inner_size,
            total_output
        );
    }));

    return output;
}

PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
    m.def(""forward"", &sum_reduce_cuda, ""Sum reduction forward (CUDA) using warp-level primitives"");
}
",other,,,,,0,1,0.8041073,3426
"#include <torch/extension.h>
#include <ATen/ATen.h>
#include <cuda.h>
#include <cuda_runtime.h>
#include <vector>
#include <cmath>
#include <limits>

// CUDA kernel minimizing warp divergence
__global__ void masked_fill_relu_kernel_warp_divergence(
    float* __restrict__ att_data,
    const bool* __restrict__ mask_data,
    const int64_t numel,
    const float fill_value
) {
    const int idx = blockIdx.x * blockDim.x + threadIdx.x;
    const int stride = blockDim.x * gridDim.x;

    // Use a mask to control the operation, reducing warp divergence
    for (int i = idx; i < numel; i += stride) {
        float val = att_data[i];
        bool mask = __ldg(&mask_data[i]);

        // Avoid conditional logic by using arithmetic operations
        val = mask ? fill_value : val;
        val = fmaxf(val, 0.0f);
        att_data[i] = val;
    }
}

// Forward function
at::Tensor forward(
    at::Tensor x,                 // [B, T, C]
    at::Tensor c_attn_weight,     // [3C, C]
    at::Tensor c_attn_bias,       // [3C]
    at::Tensor bias,              // [1, 1, T, T]
    int64_t n_head,
    int64_t n_embd
) {
    TORCH_CHECK(x.is_cuda(), ""x must be a CUDA tensor"");
    TORCH_CHECK(c_attn_weight.is_cuda(), ""c_attn_weight must be a CUDA tensor"");
    TORCH_CHECK(c_attn_bias.is_cuda(), ""c_attn_bias must be a CUDA tensor"");
    TORCH_CHECK(bias.is_cuda(), ""bias must be a CUDA tensor"");

    int64_t B = x.size(0);
    int64_t T = x.size(1);
    int64_t C = x.size(2);
    int64_t hs = C / n_head;

    // Compute qkv = x @ c_attn_weight.T + c_attn_bias
    at::Tensor x_2d = x.view({B * T, C});
    at::Tensor qkv = at::addmm(c_attn_bias, x_2d, c_attn_weight.t());
    qkv = qkv.view({B, T, 3 * C});

    // Split qkv into q, k, v
    std::vector<at::Tensor> qkv_split = qkv.split(C, /*dim=*/2);
    at::Tensor q = qkv_split[0].view({B, T, n_head, hs}).permute({0, 2, 1, 3}).contiguous();
    at::Tensor k = qkv_split[1].view({B, T, n_head, hs}).permute({0, 2, 1, 3}).contiguous();
    at::Tensor v = qkv_split[2].view({B, T, n_head, hs}).permute({0, 2, 1, 3}).contiguous();

    // Compute attention scores
    auto scale = 1.0 / std::sqrt(static_cast<float>(hs));
    at::Tensor k_t = k.transpose(-2, -1).contiguous();
    at::Tensor att = at::matmul(q, k_t) * scale;

    // Prepare mask and attention tensor
    at::Tensor bias_slice = bias.index({at::indexing::Slice(), 
                                      at::indexing::Slice(), 
                                      at::indexing::Slice(0, T), 
                                      at::indexing::Slice(0, T)}).contiguous();
    at::Tensor mask = bias_slice.eq(0).to(torch::kBool).expand({B, n_head, T, T}).contiguous();
    att = att.contiguous();

    // Launch kernel
    int64_t att_numel = att.numel();
    int threads = 512;
    int blocks = (att_numel + threads - 1) / threads;

    masked_fill_relu_kernel_warp_divergence<<<blocks, threads>>>(
        att.data_ptr<float>(),
        mask.data_ptr<bool>(),
        att_numel,
        -std::numeric_limits<float>::infinity()
    );

    // Compute output
    at::Tensor y = at::matmul(att, v);
    return y.permute({0, 2, 1, 3}).contiguous().view({B, T, C});
}

PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
    m.def(""forward"", &forward, ""module forward"");
}
",other,,,,,0,1,0.8034606,3283
"#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>
#include <vector>
#include <cmath>

// Device functions for Mish activation for float and double types.
__device__ inline float mish_activation(float x) {
    return x * tanhf(__expf(x));
}

__device__ inline double mish_activation(double x) {
    return x * tanh(log(1.0 + exp(x)));
}

// Templated CUDA kernel that performs 2D convolution (stride=1, no padding)
// followed by two sequential Mish activations.

// This kernel uses __ldg() for read-only global memory loads to optimize memory throughput,
// assuming that the input, weight, and bias pointers are aligned to 128-bit boundaries.

template <typename scalar_t>
__global__ void conv2d_mish_kernel_aligned(
    const scalar_t* __restrict__ input,
    const scalar_t* __restrict__ weight,
    const scalar_t* __restrict__ bias,
    scalar_t* __restrict__ output,
    int batch_size,
    int in_channels,
    int in_h,
    int in_w,
    int out_channels,
    int k_h,
    int k_w,
    int out_h,
    int out_w) {

    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    int total = batch_size * out_channels * out_h * out_w;
    if (idx >= total) return;

    // Compute the output pixel indices from the flattened index.
    int ow = idx % out_w;
    int oh = (idx / out_w) % out_h;
    int oc = (idx / (out_w * out_h)) % out_channels;
    int b = idx / (out_w * out_h * out_channels);

    // Use __ldg() for read-only bias load
    scalar_t sum = __ldg(&bias[oc]);

    // Convolution computation using __ldg() for global memory loads
    for (int ic = 0; ic < in_channels; ++ic) {
        for (int kh = 0; kh < k_h; ++kh) {
            int in_y = oh + kh;
            for (int kw = 0; kw < k_w; ++kw) {
                int in_x = ow + kw;
                int input_idx = b * (in_channels * in_h * in_w) 
                              + ic * (in_h * in_w) 
                              + in_y * in_w 
                              + in_x;
                int weight_idx = oc * (in_channels * k_h * k_w) 
                               + ic * (k_h * k_w) 
                               + kh * k_w 
                               + kw;
                sum += __ldg(&input[input_idx]) * __ldg(&weight[weight_idx]);
            }
        }
    }

    // Apply two sequential Mish activations
    scalar_t tmp = mish_activation(sum);
    scalar_t out_val = mish_activation(tmp);

    // Write the result to the output tensor (global memory write).
    output[idx] = out_val;
}

// Host wrapper function for launching the CUDA kernel.
at::Tensor forward(at::Tensor input, at::Tensor conv_weight, at::Tensor conv_bias) {
    // Expected input dimensions: [batch_size, in_channels, in_h, in_w]
    // conv_weight dimensions: [out_channels, in_channels, k_h, k_w]
    // conv_bias dimensions: [out_channels]

    const auto batch_size = input.size(0);
    const auto in_channels = input.size(1);
    const auto in_h = input.size(2);
    const auto in_w = input.size(3);

    const auto out_channels = conv_weight.size(0);
    const auto k_h = conv_weight.size(2);
    const auto k_w = conv_weight.size(3);

    // Compute output spatial dimensions (assuming stride=1 and no padding).
    const auto out_h = in_h - k_h + 1;
    const auto out_w = in_w - k_w + 1;

    // Allocate the output tensor.
    auto output = at::empty({batch_size, out_channels, out_h, out_w}, input.options());

    const int total = batch_size * out_channels * out_h * out_w;
    const int blockSize = 256;
    const int numBlocks = (total + blockSize - 1) / blockSize;

    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), ""conv2d_mish_forward_cuda_aligned"", ([&] {
        conv2d_mish_kernel_aligned<scalar_t><<<numBlocks, blockSize>>>(
            input.data_ptr<scalar_t>(),
            conv_weight.data_ptr<scalar_t>(),
            conv_bias.data_ptr<scalar_t>(),
            output.data_ptr<scalar_t>(),
            batch_size,
            in_channels,
            in_h,
            in_w,
            out_channels,
            k_h,
            k_w,
            out_h,
            out_w);
    }));

    return output;
}

PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
    m.def(""forward"", &forward, ""2D Convolution with double Mish activation optimized for aligned global loads (CUDA)"");
}
",conv2d,,,,,0,1,0.8019481,4296
"#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>
#include <math.h>

#define CHECK_CUDA(x) TORCH_CHECK(x.is_cuda(), #x "" must be a CUDA tensor"")
#define CHECK_CONTIGUOUS(x) TORCH_CHECK(x.is_contiguous(), #x "" must be contiguous"")
#define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)

#define WARP_SIZE 32
#define BLOCK_SIZE 256
#define ELEMENTS_PER_THREAD 4

__device__ __forceinline__ float process_element(float val) {
    // Combined ReLU and LeakyReLU without branching
    float leaky = 0.01f * val;
    val = val + (leaky - val) * (val < 0.0f);
    
    // GELU approximation
    const float sqrt_2_over_pi = 0.797884583f;
    const float coef = 0.044715f;
    float cdf = val + coef * val * val * val;
    cdf = sqrt_2_over_pi * cdf;
    cdf = 0.5f * (1.0f + tanhf(cdf));
    val = val * cdf;
    
    // Sigmoid
    val = 1.0f / (1.0f + expf(-val));
    
    return val;
}

__device__ __forceinline__ void process_vector(float4& data, const float bias_val) {
    data.x = process_element(data.x) + bias_val;
    data.y = process_element(data.y) + bias_val;
    data.z = process_element(data.z) + bias_val;
    data.w = process_element(data.w) + bias_val;
}

__global__ void apply_activations_and_bias_kernel(
    float* __restrict__ output,
    const float* __restrict__ bias,
    const int batch_size,
    const int out_channels,
    const int spatial_size,
    const int total_elements
) {
    const int tid = blockIdx.x * blockDim.x + threadIdx.x;
    const int lane_id = threadIdx.x & (WARP_SIZE - 1);
    const int vector_offset = tid * ELEMENTS_PER_THREAD;
    const int total_vectors = total_elements / ELEMENTS_PER_THREAD;
    
    // Pre-compute channel index and bias value
    const int channel_idx = (vector_offset / spatial_size) % out_channels;
    const float bias_val = __ldg(&bias[channel_idx]);
    
    // Main processing loop - no divergent branches
    if (vector_offset < total_vectors) {
        float4 data;
        data = *reinterpret_cast<float4*>(&output[vector_offset * ELEMENTS_PER_THREAD]);
        process_vector(data, bias_val);
        *reinterpret_cast<float4*>(&output[vector_offset * ELEMENTS_PER_THREAD]) = data;
    }
    
    // Handle remaining elements using predication instead of branching
    const int remainder_start = total_vectors * ELEMENTS_PER_THREAD;
    const int remainder_idx = remainder_start + tid;
    
    // Use predication for remaining elements
    bool valid = remainder_idx < total_elements;
    if (valid) {
        float val = output[remainder_idx];
        const int rem_channel_idx = (remainder_idx / spatial_size) % out_channels;
        const float rem_bias_val = __ldg(&bias[rem_channel_idx]);
        val = process_element(val) + rem_bias_val;
        output[remainder_idx] = val;
    }
}

torch::Tensor module_fn_cuda(
    torch::Tensor x,
    torch::Tensor conv_weight,
    torch::Tensor conv_bias,
    torch::Tensor bias
) {
    CHECK_INPUT(x);
    CHECK_INPUT(conv_weight);
    CHECK_INPUT(conv_bias);
    CHECK_INPUT(bias);

    auto output = torch::conv3d(x, conv_weight, conv_bias);

    const int batch_size = output.size(0);
    const int out_channels = output.size(1);
    const int depth = output.size(2);
    const int height = output.size(3);
    const int width = output.size(4);
    const int spatial_size = depth * height * width;
    const int total_elements = batch_size * out_channels * spatial_size;
    
    const int vectors = (total_elements + ELEMENTS_PER_THREAD - 1) / ELEMENTS_PER_THREAD;
    const int blocks = (vectors + BLOCK_SIZE - 1) / BLOCK_SIZE;

    apply_activations_and_bias_kernel<<<blocks, BLOCK_SIZE>>>(
        output.data_ptr<float>(),
        bias.data_ptr<float>(),
        batch_size,
        out_channels,
        spatial_size,
        total_elements
    );

    return output;
}

PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
    m.def(""forward"", &module_fn_cuda, ""CUDA implementation of module_fn"");
}",conv3d,,,,,0,1,0.798562,3952
"#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>
#include <c10/cuda/CUDAGuard.h>

#define WARP_SIZE 32
#define BLOCK_SIZE 128

// This kernel handles larger workloads by processing data in strides.
// Each warp handles multiple elements in the workload by iterating through strides of the input.
// Sync-free operation within a warp and no need for global synchronization thanks to full warp processing.

__global__ void warp_single_stride_kernel(const float* __restrict__ x,
                                           const float* __restrict__ weight,
                                           const float* __restrict__ bias,
                                           float* __restrict__ out,
                                           int in_features,
                                           int out_features) {
    int batch_idx = blockIdx.x;       // Batch index
    int out_idx = blockIdx.y;         // Output feature index
    int tid = threadIdx.x;            // Thread index within the block

    const float* x_row = x + batch_idx * in_features;
    const float* w_row = weight + out_idx * in_features;

    float sum = 0.0f;
    int vec_count = in_features / 4;  // Number of float4 loads
    int rem = in_features % 4;        // Remaining elements after full vector loads

    const float4* x_vec = reinterpret_cast<const float4*>(x_row);
    const float4* w_vec = reinterpret_cast<const float4*>(w_row);

    // Stride loop for processing multiple elements per thread
    for (int i = tid; i < vec_count; i += blockDim.x) {
        float4 a = __ldg(&x_vec[i]);
        float4 b = __ldg(&w_vec[i]);
        sum += a.x * b.x + a.y * b.y + a.z * b.z + a.w * b.w;
    }

    // Handle remaining scalar elements
    int base = vec_count * 4;
    for (int i = tid; i < rem; i += blockDim.x) {
        sum += __ldg(x_row + base + i) * __ldg(w_row + base + i);
    }

    // Warp-level reduction using shuffle operations
    for (int offset = WARP_SIZE/2; offset > 0; offset /= 2) {
         sum += __shfl_down_sync(0xffffffff, sum, offset);
    }

    // Thread 0 writes the result
    if (tid % WARP_SIZE == 0) {
         float result = sum + __ldg(&bias[out_idx]);
         out[batch_idx * out_features + out_idx] = (result > 0.0f) ? result : 0.0f;
    }
}

// Host function to launch the kernel

torch::Tensor warp_single_stride_forward(torch::Tensor x,
                                          torch::Tensor weight,
                                          torch::Tensor bias) {
    TORCH_CHECK(x.is_cuda(), ""x must be a CUDA tensor"");
    TORCH_CHECK(weight.is_cuda(), ""weight must be a CUDA tensor"");
    TORCH_CHECK(bias.is_cuda(), ""bias must be a CUDA tensor"");

    int batch_size = x.size(0);
    int in_features = x.size(1);
    int out_features = weight.size(0);

    auto out = torch::empty({batch_size, out_features}, x.options());

    // Kernel launch configuration
    dim3 gridDim(batch_size, out_features);
    dim3 blockDim(BLOCK_SIZE); // More threads per block for handling strides

    cudaStream_t stream = c10::cuda::getCurrentCUDAStream();
    warp_single_stride_kernel<<<gridDim, blockDim, 0, stream>>>(
         x.data_ptr<float>(),
         weight.data_ptr<float>(),
         bias.data_ptr<float>(),
         out.data_ptr<float>(),
         in_features,
         out_features);

    return out;
}

PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
  m.def(""forward"", &warp_single_stride_forward, ""GEMM with bias and ReLU using warp stride handling (CUDA)"");
}
",gemm,,,,,0,1,0.79827374,3518
