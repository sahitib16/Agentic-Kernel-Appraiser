code_text,op_kind,dtype,layout,math_mode,conv_role,correct,pred,proba,code_len
"#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>

__global__ void forward_kernel(
    const float* x,
    const float* weight,
    const float* bias,
    float* output,
    int B,
    int IC,
    int OC,
    int H,
    int W
) {
    const int tid = blockIdx.x * blockDim.x + threadIdx.x;
    const int stride = blockDim.x * gridDim.x;
    const int total_elements = B * OC * H * W;
    
    // Each thread processes multiple elements with stride
    for (int index = tid; index < total_elements; index += stride) {
        // Decompose linear index into 4D tensor coordinates
        const int w = index % W;
        const int h = (index / W) % H;
        const int oc = (index / (W * H)) % OC;
        const int b = index / (W * H * OC);

        float sum = 0.0f;
        
        // 1x1 convolution equivalent to matmul over channels
        for (int ic = 0; ic < IC; ++ic) {
            const int x_offset = b * IC * H * W + ic * H * W + h * W + w;
            const int w_offset = oc * IC + ic;
            sum += x[x_offset] * weight[w_offset];
        }
        
        // Handle optional bias
        output[index] = bias ? sum + bias[oc] : sum;
    }
}

torch::Tensor forward_cuda(
    torch::Tensor x,
    torch::Tensor weight,
    torch::optional<torch::Tensor> bias
) {
    // Input validation
    TORCH_CHECK(x.is_cuda() && weight.is_cuda(), ""Inputs must be CUDA tensors"");
    TORCH_CHECK(x.dim() == 4, ""x must be 4D (NCHW)"");
    TORCH_CHECK(weight.dim() == 4, ""Weight must be 4D (OC, IC, 1, 1)"");
    if (bias) {
        TORCH_CHECK(bias->is_cuda(), ""Bias must be CUDA tensor"");
        TORCH_CHECK(bias->dim() == 1, ""Bias must be 1D"");
    }

    const int B = x.size(0);
    const int IC = x.size(1);
    const int H = x.size(2);
    const int W = x.size(3);
    const int OC = weight.size(0);

    TORCH_CHECK(weight.size(1) == IC, ""Input/output channel mismatch"");
    TORCH_CHECK(weight.size(2) == 1 && weight.size(3) == 1, ""Kernel must be 1x1"");
    if (bias) {
        TORCH_CHECK(bias->size(0) == OC, ""Bias/out channel mismatch"");
    }

    auto output = torch::empty({B, OC, H, W}, x.options());

    const float* x_ptr = x.data_ptr<float>();
    const float* w_ptr = weight.data_ptr<float>();
    const float* b_ptr = bias ? bias->data_ptr<float>() : nullptr;
    float* out_ptr = output.data_ptr<float>();

    // Optimize thread and block count
    const int threads = 256;
    const int max_blocks = 65535;
    const int min_blocks = (B * OC * H * W + threads - 1) / threads;
    const int blocks = min(max_blocks, min_blocks);
    
    forward_kernel<<<blocks, threads>>>(
        x_ptr, w_ptr, b_ptr, out_ptr,
        B, IC, OC, H, W
    );
    
    cudaError_t err = cudaGetLastError();
    TORCH_CHECK(err == cudaSuccess, ""CUDA Error: "", cudaGetErrorString(err));

    return output;
}

PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
    m.def(""forward"", &forward_cuda, ""Pointwise 2D convolution forward (CUDA)"");
}",conv2d,,,,,1,0,0.022702396,2970
"#include <torch/extension.h>
#include <cooperative_groups.h>

constexpr int VEC_WIDTH = 4;
constexpr int BLOCK_SIZE = 128;

namespace cg = cooperative_groups;

template <typename scalar_t, int vec_width>
struct VectorizedAccess {
    using VecType = typename std::conditional<std::is_same<scalar_t, float>::value, float4, double2>::type;
    __device__ __forceinline__ static VecType load(const scalar_t* ptr) {
        return *reinterpret_cast<const VecType*>(ptr);
    }
};

template <typename scalar_t>
__device__ __forceinline__ scalar_t warp_reduce(scalar_t val) {
#pragma unroll
    for (int offset = 16; offset > 0; offset >>= 1)
        val += __shfl_down_sync(0xffffffff, val, offset);
    return val;
}

// Removed the block_reduce function that used unsupported cooperative_groups features

template <typename scalar_t, int vec_width>
__global__ void matvec_mul_kernel(const scalar_t* A, const scalar_t* B, scalar_t* C, int64_t M, int64_t K) {
    constexpr int vec_elements = sizeof(typename VectorizedAccess<scalar_t, vec_width>::VecType) / sizeof(scalar_t);
    __shared__ scalar_t smem[BLOCK_SIZE];
    
    cg::thread_block block = cg::this_thread_block();
    cg::thread_block_tile<32> warp = cg::tiled_partition<32>(block);
    
    const int64_t row = blockIdx.x;
    const int64_t tid = block.thread_rank();
        
    if (row >= M) return;
    
    const scalar_t* row_ptr = A + row * K;
    scalar_t thread_sum = 0;
    
    for (int64_t base = 0; base < K; base += BLOCK_SIZE * vec_elements) {
        int64_t k = base + tid * vec_elements;
        if (k + vec_elements <= K) {
            auto a_vec = VectorizedAccess<scalar_t, vec_width>::load(row_ptr + k);
            auto b_vec = VectorizedAccess<scalar_t, vec_width>::load(B + k);
            
#pragma unroll
            for (int i = 0; i < vec_elements; ++i)
                thread_sum += reinterpret_cast<scalar_t*>(&a_vec)[i] * reinterpret_cast<scalar_t*>(&b_vec)[i];
        } else {
            for (int i = 0; k + i < K && i < vec_elements; ++i)
                thread_sum += row_ptr[k + i] * B[k + i];
        }
    }
    
    thread_sum = warp_reduce(thread_sum);
    
    if (warp.thread_rank() == 0)
        smem[warp.meta_group_rank()] = thread_sum;
    
    __syncthreads();
    
    if (tid < 32) {
        scalar_t block_sum = (tid < block.size() / 32) ? smem[tid] : 0;
        block_sum = warp_reduce(block_sum);
        
        if (tid == 0)
            C[row] = block_sum;
    }
}

torch::Tensor matvec_mul_cuda(torch::Tensor A, torch::Tensor B) {
    TORCH_CHECK(A.is_cuda() && B.is_cuda(), ""Inputs must be CUDA tensors"");
    
    auto M = A.size(0);
    auto K = A.size(1);
    auto C = torch::zeros({M}, A.options());
    
    dim3 blocks(M);
    dim3 threads(BLOCK_SIZE);
    
    AT_DISPATCH_FLOATING_TYPES(A.scalar_type(), ""matvec_mul_cuda"", [&] {
        matvec_mul_kernel<scalar_t, VEC_WIDTH><<<blocks, threads>>>(
            A.contiguous().data_ptr<scalar_t>(),
            B.contiguous().view(-1).data_ptr<scalar_t>(),
            C.data_ptr<scalar_t>(),
            M,
            K
        );
    });
    
    return C.view({M, 1});
}

PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
    m.def(""forward"", &matvec_mul_cuda, ""Matrix-Vector Multiplication (CUDA)"");
}
",other,,,,,1,0,0.04005022,3274
"#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>

// Define maximum sizes for constant memory. Adjust as needed based on expected tensor sizes and hardware limits.
// For example, 64KB constant memory is available, so the number of float elements should be <= 16384.
// Here we assume the weight matrix (out_features x in_features) and bias (out_features) fit within these limits.

#define MAX_WEIGHT_SIZE 1024
#define MAX_BIAS_SIZE 256

// Read-only constant memory for weight and bias
__constant__ float const_weight[MAX_WEIGHT_SIZE];
__constant__ float const_bias[MAX_BIAS_SIZE];

// GELU approximation following PyTorch implementation
__device__ float gelu(float x) {
    const float sqrt_2_over_pi = 0.7978845608028654f;
    const float coef = 0.044715f;
    float cdf = 0.5f * (1.0f + tanhf(sqrt_2_over_pi * x * (1.0f + coef * x * x)));
    return x * cdf;
}

// Kernel for computing linear transformation and GELU using constant memory for read-only weight and bias
__global__ void linear_gelu_kernel(
    const float* x,
    float* intermediate,
    const int batch_size,
    const int in_features,
    const int out_features
) {
    const int row = blockIdx.x;
    const int col = threadIdx.x;

    if (row < batch_size && col < out_features) {
        float sum = 0.0f;
        // Linear transformation using weight from constant memory
        for (int k = 0; k < in_features; k++) {
            sum += x[row * in_features + k] * const_weight[col * in_features + k];
        }
        sum += const_bias[col];
        // Apply GELU activation
        intermediate[row * out_features + col] = gelu(sum);
    }
}

// Kernel for softmax computation
__global__ void softmax_kernel(
    float* output,
    const int batch_size,
    const int out_features
) {
    const int row = blockIdx.x;
    if (row < batch_size) {
        // Find max for numerical stability
        float max_val = output[row * out_features];
        for (int i = 1; i < out_features; i++) {
            max_val = max(max_val, output[row * out_features + i]);
        }
        
        // Compute exponentials and their sum
        float sum = 0.0f;
        for (int i = 0; i < out_features; i++) {
            float val = expf(output[row * out_features + i] - max_val);
            output[row * out_features + i] = val;
            sum += val;
        }
        
        // Normalize to get softmax
        for (int i = 0; i < out_features; i++) {
            output[row * out_features + i] /= sum;
        }
    }
}

// Forward function that sets up constant memory and launches the kernels
torch::Tensor forward(
    torch::Tensor x,
    torch::Tensor weight,
    torch::Tensor bias
) {
    const int batch_size = x.size(0);
    const int in_features = x.size(1);
    const int out_features = weight.size(0);
    
    // Ensure the weight and bias tensors fit in the allocated constant memory
    TORCH_CHECK(weight.numel() <= MAX_WEIGHT_SIZE, ""Weight tensor exceeds constant memory limits."");
    TORCH_CHECK(bias.numel() <= MAX_BIAS_SIZE, ""Bias tensor exceeds constant memory limits."");
    
    auto options = torch::TensorOptions()
        .dtype(x.dtype())
        .device(x.device());
    auto output = torch::empty({batch_size, out_features}, options);
    
    // Copy weight and bias to constant memory
    cudaMemcpyToSymbol(const_weight, weight.data_ptr<float>(), weight.numel() * sizeof(float));
    cudaMemcpyToSymbol(const_bias, bias.data_ptr<float>(), bias.numel() * sizeof(float));
    
    dim3 blocks(batch_size);
    dim3 threads(out_features);
    
    // Launch the linear + GELU kernel using constant memory
    linear_gelu_kernel<<<blocks, threads>>>(
        x.data_ptr<float>(),
        output.data_ptr<float>(),
        batch_size,
        in_features, 
        out_features
    );
    
    // Launch the softmax kernel
    softmax_kernel<<<batch_size, 1>>>(
        output.data_ptr<float>(),
        batch_size,
        out_features
    );
    
    return output;
}

PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
    m.def(""forward"", &forward, ""Linear + GELU + Softmax forward with constant memory optimization"");
}
",other,,,,,1,0,0.07649175,4145
"#include <torch/extension.h>
#include <vector>
#include <cuda.h>

template<typename T>
__global__ void linear_kernel(
    const T* __restrict__ input,
    const T* __restrict__ weight,
    const T* __restrict__ bias,
    T* __restrict__ output,
    int in_dim,
    int batch_size,
    int out_dim) {
    
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx >= batch_size * out_dim) return;

    int row = idx / out_dim;
    int col = idx % out_dim;

    T sum = 0;
    const int vec_size = 4;
    int k = 0;
    
    // Vectorized loads for aligned 128-bit accesses
    for (; k <= in_dim - vec_size; k += vec_size) {
        float4 in = *reinterpret_cast<const float4*>(&input[row * in_dim + k]);
        float4 wt = *reinterpret_cast<const float4*>(&weight[col * in_dim + k]);
        sum += in.x * wt.x + in.y * wt.y + in.z * wt.z + in.w * wt.w;
    }
    
    // Remainder elements
    for (; k < in_dim; ++k) {
        sum += __ldg(&input[row * in_dim + k]) * __ldg(&weight[col * in_dim + k]);
    }
    
    output[idx] = sum + __ldg(&bias[col]);
}

torch::Tensor forward_optimized(
    torch::Tensor x,
    std::vector<std::vector<torch::Tensor>> stage_params,
    torch::Tensor fc_weight,
    torch::Tensor fc_bias,
    bool is_training) {

    // Original stage processing remains unchanged
    for (auto& params : stage_params) {
        auto conv1_weight = params[0];
        auto conv1_bias = params[1];
        auto bn1_weight = params[2];
        auto bn1_bias = params[3];
        auto bn1_mean = params[4];
        auto bn1_var = params[5];
        auto conv2_weight = params[6];
        auto conv2_bias = params[7];
        auto bn2_weight = params[8];
        auto bn2_bias = params[9];
        auto bn2_mean = params[10];
        auto bn2_var = params[11];

        x = torch::conv2d(x, conv1_weight, conv1_bias, 1, 1);
        x = torch::batch_norm(x, bn1_weight, bn1_bias, bn1_mean, bn1_var, 
                            is_training, 0.1, 1e-5, true);
        x = torch::relu(x);

        x = torch::conv2d(x, conv2_weight, conv2_bias, 1, 1);
        x = torch::batch_norm(x, bn2_weight, bn2_bias, bn2_mean, bn2_var,
                            is_training, 0.1, 1e-5, true);
        x = torch::relu(x);

        x = torch::max_pool2d(x, {2, 2}, {2, 2});
    }

    x = torch::mean(x, {2, 3}, /*keepdim=*/false);

    // Optimized linear layer
    int batch_size = x.size(0);
    int in_dim = x.size(1);
    int out_dim = fc_weight.size(0);
    
    torch::Tensor output = torch::zeros({batch_size, out_dim}, x.options());
    
    const int threads = 256;
    const int elements = batch_size * out_dim;
    const int blocks = (elements + threads - 1) / threads;
    
    AT_DISPATCH_FLOATING_TYPES(x.scalar_type(), ""linear_forward"", [&] {
        linear_kernel<scalar_t><<<blocks, threads>>>(
            x.data_ptr<scalar_t>(),
            fc_weight.data_ptr<scalar_t>(),
            fc_bias.data_ptr<scalar_t>(),
            output.data_ptr<scalar_t>(),
            in_dim,
            batch_size,
            out_dim);
    });
    
    return output;
}

PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
    m.def(""forward"", &forward_optimized, ""RegNet optimized forward"");
}",other,,,,,1,0,0.08241427,3206
"#include <torch/extension.h>
#include <vector>
#include <pybind11/pybind11.h>
#include <pybind11/stl.h>

namespace py = pybind11;

inline std::vector<int64_t> parseIntArrayRef(const py::object& obj) {
    std::vector<int64_t> result;
    if (py::isinstance<py::int_>(obj)) {
        result.push_back(obj.cast<int64_t>());
    } else if (py::isinstance<py::sequence>(obj)) {
        for (auto item : obj.cast<py::sequence>()) {
            result.push_back(py::cast<int64_t>(item));
        }
    } else {
        throw std::runtime_error(""Expected int or sequence of ints"");
    }
    return result;
}

template<int KERNEL_SIZE>
__global__ void conv_transpose2d_kernel(
    const float4* __restrict__ input,
    const float4* __restrict__ weight,
    float4* __restrict__ output,
    const int batch_size,
    const int in_channels,
    const int out_channels,
    const int height,
    const int width,
    const int stride,
    const int padding,
    const int output_padding
) {
    extern __shared__ float shared_mem[];
    float* shared_input = shared_mem;
    float* shared_weight = shared_mem + blockDim.x * KERNEL_SIZE * KERNEL_SIZE;

    const int tid = threadIdx.x;
    const int lid = tid % 4;  // Lane ID within float4
    const int wid = tid / 4;  // Warp ID
    
    // Load weights into shared memory using vectorized loads
    if (tid < out_channels) {
        #pragma unroll
        for (int i = 0; i < KERNEL_SIZE * KERNEL_SIZE / 4; i++) {
            float4 w = weight[tid * KERNEL_SIZE * KERNEL_SIZE / 4 + i];
            shared_weight[tid * KERNEL_SIZE * KERNEL_SIZE + i * 4 + 0] = w.x;
            shared_weight[tid * KERNEL_SIZE * KERNEL_SIZE + i * 4 + 1] = w.y;
            shared_weight[tid * KERNEL_SIZE * KERNEL_SIZE + i * 4 + 2] = w.z;
            shared_weight[tid * KERNEL_SIZE * KERNEL_SIZE + i * 4 + 3] = w.w;
        }
    }
    __syncthreads();

    // Process output points with fully unrolled loops
    #pragma unroll
    for (int n = 0; n < batch_size; n++) {
        #pragma unroll
        for (int h = 0; h < height; h += 4) {
            #pragma unroll
            for (int w = 0; w < width; w += 4) {
                float4 sum = make_float4(0.0f, 0.0f, 0.0f, 0.0f);
                
                // Fully unrolled kernel loops
                #pragma unroll
                for (int kh = 0; kh < KERNEL_SIZE; kh++) {
                    #pragma unroll
                    for (int kw = 0; kw < KERNEL_SIZE; kw++) {
                        const float weight_val = shared_weight[tid * KERNEL_SIZE * KERNEL_SIZE + kh * KERNEL_SIZE + kw];
                        const int ih = h + kh;
                        const int iw = w + kw;
                        
                        if (ih < height && iw < width) {
                            float4 in_val = input[(n * height * width + ih * width + iw) / 4];
                            sum.x += weight_val * in_val.x;
                            sum.y += weight_val * in_val.y;
                            sum.z += weight_val * in_val.z;
                            sum.w += weight_val * in_val.w;
                        }
                    }
                }
                
                // Store results using vectorized writes
                if (h < height && w < width) {
                    const int out_idx = (n * out_channels * height * width + tid * height * width + h * width + w) / 4;
                    output[out_idx] = sum;
                }
            }
        }
    }
}

torch::Tensor forward(
    torch::Tensor x,
    torch::Tensor weight,
    c10::optional<torch::Tensor> bias,
    py::object stride = py::int_(1),
    py::object padding = py::int_(0),
    py::object output_padding = py::int_(0),
    int64_t groups = 1
) {
    auto stride_vec = parseIntArrayRef(stride);
    auto padding_vec = parseIntArrayRef(padding);
    auto output_padding_vec = parseIntArrayRef(output_padding);
    
    return at::conv_transpose2d(
        x,
        weight,
        bias,
        stride_vec,
        padding_vec,
        output_padding_vec,
        groups,
        /* dilation */ {1, 1}
    );
}

PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
    m.def(""forward"", &forward, ""ConvTranspose2d forward"",
          py::arg(""x""),
          py::arg(""weight""),
          py::arg(""bias"") = py::none(),
          py::arg(""stride"") = 1,
          py::arg(""padding"") = 0,
          py::arg(""output_padding"") = 0,
          py::arg(""groups"") = 1);
}",conv2d,,,,,1,0,0.09212197,4452
"#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>
#include <cooperative_groups.h>
#include <cmath>
#include <limits>

namespace cg = cooperative_groups;

#define TILE_DIM 32
#define WARP_SIZE 32

// Device function to load input tile into shared memory
__device__ void load_input_tile(
    const float* __restrict__ input,
    float* __restrict__ tile,
    int row, int col,
    int M, int K,
    int tile_row, int tile_col
) {
    if (row < M && col < K) {
        tile[tile_row * TILE_DIM + tile_col] = input[row * K + col];
    } else {
        tile[tile_row * TILE_DIM + tile_col] = 0.0f;
    }
}

// Device function to load weight tile into shared memory
__device__ void load_weight_tile(
    const float* __restrict__ weight,
    float* __restrict__ tile,
    int row, int col,
    int K, int N,
    int tile_row, int tile_col
) {
    if (row < K && col < N) {
        tile[tile_row * TILE_DIM + tile_col] = weight[row * N + col];
    } else {
        tile[tile_row * TILE_DIM + tile_col] = 0.0f;
    }
}

// Device function to compute tile multiplication
__device__ float compute_tile_product(
    const float* __restrict__ tileA,
    const float* __restrict__ tileB,
    const cg::thread_block_tile<WARP_SIZE>& warp,
    int row, int col
) {
    float sum = 0.0f;
    #pragma unroll
    for (int k = 0; k < TILE_DIM; k++) {
        sum += tileA[row * TILE_DIM + k] * tileB[k * TILE_DIM + col];
    }
    return sum;
}

__global__ void modular_qkv_transform_kernel(
    const float* __restrict__ input,
    const float* __restrict__ weight,
    const float* __restrict__ bias,
    float* __restrict__ output,
    int M, int K, int N
) {
    cg::thread_block block = cg::this_thread_block();
    cg::thread_block_tile<WARP_SIZE> warp = cg::tiled_partition<WARP_SIZE>(block);

    __shared__ float shared_A[TILE_DIM][TILE_DIM];
    __shared__ float shared_B[TILE_DIM][TILE_DIM];

    const int row = blockIdx.y * TILE_DIM + threadIdx.y;
    const int col = blockIdx.x * TILE_DIM + threadIdx.x;
    const int tile_row = threadIdx.y;
    const int tile_col = threadIdx.x;

    float sum = 0.0f;

    for (int t = 0; t < (K + TILE_DIM - 1) / TILE_DIM; ++t) {
        load_input_tile(
            input,
            (float*)shared_A,
            row,
            t * TILE_DIM + tile_col,
            M, K,
            tile_row,
            tile_col
        );

        load_weight_tile(
            weight,
            (float*)shared_B,
            t * TILE_DIM + tile_row,
            col,
            K, N,
            tile_row,
            tile_col
        );

        block.sync();

        sum += compute_tile_product(
            (float*)shared_A,
            (float*)shared_B,
            warp,
            tile_row,
            tile_col
        );

        block.sync();
    }

    if (row < M && col < N) {
        output[row * N + col] = sum + bias[col];
    }
}

torch::Tensor forward(
    torch::Tensor x,
    torch::Tensor c_attn_weight,
    torch::Tensor c_attn_bias,
    torch::Tensor c_proj_weight,
    torch::Tensor c_proj_bias,
    torch::Tensor bias,
    int64_t n_head,
    int64_t n_embd,
    bool is_training
) {
    using namespace torch::indexing;
    const auto B = x.size(0), T = x.size(1), C = x.size(2);
    const auto M = B * T, K = C, N = 3 * C;

    auto x_2d = x.contiguous().view({M, K});
    auto weight_t = c_attn_weight.transpose(0, 1).contiguous();
    auto qkv = torch::empty({B, T, 3 * C}, x.options());

    dim3 threads(TILE_DIM, TILE_DIM);
    dim3 blocks((N + TILE_DIM - 1) / TILE_DIM, (M + TILE_DIM - 1) / TILE_DIM);

    modular_qkv_transform_kernel<<<blocks, threads>>>(
        x_2d.data_ptr<float>(),
        weight_t.data_ptr<float>(),
        c_attn_bias.data_ptr<float>(),
        qkv.data_ptr<float>(),
        M, K, N
    );

    auto q = qkv.slice(2, 0, n_embd);
    auto k = qkv.slice(2, n_embd, 2 * n_embd);
    auto v = qkv.slice(2, 2 * n_embd, 3 * n_embd);

    const auto head_size = C / n_head;
    q = q.reshape({B, T, n_head, head_size}).permute({0, 2, 1, 3}).contiguous();
    k = k.reshape({B, T, n_head, head_size}).permute({0, 2, 1, 3}).contiguous();
    v = v.reshape({B, T, n_head, head_size}).permute({0, 2, 1, 3}).contiguous();

    const auto scale = 1.0 / std::sqrt(static_cast<double>(head_size));
    auto att = torch::matmul(q, k.transpose(-2, -1)) * scale;
    
    auto mask = bias.index({Slice(), Slice(), Slice(None, T), Slice(None, T)});
    att.masked_fill_(mask.eq(0), -std::numeric_limits<float>::infinity());
    att = torch::softmax(att, -1);

    auto y = torch::matmul(att, v)
        .permute({0, 2, 1, 3})
        .contiguous()
        .reshape({B, T, C});

    return torch::addmm(
        c_proj_bias,
        y.view({M, C}),
        c_proj_weight.transpose(0, 1)
    ).view({B, T, C});
}

PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
    m.def(""forward"", &forward, ""Modular Tiled QKV Transform Forward"");
}",other,,,,,1,0,0.102339245,4920
"#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>

__global__ void depthwise_conv2d_kernel(
    const float* input,
    const float* weight,
    const float* bias,
    float* output,
    int batch_size,
    int in_channels,
    int input_h,
    int input_w,
    int out_channels,
    int output_h,
    int output_w,
    int kernel_size,
    int stride,
    int padding,
    int channels_per_group
) {
    // Use warp-level parallelism (32 threads per warp) and shared memory for input caching
    const unsigned int warp_size = 32;
    const unsigned int lane_id = threadIdx.x % warp_size;
    const unsigned int warp_id = threadIdx.x / warp_size;
    
    // Shared memory for caching input tile
    extern __shared__ float shared_input[];
    
    int total_elements = batch_size * out_channels * output_h * output_w;
    int warp_idx = (blockIdx.x * blockDim.x + threadIdx.x) / warp_size;
    
    // Each warp processes multiple output elements
    for (int idx = warp_idx; idx < total_elements; idx += gridDim.x * blockDim.x / warp_size) {
        int w_out = idx % output_w;
        int h_out = (idx / output_w) % output_h;
        int oc = (idx / (output_w * output_h)) % out_channels;
        int b = idx / (out_channels * output_h * output_w);
        
        int in_ch = oc / channels_per_group;
        int weight_ch = oc % channels_per_group;
        
        float partial_sum = 0.0f;
        
        // Distribute kernel elements across warp threads
        for (int k = lane_id; k < kernel_size * kernel_size; k += warp_size) {
            int kh = k / kernel_size;
            int kw = k % kernel_size;
            
            int h_in = h_out * stride + kh - padding;
            int w_in = w_out * stride + kw - padding;
            
            if (h_in >= 0 && h_in < input_h && w_in >= 0 && w_in < input_w) {
                int input_idx = b * (in_channels * input_h * input_w)
                              + in_ch * (input_h * input_w)
                              + h_in * input_w
                              + w_in;
                              
                int weight_idx = in_ch * (channels_per_group * kernel_size * kernel_size)
                               + weight_ch * (kernel_size * kernel_size)
                               + kh * kernel_size
                               + kw;
                               
                partial_sum += input[input_idx] * weight[weight_idx];
            }
        }
        
        // Warp-level reduction using shuffle operations
        #pragma unroll
        for (int offset = warp_size/2; offset > 0; offset /= 2) {
            partial_sum += __shfl_down_sync(0xffffffff, partial_sum, offset);
        }
        
        // First thread in warp writes the result
        if (lane_id == 0) {
            float final_sum = partial_sum;
            if (bias != nullptr) {
                final_sum += bias[oc];
            }
            
            output[b * out_channels * output_h * output_w +
                   oc * output_h * output_w +
                   h_out * output_w +
                   w_out] = final_sum;
        }
    }
}

torch::Tensor forward(
    torch::Tensor input,
    torch::Tensor weight,
    torch::optional<torch::Tensor> bias,
    int stride,
    int padding
) {
    TORCH_CHECK(input.is_cuda() && weight.is_cuda(), ""Inputs must be CUDA tensors"");
    if (bias.has_value()) {
        TORCH_CHECK(bias->is_cuda(), ""Bias must be a CUDA tensor"");
    }
    TORCH_CHECK(input.is_contiguous() && weight.is_contiguous(), ""Input and weight must be contiguous"");
    if (bias.has_value()) {
        TORCH_CHECK(bias->is_contiguous(), ""Bias must be contiguous"");
    }
    
    int batch_size = input.size(0);
    int in_channels = input.size(1);
    int input_h = input.size(2);
    int input_w = input.size(3);
    int kernel_size = weight.size(2);
    int channels_per_group = weight.size(1);
    int out_channels = in_channels * channels_per_group;
    
    int output_h = (input_h + 2 * padding - kernel_size) / stride + 1;
    int output_w = (input_w + 2 * padding - kernel_size) / stride + 1;
    
    auto output = torch::empty({batch_size, out_channels, output_h, output_w}, input.options());
    
    // Adjust block size to be multiple of warp size
    int threads = 256;
    int blocks = (batch_size * out_channels * output_h * output_w + threads - 1) / threads;
    
    const float* bias_ptr = bias.has_value() ? bias->data_ptr<float>() : nullptr;
    
    depthwise_conv2d_kernel<<<blocks, threads>>>(
        input.data_ptr<float>(),
        weight.data_ptr<float>(),
        bias_ptr,
        output.data_ptr<float>(),
        batch_size,
        in_channels,
        input_h,
        input_w,
        out_channels,
        output_h,
        output_w,
        kernel_size,
        stride,
        padding,
        channels_per_group
    );
    
    return output;
}

PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
    m.def(""forward"", &forward, ""Depthwise 2D Convolution (CUDA)"",
          py::arg(""input""), py::arg(""weight""), py::arg(""bias"") = py::none(), py::arg(""stride""), py::arg(""padding""));
}",conv2d,,,,,1,0,0.11364906,5144
"#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>
#include <cstdio>
#include <pybind11/pybind11.h>

#define MAX_KERNEL_SIZE 16
#define MAX_CONSTANT_WEIGHTS 16384  // 64KB / sizeof(float)

// Constant memory for weights - 64KB on H100
__constant__ float const_weight[MAX_CONSTANT_WEIGHTS];

__global__ void conv_transpose2d_forward_kernel_constant(
    const float* __restrict__ input,
    const float* __restrict__ weight,  // Fallback for large weights
    const float* __restrict__ bias,
    float* __restrict__ output,
    int batch_size,
    int in_channels,
    int out_channels,
    int in_height,
    int in_width,
    int kernel_size,
    int out_height,
    int out_width,
    int stride,
    int padding,
    int dilation,
    bool use_const_mem) {

  int index = blockIdx.x * blockDim.x + threadIdx.x;
  int total = batch_size * out_channels * out_height * out_width;
  if (index >= total)
    return;

  // Decode index
  int w_out = index % out_width;
  int temp = index / out_width;
  int h_out = temp % out_height;
  temp /= out_height;
  int o = temp % out_channels;
  int b = temp / out_channels;

  // Precompute base indices
  int base_h = h_out + padding;
  int base_w = w_out + padding;

  // Precompute valid kernel indices for h dimension
  int valid_p_count = 0;
  int valid_p[MAX_KERNEL_SIZE];
  int h_in_list[MAX_KERNEL_SIZE];
  
  #pragma unroll
  for (int p = 0; p < kernel_size; p++) {
    int p_dilated = p * dilation;
    if (base_h >= p_dilated && ((base_h - p_dilated) % stride) == 0) {
      int h_in = (base_h - p_dilated) / stride;
      if (h_in < in_height) {
        valid_p[valid_p_count] = p;
        h_in_list[valid_p_count] = h_in;
        valid_p_count++;
      }
    }
  }

  // Precompute valid kernel indices for w dimension
  int valid_q_count = 0;
  int valid_q[MAX_KERNEL_SIZE];
  int w_in_list[MAX_KERNEL_SIZE];
  
  #pragma unroll
  for (int q = 0; q < kernel_size; q++) {
    int q_dilated = q * dilation;
    if (base_w >= q_dilated && ((base_w - q_dilated) % stride) == 0) {
      int w_in = (base_w - q_dilated) / stride;
      if (w_in < in_width) {
        valid_q[valid_q_count] = q;
        w_in_list[valid_q_count] = w_in;
        valid_q_count++;
      }
    }
  }

  float out_val = __ldg(&bias[o]);

  for (int c = 0; c < in_channels; ++c) {
    #pragma unroll
    for (int i = 0; i < valid_p_count; i++) {
      int p = valid_p[i];
      int h_in = h_in_list[i];
      
      #pragma unroll
      for (int j = 0; j < valid_q_count; j++) {
        int q = valid_q[j];
        int w_in = w_in_list[j];
        
        int input_idx = ((b * in_channels + c) * in_height + h_in) * in_width + w_in;
        int weight_idx = ((c * out_channels + o) * kernel_size + p) * kernel_size + q;
        
        float weight_val;
        if (use_const_mem) {
          weight_val = const_weight[weight_idx];
        } else {
          weight_val = __ldg(&weight[weight_idx]);
        }
        
        out_val += __ldg(&input[input_idx]) * weight_val;
      }
    }
  }

  output[((b * out_channels + o) * out_height + h_out) * out_width + w_out] = out_val;
}

torch::Tensor conv_transpose2d_forward_cuda_constant(
    torch::Tensor input,
    torch::Tensor weight,
    torch::Tensor bias,
    int stride,
    int padding,
    int dilation) {
    
  int batch_size = input.size(0);
  int in_channels = input.size(1);
  int in_height = input.size(2);
  int in_width = input.size(3);
  
  int out_channels = weight.size(1);
  int kernel_size = weight.size(2);
  
  int out_height = (in_height - 1) * stride - 2 * padding + dilation * (kernel_size - 1) + 1;
  int out_width  = (in_width - 1) * stride - 2 * padding + dilation * (kernel_size - 1) + 1;
  
  auto output = torch::zeros({batch_size, out_channels, out_height, out_width}, input.options());

  // Determine if weight tensor fits in constant memory
  int weight_size = weight.numel() * sizeof(float);
  bool use_const_mem = weight_size <= (MAX_CONSTANT_WEIGHTS * sizeof(float));
  
  if (use_const_mem) {
    // Copy weights to constant memory
    cudaMemcpyToSymbol(const_weight, weight.data_ptr<float>(), weight_size);
  }

  int total_threads = batch_size * out_channels * out_height * out_width;
  int threads = 128;
  int blocks = (total_threads + threads - 1) / threads;

  conv_transpose2d_forward_kernel_constant<<<blocks, threads>>>(
      input.data_ptr<float>(),
      weight.data_ptr<float>(),
      bias.data_ptr<float>(),
      output.data_ptr<float>(),
      batch_size,
      in_channels,
      out_channels,
      in_height,
      in_width,
      kernel_size,
      out_height,
      out_width,
      stride,
      padding,
      dilation,
      use_const_mem);

  return output;
}

torch::Tensor conv_transpose2d_forward_wrapper_constant(
    torch::Tensor input,
    torch::Tensor weight,
    pybind11::object bias_obj,
    int stride,
    int padding,
    int dilation) {
    
  int out_channels = weight.size(1);
  torch::Tensor bias;
  if (bias_obj.is(pybind11::none())) {
    bias = torch::zeros({out_channels}, weight.options());
  } else {
    bias = bias_obj.cast<torch::Tensor>();
  }
  
  return conv_transpose2d_forward_cuda_constant(input, weight, bias, stride, padding, dilation);
}

PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
  m.def(""forward"", &conv_transpose2d_forward_wrapper_constant,
        ""ConvTranspose2d forward (CUDA) with constant memory optimization"",
        pybind11::arg(""input""),
        pybind11::arg(""weight""),
        pybind11::arg(""bias""),
        pybind11::arg(""stride""),
        pybind11::arg(""padding""),
        pybind11::arg(""dilation""));
}",conv2d,,,,,1,0,0.11547159,5630
"#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>
#include <vector>

#define CHECK_CUDA(x) TORCH_CHECK(x.is_cuda(), #x "" must be a CUDA tensor"")
#define CHECK_CONTIGUOUS(x) TORCH_CHECK(x.is_contiguous(), #x "" must be contiguous"")
#define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)

// Unified CUDA kernel for adaptive convolution
__global__ void conv2d_adaptive_kernel(
    const float* __restrict__ input,
    const float* __restrict__ weight,
    const float* __restrict__ bias,
    float* __restrict__ output,
    const int batch,
    const int in_channels,
    const int out_channels,
    const int in_height,
    const int in_width,
    const int out_height,
    const int out_width,
    const int kernel_size,
    const int stride,
    const int padding,
    const int dilation,
    const bool use_shared_memory) {
    
    extern __shared__ float shared_input[];
    
    int n = blockIdx.x;
    int oc = blockIdx.y;
    int out_y = blockIdx.z * blockDim.y + threadIdx.y;
    int out_x = threadIdx.x;

    if (out_y >= out_height || out_x >= out_width) return;

    float sum = 0.0f;
    
    const int in_y_start = out_y * stride - padding;
    const int in_x_start = out_x * stride - padding;
    
    for (int ic = 0; ic < in_channels; ++ic) {
        for (int ky = 0; ky < kernel_size; ++ky) {
            for (int kx = 0; kx < kernel_size; ++kx) {
                int in_y = in_y_start + ky * dilation;
                int in_x = in_x_start + kx * dilation;
                
                if (use_shared_memory) {
                    if (in_y >= 0 && in_y < in_height && in_x >= 0 && in_x < in_width) {
                        shared_input[threadIdx.y * blockDim.x + threadIdx.x] = 
                            input[n * in_channels * in_height * in_width +
                                  ic * in_height * in_width +
                                  in_y * in_width + in_x];
                    } else {
                        shared_input[threadIdx.y * blockDim.x + threadIdx.x] = 0.0f;
                    }
                    __syncthreads();
                }
                
                float input_value = use_shared_memory ? shared_input[threadIdx.y * blockDim.x + threadIdx.x] :
                    (in_y >= 0 && in_y < in_height && in_x >= 0 && in_x < in_width ?
                     input[n * in_channels * in_height * in_width +
                           ic * in_height * in_width +
                           in_y * in_width + in_x] : 0.0f);

                sum += input_value *
                       weight[oc * in_channels * kernel_size * kernel_size +
                             ic * kernel_size * kernel_size +
                             ky * kernel_size + kx];

                if (use_shared_memory) __syncthreads();
            }
        }
    }

    if (bias) {
        sum += bias[oc];
    }
    
    output[n * out_channels * out_height * out_width +
           oc * out_height * out_width +
           out_y * out_width + out_x] = sum;
}

torch::Tensor forward(
    torch::Tensor x,
    torch::Tensor weight,
    torch::optional<torch::Tensor> bias,
    int stride,
    int padding,
    int dilation,
    int groups) {
    
    CHECK_INPUT(x);
    CHECK_INPUT(weight);
    if (bias.has_value()) {
        CHECK_INPUT(bias.value());
    }

    const int batch = x.size(0);
    const int in_channels = x.size(1);
    const int in_height = x.size(2);
    const int in_width = x.size(3);
    const int out_channels = weight.size(0);
    const int kernel_size = weight.size(2);
    
    const int out_height = (in_height + 2 * padding - dilation * (kernel_size - 1) - 1) / stride + 1;
    const int out_width = (in_width + 2 * padding - dilation * (kernel_size - 1) - 1) / stride + 1;
    
    auto output = torch::zeros({batch, out_channels, out_height, out_width}, x.options());
    
    bool use_shared_memory = (in_height <= 32 && in_width <= 32 && groups == 1);

    if (use_shared_memory) {
        dim3 threads(out_width, 1);
        dim3 blocks(batch, out_channels, (out_height + threads.y - 1) / threads.y);
        const size_t shared_memory_size = threads.x * threads.y * sizeof(float);
        conv2d_adaptive_kernel<<<blocks, threads, shared_memory_size>>>(
            x.data_ptr<float>(),
            weight.data_ptr<float>(),
            bias.has_value() ? bias.value().data_ptr<float>() : nullptr,
            output.data_ptr<float>(),
            batch, in_channels, out_channels,
            in_height, in_width, out_height, out_width,
            kernel_size, stride, padding, dilation,
            use_shared_memory);
    } else {
        output = torch::conv2d(x, weight, bias.has_value() ? bias.value() : torch::Tensor(),
                               {stride, stride}, {padding, padding}, {dilation, dilation}, groups);
    }
    return output;
}

PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
    m.def(""forward"", &forward, ""Adaptive CUDA convolution implementation"");
}",conv2d,,,,,1,0,0.11821934,4973
"#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>
#include <c10/cuda/CUDAStream.h>
#include <cmath>

#define TILE_SIZE 16
#define KERNEL_SIZE 3

__global__ void conv_mish_shared_kernel(
    const float* __restrict__ input,
    const float* __restrict__ weight,
    const float* __restrict__ bias,
    float subtract1,
    float subtract2,
    float* __restrict__ output,
    int batch_size,
    int in_channels,
    int in_h,
    int in_w,
    int out_channels,
    int out_h,
    int out_w) {

    __shared__ float sh_input[TILE_SIZE + KERNEL_SIZE-1][TILE_SIZE + KERNEL_SIZE-1];
    __shared__ float sh_weights[KERNEL_SIZE][KERNEL_SIZE];

    const int tx = threadIdx.x;
    const int ty = threadIdx.y;
    const int bx = blockIdx.x * TILE_SIZE;
    const int by = blockIdx.y * TILE_SIZE;
    const int oc = blockIdx.z % out_channels;
    const int n = blockIdx.z / out_channels;

    float sum = bias[oc];

    for (int ic = 0; ic < in_channels; ic++) {
        // Load input tile into shared memory
        for (int i = ty; i < TILE_SIZE + KERNEL_SIZE-1; i += blockDim.y) {
            for (int j = tx; j < TILE_SIZE + KERNEL_SIZE-1; j += blockDim.x) {
                int y = by + i;
                int x = bx + j;
                if (y < in_h && x < in_w) {
                    sh_input[i][j] = input[n * (in_channels * in_h * in_w) +
                                          ic * (in_h * in_w) +
                                          y * in_w + x];
                } else {
                    sh_input[i][j] = 0.0f;
                }
            }
        }

        // Load weights into shared memory
        if (tx < KERNEL_SIZE && ty < KERNEL_SIZE) {
            sh_weights[ty][tx] = weight[oc * (in_channels * KERNEL_SIZE * KERNEL_SIZE) +
                                      ic * (KERNEL_SIZE * KERNEL_SIZE) +
                                      ty * KERNEL_SIZE + tx];
        }

        __syncthreads();

        // Compute convolution
        if (bx + tx < out_w && by + ty < out_h) {
            #pragma unroll
            for (int kh = 0; kh < KERNEL_SIZE; kh++) {
                #pragma unroll
                for (int kw = 0; kw < KERNEL_SIZE; kw++) {
                    sum += sh_input[ty + kh][tx + kw] * sh_weights[kh][kw];
                }
            }
        }
        __syncthreads();
    }

    // Store result
    if (bx + tx < out_w && by + ty < out_h) {
        int out_idx = n * (out_channels * out_h * out_w) +
                     oc * (out_h * out_w) +
                     (by + ty) * out_w + (bx + tx);
        sum = sum - subtract1 - subtract2;
        float softplus = logf(1.0f + expf(sum));
        output[out_idx] = sum * tanhf(softplus);
    }
}

torch::Tensor forward(
    torch::Tensor x,
    torch::Tensor conv_weight,
    torch::Tensor conv_bias,
    float subtract_value_1,
    float subtract_value_2) {

    TORCH_CHECK(x.is_cuda(), ""x must be a CUDA tensor"");
    TORCH_CHECK(conv_weight.size(2) == KERNEL_SIZE, ""Kernel size must be 3"");

    x = x.contiguous();
    conv_weight = conv_weight.contiguous();
    conv_bias = conv_bias.contiguous();

    int batch_size = x.size(0);
    int in_channels = x.size(1);
    int in_h = x.size(2);
    int in_w = x.size(3);
    int out_channels = conv_weight.size(0);
    int out_h = in_h - KERNEL_SIZE + 1;
    int out_w = in_w - KERNEL_SIZE + 1;

    auto output = torch::empty({batch_size, out_channels, out_h, out_w}, x.options());

    dim3 threads(TILE_SIZE, TILE_SIZE);
    dim3 blocks(
        (out_w + TILE_SIZE - 1) / TILE_SIZE,
        (out_h + TILE_SIZE - 1) / TILE_SIZE,
        batch_size * out_channels
    );

    cudaStream_t stream = c10::cuda::getCurrentCUDAStream().stream();
    conv_mish_shared_kernel<<<blocks, threads, 0, stream>>>(
        x.data_ptr<float>(),
        conv_weight.data_ptr<float>(),
        conv_bias.data_ptr<float>(),
        subtract_value_1,
        subtract_value_2,
        output.data_ptr<float>(),
        batch_size,
        in_channels,
        in_h,
        in_w,
        out_channels,
        out_h,
        out_w
    );

    return output;
}

PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
    m.def(""forward"", &forward, ""Conv2D+Subtract+Mish with shared memory optimization"");
}",conv2d,,,,,1,0,0.120435804,4256
"#include <torch/extension.h>
#include <vector>
#include <cuda.h>
#include <cuda_runtime.h>

__global__ void conv_shared_memory_kernel(
    const float* __restrict__ input,
    const float* __restrict__ weight,
    const float* __restrict__ bias,
    float* __restrict__ output,
    int batch,
    int in_channels,
    int in_height,
    int in_width,
    int out_channels,
    int kernel_size,
    int pad
) {
    extern __shared__ float shared_input[];

    int out_height = in_height;
    int out_width = in_width;
    int index = blockIdx.x;
    int total_output = batch * out_channels * out_height * out_width;
    if (index >= total_output) return;

    int w = index % out_width;
    int tmp = index / out_width;
    int h = tmp % out_height;
    tmp /= out_height;
    int f = tmp % out_channels;
    int n = tmp / out_channels;

    int T = in_channels * kernel_size * kernel_size;
    float local_sum = 0.0f;

    // Preload part of the input into shared memory
    for (int t = threadIdx.x; t < in_channels * in_height * in_width; t += blockDim.x) {
        shared_input[t] = input[t + n * in_channels * in_height * in_width];
    }
    __syncthreads();

    for (int t = threadIdx.x; t < T; t += blockDim.x) {
        int k_index = t / in_channels;
        int c = t % in_channels;
        int r = k_index / kernel_size;
        int s = k_index % kernel_size;
        int in_r = h + r - pad;
        int in_c = w + s - pad;

        float in_val = 0.0f;
        if (in_r >= 0 && in_r < in_height && in_c >= 0 && in_c < in_width) {
            int shared_index = ((c * in_height + in_r) * in_width + in_c);
            in_val = shared_input[shared_index];
        }

        int weight_idx = ((f * in_channels + c) * kernel_size + r) * kernel_size + s;
        float w_val = __ldg(&weight[weight_idx]);
        local_sum += in_val * w_val;
    }

    // Warp reduction
    unsigned mask = 0xffffffff;
    for (int offset = warpSize/2; offset > 0; offset /= 2) {
        local_sum += __shfl_down_sync(mask, local_sum, offset);
    }

    __shared__ float shared_data[64];
    int lane = threadIdx.x & (warpSize - 1);
    int warp_id = threadIdx.x / warpSize;
    if (lane == 0) {
        shared_data[warp_id] = local_sum;
    }
    __syncthreads();

    if (threadIdx.x == 0) {
        float block_sum = 0.0f;
        int num_warps = (blockDim.x + warpSize - 1) / warpSize;
        for (int i = 0; i < num_warps; ++i) {
            block_sum += shared_data[i];
        }
        output[index] = block_sum + __ldg(&bias[f]);
    }
}

torch::Tensor forward(
    torch::Tensor x,
    torch::Tensor branch1x1_weight,
    torch::Tensor branch1x1_bias,
    torch::Tensor branch3x3_reduce_weight,
    torch::Tensor branch3x3_reduce_bias,
    torch::Tensor branch3x3_weight,
    torch::Tensor branch3x3_bias,
    torch::Tensor branch5x5_reduce_weight,
    torch::Tensor branch5x5_reduce_bias,
    torch::Tensor branch5x5_weight,
    torch::Tensor branch5x5_bias,
    torch::Tensor branch_pool_conv_weight,
    torch::Tensor branch_pool_conv_bias
) {
    auto device = x.device();
    branch1x1_weight = branch1x1_weight.to(device);
    branch1x1_bias = branch1x1_bias.to(device);
    branch3x3_reduce_weight = branch3x3_reduce_weight.to(device);
    branch3x3_reduce_bias = branch3x3_reduce_bias.to(device);
    branch3x3_weight = branch3x3_weight.to(device);
    branch3x3_bias = branch3x3_bias.to(device);
    branch5x5_reduce_weight = branch5x5_reduce_weight.to(device);
    branch5x5_reduce_bias = branch5x5_reduce_bias.to(device);
    branch5x5_weight = branch5x5_weight.to(device);
    branch5x5_bias = branch5x5_bias.to(device);
    branch_pool_conv_weight = branch_pool_conv_weight.to(device);
    branch_pool_conv_bias = branch_pool_conv_bias.to(device);

    auto branch1x1 = at::conv2d(x, branch1x1_weight, branch1x1_bias);

    auto branch3x3_reduce = at::conv2d(x, branch3x3_reduce_weight, branch3x3_reduce_bias);
    int B = branch3x3_reduce.size(0);
    int inC = branch3x3_reduce.size(1);
    int H = branch3x3_reduce.size(2);
    int W = branch3x3_reduce.size(3);
    int outC = branch3x3_weight.size(0);
    int k = branch3x3_weight.size(2);
    int pad = k / 2;

    auto branch3x3 = at::empty({B, outC, H, W}, branch3x3_reduce.options());
    const float* in_ptr = branch3x3_reduce.data_ptr<float>();
    const float* weight_ptr = branch3x3_weight.data_ptr<float>();
    const float* bias_ptr = branch3x3_bias.data_ptr<float>();
    float* out_ptr = branch3x3.data_ptr<float>();

    int total_outputs = B * outC * H * W;
    int blockSize = 256;
    int sharedMemSize = inC * H * W * sizeof(float);
    int gridSize = (total_outputs + blockSize - 1) / blockSize;
    conv_shared_memory_kernel<<<gridSize, blockSize, sharedMemSize>>>(in_ptr, weight_ptr, bias_ptr, out_ptr, B, inC, H, W, outC, k, pad);

    auto branch5x5 = at::conv2d(x, branch5x5_reduce_weight, branch5x5_reduce_bias);
    branch5x5 = at::conv2d(branch5x5, branch5x5_weight, branch5x5_bias, 1, 2);

    auto branch_pool = at::max_pool2d(x, {3, 3}, {1, 1}, {1, 1});
    branch_pool = at::conv2d(branch_pool, branch_pool_conv_weight, branch_pool_conv_bias);

    std::vector<torch::Tensor> outputs = {branch1x1, branch3x3, branch5x5, branch_pool};
    return at::cat(outputs, 1);
}

PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
    m.def(""forward"", &forward, ""Inception module forward with shared memory optimization (CUDA)"");
}
",other,,,,,1,0,0.12443472,5414
"#include <torch/extension.h>
#include <ATen/cuda/CUDAContext.h>
#include <cuda.h>
#include <cuda_runtime.h>
#include <vector>

// Fused kernel with shared memory optimization
__global__ void fused_act_residual_kernel(const float* __restrict__ x_conv,
                                         const float* __restrict__ x_norm,
                                         float* __restrict__ res,
                                         const int total) {
    extern __shared__ float shared_data[];
    float* shared_conv = shared_data;
    float* shared_norm = shared_data + blockDim.x;
    
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    int stride = blockDim.x * gridDim.x;
    
    for (int i = idx; i < total; i += stride) {
        // Load data into shared memory
        if (i < total) {
            shared_conv[threadIdx.x] = x_conv[i];
            shared_norm[threadIdx.x] = x_norm[i];
        }
        __syncthreads();
        
        if (i < total) {
            float t = tanhf(shared_norm[threadIdx.x]);
            float relu6 = fminf(fmaxf(t + 3.0f, 0.0f), 6.0f);
            float hs = t * relu6 / 6.0f;
            res[i] = shared_conv[threadIdx.x] + hs;
        }
        __syncthreads();
    }
}

// LogSumExp kernel with shared memory for channel reduction
__global__ void logsumexp_kernel(const float* __restrict__ res,
                                float* __restrict__ out,
                                const int N, const int C,
                                const int H, const int W) {
    extern __shared__ float shared_data[];
    float* shared_max = shared_data;
    float* shared_sum = &shared_data[blockDim.x];
    
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    int total = N * H * W;
    
    if (idx < total) {
        int w_idx = idx % W;
        int h_idx = (idx / W) % H;
        int n_idx = idx / (H * W);
        
        // First pass: find max value
        float max_val = -FLT_MAX;
        #pragma unroll 4
        for (int c = 0; c < C; c++) {
            int index = ((n_idx * C + c) * H + h_idx) * W + w_idx;
            max_val = fmaxf(max_val, res[index]);
        }
        shared_max[threadIdx.x] = max_val;
        __syncthreads();
        
        // Compute sum of exponentials
        float sum_exp = 0.0f;
        #pragma unroll 4
        for (int c = 0; c < C; c++) {
            int index = ((n_idx * C + c) * H + h_idx) * W + w_idx;
            sum_exp += expf(res[index] - shared_max[threadIdx.x]);
        }
        shared_sum[threadIdx.x] = sum_exp;
        __syncthreads();
        
        // Write final result
        if (idx < total) {
            out[idx] = shared_max[threadIdx.x] + logf(shared_sum[threadIdx.x]);
        }
    }
}

torch::Tensor module_fn_forward(
    torch::Tensor x,
    double eps,
    torch::Tensor conv_weight,
    torch::Tensor conv_bias,
    torch::Tensor group_norm_weight,
    torch::Tensor group_norm_bias,
    int64_t groups) {

    // Ensure input tensors are contiguous
    x = x.contiguous();
    conv_weight = conv_weight.contiguous();
    conv_bias = conv_bias.contiguous();
    group_norm_weight = group_norm_weight.contiguous();
    group_norm_bias = group_norm_bias.contiguous();

    // Convolution
    torch::Tensor x_conv = torch::conv2d(x, conv_weight, conv_bias);

    // Group Normalization
    torch::Tensor x_norm = torch::group_norm(x_conv, groups, group_norm_weight, group_norm_bias, eps);

    // Allocate output tensors
    torch::Tensor res = torch::empty_like(x_conv);
    
    const int threads = 256;
    int64_t total_elements = x_conv.numel();
    const int blocks = (total_elements + threads - 1) / threads;
    
    // Shared memory size for fused kernel
    size_t shared_mem_size_fused = 2 * threads * sizeof(float);
    
    // Launch fused kernel
    fused_act_residual_kernel<<<blocks, threads, shared_mem_size_fused, at::cuda::getCurrentCUDAStream()>>>(
        x_conv.data_ptr<float>(),
        x_norm.data_ptr<float>(),
        res.data_ptr<float>(),
        total_elements);

    // Get dimensions
    int N = x_conv.size(0);
    int C = x_conv.size(1);
    int H = x_conv.size(2);
    int W = x_conv.size(3);

    // Allocate output tensor
    torch::Tensor out = torch::empty({N, 1, H, W}, x_conv.options());
    
    int logsumexp_total = N * H * W;
    int logsumexp_blocks = (logsumexp_total + threads - 1) / threads;
    
    // Shared memory size for logsumexp kernel
    size_t shared_mem_size_logsumexp = 2 * threads * sizeof(float);
    
    // Launch logsumexp kernel
    logsumexp_kernel<<<logsumexp_blocks, threads, shared_mem_size_logsumexp, at::cuda::getCurrentCUDAStream()>>>(
        res.data_ptr<float>(),
        out.data_ptr<float>(),
        N, C, H, W);

    return out;
}

PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
    m.def(""forward"", &module_fn_forward, ""Fused Conv2d, GroupNorm, Tanh, HardSwish, ResidualAdd, LogSumExp forward function"");
}",conv2d,,,,,1,0,0.12834339,4923
"#include <torch/extension.h>
#include <pybind11/pybind11.h>
#include <c10/cuda/CUDAGuard.h>

namespace py = pybind11;

torch::Tensor basic_block_fn(
    torch::Tensor x,
    torch::Tensor conv1_w,
    torch::Tensor bn1_w,
    torch::Tensor bn1_b,
    torch::Tensor bn1_rm,
    torch::Tensor bn1_rv,
    torch::Tensor conv2_w,
    torch::Tensor bn2_w,
    torch::Tensor bn2_b,
    torch::Tensor bn2_rm,
    torch::Tensor bn2_rv,
    torch::Tensor downsample_conv_w,
    torch::Tensor downsample_bn_w,
    torch::Tensor downsample_bn_b,
    torch::Tensor downsample_bn_rm,
    torch::Tensor downsample_bn_rv,
    int64_t stride,
    bool is_training
) {
    at::cuda::CUDAStreamGuard guard(at::cuda::getCurrentCUDAStream());
    
    torch::Tensor identity = x;
    
    auto main_path = [&]() {
        auto out = torch::conv2d(x, conv1_w, /*bias=*/{}, /*stride=*/{stride, stride}, /*padding=*/{1, 1});
        out = torch::batch_norm(out, bn1_w, bn1_b, bn1_rm, bn1_rv, is_training, 0.0, 1e-5, true);
        out = torch::relu(out);
        out = torch::conv2d(out, conv2_w, /*bias=*/{}, /*stride=*/{1, 1}, /*padding=*/{1, 1});
        return torch::batch_norm(out, bn2_w, bn2_b, bn2_rm, bn2_rv, is_training, 0.0, 1e-5, true);
    }();

    if (downsample_conv_w.defined()) {
        identity = torch::conv2d(identity, downsample_conv_w, /*bias=*/{}, /*stride=*/{stride, stride});
        identity = torch::batch_norm(
            identity,
            downsample_bn_w,
            downsample_bn_b,
            downsample_bn_rm,
            downsample_bn_rv,
            is_training,
            0.0,
            1e-5,
            true
        );
    }

    return torch::relu(main_path + identity);
}

torch::Tensor module_fn(torch::Tensor x, py::object params_py, bool is_training) {
    at::cuda::CUDAStreamGuard guard(at::cuda::getCurrentCUDAStream());
    
    auto get_param = [&](const std::string& key) -> torch::Tensor {
        return params_py.attr(""__getitem__"")(key.c_str()).cast<torch::Tensor>();
    };

    auto conv1_weight = get_param(""conv1_weight"");
    auto bn1_weight = get_param(""bn1_weight"");
    auto bn1_bias = get_param(""bn1_bias"");
    auto bn1_running_mean = get_param(""bn1_running_mean"");
    auto bn1_running_var = get_param(""bn1_running_var"");

    x = torch::conv2d(x, conv1_weight, /*bias=*/{}, /*stride=*/{2, 2}, /*padding=*/{3, 3});
    x = torch::batch_norm(x, bn1_weight, bn1_bias, bn1_running_mean, bn1_running_var, 
                         is_training, 0.0, 1e-5, true);
    x = torch::relu(x);
    x = torch::max_pool2d(x, /*kernel_size=*/{3, 3}, /*stride=*/{2, 2}, /*padding=*/{1, 1});

    for (int i = 1; i <= 4; ++i) {
        std::string layer_name = ""layer"" + std::to_string(i);
        for (int j = 0; j < 2; ++j) {
            std::string block_name = layer_name + ""_"" + std::to_string(j);
            int64_t stride = (i > 1 && j == 0) ? 2 : 1;

            auto conv1_w = get_param(block_name + ""_conv1_weight"");
            auto bn1_w = get_param(block_name + ""_bn1_weight"");
            auto bn1_b = get_param(block_name + ""_bn1_bias"");
            auto bn1_rm = get_param(block_name + ""_bn1_running_mean"");
            auto bn1_rv = get_param(block_name + ""_bn1_running_var"");
            auto conv2_w = get_param(block_name + ""_conv2_weight"");
            auto bn2_w = get_param(block_name + ""_bn2_weight"");
            auto bn2_b = get_param(block_name + ""_bn2_bias"");
            auto bn2_rm = get_param(block_name + ""_bn2_running_mean"");
            auto bn2_rv = get_param(block_name + ""_bn2_running_var"");

            std::string downsample_conv_key = block_name + ""_downsample_0_weight"";
            bool has_downsample = PyMapping_HasKeyString(params_py.ptr(), downsample_conv_key.c_str()) == 1;

            torch::Tensor downsample_conv_w, downsample_bn_w, downsample_bn_b, downsample_bn_rm, downsample_bn_rv;

            if (has_downsample) {
                downsample_conv_w = get_param(block_name + ""_downsample_0_weight"");
                downsample_bn_w = get_param(block_name + ""_downsample_1_weight"");
                downsample_bn_b = get_param(block_name + ""_downsample_1_bias"");
                downsample_bn_rm = get_param(block_name + ""_downsample_1_running_mean"");
                downsample_bn_rv = get_param(block_name + ""_downsample_1_running_var"");
            }

            x = basic_block_fn(
                x, conv1_w, bn1_w, bn1_b, bn1_rm, bn1_rv,
                conv2_w, bn2_w, bn2_b, bn2_rm, bn2_rv,
                downsample_conv_w, downsample_bn_w, downsample_bn_b,
                downsample_bn_rm, downsample_bn_rv,
                stride, is_training
            );
        }
    }

    x = torch::adaptive_avg_pool2d(x, {1, 1});
    x = x.view({x.size(0), -1});
    return torch::linear(x, get_param(""fc_weight""), get_param(""fc_bias""));
}

PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
    m.def(""forward"", &module_fn, ""ResNet18 forward function (CUDA)"");
}",other,,,,,1,0,0.12928604,4954
"#include <torch/extension.h>
#include <pybind11/pybind11.h>
#include <cuda_runtime.h>
#include <cooperative_groups.h>

namespace py = pybind11;
namespace cg = cooperative_groups;

// Optimized batch norm using warp-level primitives
__global__ void warp_batch_norm_stats_kernel(
    const float* __restrict__ input,
    float* __restrict__ mean,
    float* __restrict__ var,
    const int N, const int C, const int H, const int W
) {
    const int c = blockIdx.x;
    const int tid = threadIdx.x;
    const int warp_id = tid / 32;
    const int lane_id = tid % 32;
    const int warps_per_block = blockDim.x / 32;
    const int HW = H * W;
    const int CHW = C * H * W;
    
    float sum = 0.0f;
    float sq_sum = 0.0f;
    
    // Each thread processes multiple elements
    for (int n = 0; n < N; n++) {
        for (int hw = tid; hw < HW; hw += blockDim.x) {
            const float val = input[n * CHW + c * HW + hw];
            sum += val;
            sq_sum += val * val;
        }
    }
    
    // Warp-level reduction using shuffle
    #pragma unroll
    for (int offset = 16; offset > 0; offset /= 2) {
        sum += __shfl_down_sync(0xffffffff, sum, offset);
        sq_sum += __shfl_down_sync(0xffffffff, sq_sum, offset);
    }
    
    // First thread in each warp writes results
    if (lane_id == 0) {
        atomicAdd(&mean[c], sum);
        atomicAdd(&var[c], sq_sum);
    }
}

torch::Tensor optimized_batch_norm(
    const torch::Tensor& input,
    const torch::Tensor& weight,
    const torch::Tensor& bias,
    const torch::Tensor& running_mean,
    const torch::Tensor& running_var,
    bool training
) {
    const int N = input.size(0);
    const int C = input.size(1);
    const int H = input.size(2);
    const int W = input.size(3);
    
    if (training) {
        auto options = torch::TensorOptions()
            .dtype(input.dtype())
            .device(input.device());
        
        auto mean = torch::zeros({C}, options);
        auto var = torch::zeros({C}, options);
        
        const int block_size = 256;
        const dim3 blocks(C);
        const dim3 threads(block_size);
        
        warp_batch_norm_stats_kernel<<<blocks, threads>>>(
            input.data_ptr<float>(),
            mean.data_ptr<float>(),
            var.data_ptr<float>(),
            N, C, H, W
        );
        
        // Finalize statistics
        mean.div_(N * H * W);
        var.div_(N * H * W).sub_(mean * mean).add_(1e-5);
        
        // Update running statistics
        running_mean.mul_(0.9).add_(mean * 0.1);
        running_var.mul_(0.9).add_(var * 0.1);
        
        // Apply batch norm with computed statistics
        return (input - mean.view({1, -1, 1, 1})) 
               * torch::rsqrt(var.view({1, -1, 1, 1})) 
               * weight.view({1, -1, 1, 1}) 
               + bias.view({1, -1, 1, 1});
    }
    
    return torch::batch_norm(
        input, weight, bias,
        running_mean, running_var,
        training, 0.1, 1e-5,
        true
    );
}

template<bool HasDownsample>
torch::Tensor basic_block_optimized(
    const torch::Tensor& x,
    const torch::Tensor& conv1_w,
    const torch::Tensor& bn1_w,
    const torch::Tensor& bn1_b,
    const torch::Tensor& bn1_rm,
    const torch::Tensor& bn1_rv,
    const torch::Tensor& conv2_w,
    const torch::Tensor& bn2_w,
    const torch::Tensor& bn2_b,
    const torch::Tensor& bn2_rm,
    const torch::Tensor& bn2_rv,
    const torch::Tensor& downsample_conv_w,
    const torch::Tensor& downsample_bn_w,
    const torch::Tensor& downsample_bn_b,
    const torch::Tensor& downsample_bn_rm,
    const torch::Tensor& downsample_bn_rv,
    int64_t stride,
    bool is_training
) {
    auto identity = x;
    auto out = torch::conv2d(x, conv1_w, {}, {stride, stride}, {1, 1});
    out = optimized_batch_norm(out, bn1_w, bn1_b, bn1_rm, bn1_rv, is_training);
    out = torch::relu(out);
    
    out = torch::conv2d(out, conv2_w, {}, {1, 1}, {1, 1});
    out = optimized_batch_norm(out, bn2_w, bn2_b, bn2_rm, bn2_rv, is_training);
    
    if constexpr (HasDownsample) {
        identity = torch::conv2d(identity, downsample_conv_w, {}, {stride, stride});
        identity = optimized_batch_norm(
            identity, downsample_bn_w, downsample_bn_b,
            downsample_bn_rm, downsample_bn_rv, is_training
        );
    }
    
    return torch::relu(out + identity);
}

torch::Tensor module_fn(torch::Tensor x, py::object params_py, bool is_training) {
    auto get_param = [&](const std::string& key) -> torch::Tensor {
        return params_py.attr(""__getitem__"")(key.c_str()).cast<torch::Tensor>();
    };
    
    // Initial convolution and batch norm
    x = torch::conv2d(x, get_param(""conv1_weight""), {}, {2, 2}, {3, 3});
    x = optimized_batch_norm(
        x,
        get_param(""bn1_weight""),
        get_param(""bn1_bias""),
        get_param(""bn1_running_mean""),
        get_param(""bn1_running_var""),
        is_training
    );
    
    x = torch::relu(x);
    x = torch::max_pool2d(x, {3, 3}, {2, 2}, {1, 1});
    
    // Process residual blocks
    for (int i = 1; i <= 4; ++i) {
        for (int j = 0; j < 2; ++j) {
            std::string block_name = ""layer"" + std::to_string(i) + ""_"" + std::to_string(j);
            int64_t stride = (i > 1 && j == 0) ? 2 : 1;
            
            bool has_downsample = PyMapping_HasKeyString(
                params_py.ptr(),
                (block_name + ""_downsample_0_weight"").c_str()
            ) == 1;
            
            if (has_downsample) {
                x = basic_block_optimized<true>(
                    x,
                    get_param(block_name + ""_conv1_weight""),
                    get_param(block_name + ""_bn1_weight""),
                    get_param(block_name + ""_bn1_bias""),
                    get_param(block_name + ""_bn1_running_mean""),
                    get_param(block_name + ""_bn1_running_var""),
                    get_param(block_name + ""_conv2_weight""),
                    get_param(block_name + ""_bn2_weight""),
                    get_param(block_name + ""_bn2_bias""),
                    get_param(block_name + ""_bn2_running_mean""),
                    get_param(block_name + ""_bn2_running_var""),
                    get_param(block_name + ""_downsample_0_weight""),
                    get_param(block_name + ""_downsample_1_weight""),
                    get_param(block_name + ""_downsample_1_bias""),
                    get_param(block_name + ""_downsample_1_running_mean""),
                    get_param(block_name + ""_downsample_1_running_var""),
                    stride,
                    is_training
                );
            } else {
                x = basic_block_optimized<false>(
                    x,
                    get_param(block_name + ""_conv1_weight""),
                    get_param(block_name + ""_bn1_weight""),
                    get_param(block_name + ""_bn1_bias""),
                    get_param(block_name + ""_bn1_running_mean""),
                    get_param(block_name + ""_bn1_running_var""),
                    get_param(block_name + ""_conv2_weight""),
                    get_param(block_name + ""_bn2_weight""),
                    get_param(block_name + ""_bn2_bias""),
                    get_param(block_name + ""_bn2_running_mean""),
                    get_param(block_name + ""_bn2_running_var""),
                    torch::Tensor(),
                    torch::Tensor(),
                    torch::Tensor(),
                    torch::Tensor(),
                    torch::Tensor(),
                    stride,
                    is_training
                );
            }
        }
    }
    
    x = torch::adaptive_avg_pool2d(x, {1, 1});
    x = x.view({x.size(0), -1});
    return torch::linear(x, get_param(""fc_weight""), get_param(""fc_bias""));
}

PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
    m.def(""forward"", &module_fn, ""ResNet18 forward function (CUDA)"");
}",other,,,,,1,0,0.13305682,7923
"#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>
#include <vector>
#include <cfloat>

// Constants in GPU constant memory
__constant__ float hswish_offset = 3.0f;
__constant__ float hswish_scale = 6.0f;
__constant__ float hswish_inv_scale = 1.0f/6.0f;

// Fused HardSwish+ReLU activation
__device__ inline float hardswish_relu(float x) {
    float activated = fminf(fmaxf(x + hswish_offset, 0.0f), hswish_scale);
    return fmaxf(x * activated * hswish_inv_scale, 0.0f);
}

__global__ void activation_kernel(float* input, float* output, int64_t size) {
    int index = blockIdx.x * blockDim.x + threadIdx.x;
    if (index < size) {
        output[index] = hardswish_relu(input[index]);
    }
}

// Parallel reduction for max value
__device__ float channel_max_reduce(float val, float* shared) {
    int tid = threadIdx.x;
    shared[tid] = val;
    __syncthreads();

    for (int stride = blockDim.x/2; stride > 0; stride >>= 1) {
        if (tid < stride && shared[tid + stride] > shared[tid])
            shared[tid] = shared[tid + stride];
        __syncthreads();
    }
    return shared[0];
}

// Parallel reduction for sum
__device__ float channel_sum_reduce(float val, float* shared) {
    int tid = threadIdx.x;
    shared[tid] = val;
    __syncthreads();

    for (int stride = blockDim.x/2; stride > 0; stride >>= 1) {
        if (tid < stride)
            shared[tid] += shared[tid + stride];
        __syncthreads();
    }
    return shared[0];
}

__global__ void optimized_softmax_kernel(float* input, float* output,
                                        int batch_size, int channels, int spatial_size) {
    extern __shared__ float shared[];
    
    // Reorganize thread mapping for better memory coalescing
    int tid = blockIdx.x * blockDim.x + threadIdx.x;
    int total_elements = batch_size * channels * spatial_size;
    
    if (tid >= total_elements) return;
    
    // Calculate indices using division/modulo to maintain the same logical mapping
    int spatial_idx = tid % spatial_size;
    int tmp = tid / spatial_size;
    int channel_idx = tmp % channels;
    int batch_idx = tmp / channels;
    
    if (batch_idx >= batch_size) return;
    
    int input_idx = batch_idx * channels * spatial_size +
                    channel_idx * spatial_size + spatial_idx;

    // Load value and find max
    float val = input[input_idx];
    float max_val = channel_max_reduce(val, shared);

    // Compute exp normalized by max
    float exp_val = expf(val - max_val);
    float sum_exp = channel_sum_reduce(exp_val, shared);

    // Write normalized value
    output[input_idx] = exp_val / sum_exp;
}

torch::Tensor module_forward(
    torch::Tensor x,
    torch::Tensor conv_weight,
    torch::Tensor conv_bias)
{
    x = x.contiguous().cuda();
    conv_weight = conv_weight.contiguous().cuda();
    conv_bias = conv_bias.contiguous().cuda();

    x = torch::conv3d(x, conv_weight, conv_bias);

    // Activation phase
    int64_t total_elements = x.numel();
    torch::Tensor activated = torch::empty_like(x);
    const int threads = 256;
    int blocks = (total_elements + threads - 1) / threads;
    activation_kernel<<<blocks, threads>>>(x.data_ptr<float>(), activated.data_ptr<float>(), total_elements);

    // Softmax phase
    auto sizes = x.sizes();
    int batch_size = sizes[0];
    int channels = sizes[1];
    int spatial_size = sizes[2] * sizes[3] * sizes[4];
    torch::Tensor softmax_out = torch::empty_like(activated);

    dim3 softmax_blocks(batch_size * spatial_size);
    optimized_softmax_kernel<<<softmax_blocks, channels, channels*sizeof(float)>>>(
        activated.data_ptr<float>(),
        softmax_out.data_ptr<float>(),
        batch_size,
        channels,
        spatial_size
    );

    // Mean reduction
    return softmax_out.view(sizes).mean({2, 3, 4});
}

PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
    m.def(""forward"", &module_forward, ""Optimized fused forward"");
}",conv3d,,,,,1,0,0.13463005,3953
"#include <torch/extension.h>
#include <cuda_runtime.h>
#include <cuda_fp16.h>

#define TILE_SIZE 16
#define UNROLL_FACTOR 4

__global__ void warp_optimized_upper_matmul_kernel(const float* __restrict__ A,
                                                  const float* __restrict__ B,
                                                  float* __restrict__ C,
                                                  int N) {
    // Diagonal block mapping to ensure uniform warp execution
    int block_offset = blockIdx.x * TILE_SIZE;
    int row = block_offset + threadIdx.y;
    int col_start = block_offset + threadIdx.x;
    
    // Process multiple tiles in diagonal band
    for (int tile = 0; tile < gridDim.x; ++tile) {
        int col = col_start + tile * TILE_SIZE;
        
        if (row < N && col < N && row <= col) {
            float sum = 0.0f;
            const int start_k = row;
            const int end_k = col;
            
            // Vectorized loads for aligned access
            const float* A_ptr = &A[row*N + start_k];
            const bool aligned = ((uintptr_t)A_ptr % 16) == 0;
            
            int k = start_k;
            if (aligned && (end_k - start_k + 1) >= UNROLL_FACTOR) {
                const float4* A_vec = reinterpret_cast<const float4*>(A_ptr);
                const int vec_steps = (end_k - start_k + 1) / UNROLL_FACTOR;
                
                #pragma unroll
                for (int i = 0; i < vec_steps; ++i) {
                    float4 a_chunk = __ldg(A_vec + i);
                    sum += a_chunk.x * __ldg(&B[(k + i*UNROLL_FACTOR)*N + col]);
                    sum += a_chunk.y * __ldg(&B[(k + i*UNROLL_FACTOR + 1)*N + col]);
                    sum += a_chunk.z * __ldg(&B[(k + i*UNROLL_FACTOR + 2)*N + col]);
                    sum += a_chunk.w * __ldg(&B[(k + i*UNROLL_FACTOR + 3)*N + col]);
                }
                k += vec_steps * UNROLL_FACTOR;
            }
            
            // Process remaining elements
            #pragma unroll
            for (; k <= end_k; ++k) {
                sum += __ldg(&A[row*N + k]) * __ldg(&B[k*N + col]);
            }
            
            C[row*N + col] = sum;
        }
    }
}

torch::Tensor warp_optimized_upper_matmul(torch::Tensor A, torch::Tensor B) {
    int N = A.size(0);
    auto C = torch::zeros_like(A);
    
    dim3 threads(TILE_SIZE, TILE_SIZE);
    int num_blocks = (N + TILE_SIZE - 1) / TILE_SIZE;
    
    warp_optimized_upper_matmul_kernel<<<num_blocks, threads>>>(A.data_ptr<float>(),
                                                                B.data_ptr<float>(),
                                                                C.data_ptr<float>(),
                                                                N);
    return C;
}

PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
    m.def(""forward"", &warp_optimized_upper_matmul, ""Warp-optimized upper triangular matmul"");
}",other,,,,,1,0,0.13938712,2932
"#include <pybind11/pybind11.h>
#include <torch/extension.h>
#include <cuda_runtime.h>
#include <math.h>

namespace py = pybind11;

__device__ __forceinline__ float warp_reduce_sum(float val) {
    #pragma unroll
    for (int offset = 16; offset > 0; offset /= 2)
        val += __shfl_down_sync(0xffffffff, val, offset);
    return val;
}

__global__ void fused_kernel(
    const float* __restrict__ x,                    // [N, K]
    const float* __restrict__ gemm_weight,          // [M, K]
    const float* __restrict__ gemm_bias,            // [M]
    const float* __restrict__ batch_norm_weight,    // [M]
    const float* __restrict__ batch_norm_bias,      // [M]
    const float* __restrict__ bn_running_mean,      // [M]
    const float* __restrict__ bn_running_var,       // [M]
    const float* __restrict__ group_norm_weight,    // [M]
    const float* __restrict__ group_norm_bias,      // [M]
    int num_groups,
    int N,                                          // batch size
    int K,                                          // input feature dimension
    int M,                                          // output feature dimension
    float* __restrict__ output                      // [N, 1]
) {
    extern __shared__ float shared_mem[];
    float* s_data = shared_mem;                     // size = M floats
    float* s_group_stats = &shared_mem[M];          // size = num_groups * 2 floats

    const int tid = threadIdx.x;
    const int bid = blockIdx.x;
    const int num_threads = blockDim.x;
    const int lane_id = tid & 31;
    const int warp_id = tid >> 5;
    const int num_warps = num_threads >> 5;

    // Step 1: GEMM + BatchNorm + GELU using grid-stride loop
    const int stride = num_threads * gridDim.x;
    for (int idx = tid + bid * num_threads; idx < N * M; idx += stride) {
        const int n = idx / M;
        const int m = idx % M;
        
        if (n < N && m < M) {
            float dot = 0.0f;
            #pragma unroll 4
            for (int k = 0; k < K; k++) {
                dot += x[n * K + k] * gemm_weight[m * K + k];
            }
            
            float val = dot + gemm_bias[m];
            val = (val - bn_running_mean[m]) / sqrtf(bn_running_var[m] + 1e-5f);
            val = val * batch_norm_weight[m] + batch_norm_bias[m];
            val = 0.5f * val * (1.0f + erff(val / 1.41421356f));
            
            if (n == bid) {  // Only store in shared memory for current block's sample
                s_data[m] = val;
            }
        }
    }
    __syncthreads();

    if (bid >= N) return;  // Early exit for excess blocks

    // Step 2: Group Normalization
    const int group_size = M / num_groups;
    
    // Compute group statistics using warp-level reductions
    for (int g = warp_id; g < num_groups; g += num_warps) {
        float sum = 0.0f;
        float sum_sq = 0.0f;
        
        #pragma unroll 4
        for (int i = lane_id; i < group_size; i += 32) {
            const int idx = g * group_size + i;
            if (idx < M) {
                const float val = s_data[idx];
                sum += val;
                sum_sq += val * val;
            }
        }
        
        sum = warp_reduce_sum(sum);
        sum_sq = warp_reduce_sum(sum_sq);
        
        if (lane_id == 0) {
            const float mean = sum / group_size;
            const float var = fmaxf((sum_sq / group_size) - (mean * mean), 0.0f);
            s_group_stats[g * 2] = mean;
            s_group_stats[g * 2 + 1] = var;
        }
    }
    __syncthreads();

    // Apply group normalization using grid-stride loop
    for (int m = tid; m < M; m += num_threads) {
        const int g = m / group_size;
        const float mean = s_group_stats[g * 2];
        const float var = s_group_stats[g * 2 + 1];
        float val = (s_data[m] - mean) / sqrtf(var + 1e-5f);
        val = val * group_norm_weight[m] + group_norm_bias[m];
        s_data[m] = val;
    }
    __syncthreads();

    // Step 3: Final mean reduction using grid-stride loop
    float local_sum = 0.0f;
    for (int m = tid; m < M; m += num_threads) {
        local_sum += s_data[m];
    }
    
    local_sum = warp_reduce_sum(local_sum);
    
    if (lane_id == 0) {
        s_data[warp_id] = local_sum;
    }
    __syncthreads();
    
    if (tid == 0) {
        float final_sum = 0.0f;
        for (int i = 0; i < num_warps; i++) {
            final_sum += s_data[i];
        }
        float mean = final_sum / M;
        output[bid] = mean > 0.0f ? mean : 0.0f;
    }
}

torch::Tensor forward(
    const torch::Tensor& x,
    const torch::Tensor& gemm_weight,
    const torch::Tensor& gemm_bias,
    const torch::Tensor& batch_norm_weight,
    const torch::Tensor& batch_norm_bias,
    const torch::Tensor& batch_norm_running_mean,
    const torch::Tensor& batch_norm_running_var,
    const torch::Tensor& group_norm_weight,
    const torch::Tensor& group_norm_bias,
    const int64_t num_groups
) {
    const int N = x.size(0);
    const int K = x.size(1);
    const int M = gemm_weight.size(0);

    auto output = torch::empty({N, 1}, x.options());

    const int threads_per_block = 256;
    const int num_sm = 108;  // H100 has 108 SMs
    const int blocks = min(N, num_sm * 2);  // 2 blocks per SM for better occupancy
    const size_t shared_mem_size = (M + num_groups * 2) * sizeof(float);

    fused_kernel<<<blocks, threads_per_block, shared_mem_size>>>(
        x.data_ptr<float>(),
        gemm_weight.data_ptr<float>(),
        gemm_bias.data_ptr<float>(),
        batch_norm_weight.data_ptr<float>(),
        batch_norm_bias.data_ptr<float>(),
        batch_norm_running_mean.data_ptr<float>(),
        batch_norm_running_var.data_ptr<float>(),
        group_norm_weight.data_ptr<float>(),
        group_norm_bias.data_ptr<float>(),
        num_groups,
        N,
        K,
        M,
        output.data_ptr<float>()
    );

    return output;
}

PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
    m.def(""forward"", &forward, ""Fused GEMM-BatchNorm-GELU-GroupNorm-Mean-ReLU forward (CUDA)"");
}",gemm,,,,,1,0,0.14324783,6060
"#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>
#include <vector>
#include <cooperative_groups.h>

namespace cg = cooperative_groups;

__inline__ __device__ float blockReduceSum(float val) {
    auto grid = cg::this_grid();
    auto block = cg::this_thread_block();
    auto warp = cg::tiled_partition<32>(block);
    
    __shared__ float shared[32];
    int lane = threadIdx.x % warpSize;
    int wid = threadIdx.x / warpSize;

    // Warp-level reduction
    #pragma unroll
    for (int offset = warpSize/2; offset > 0; offset /= 2) {
        val += warp.shfl_down(val, offset);
    }

    // Block-level reduction
    if (warp.thread_rank() == 0) shared[wid] = val;
    block.sync();

    val = (threadIdx.x < (blockDim.x / warpSize)) ? shared[lane] : 0.0f;
    
    if (wid == 0) {
        #pragma unroll
        for (int offset = warpSize/2; offset > 0; offset /= 2) {
            val += warp.shfl_down(val, offset);
        }
    }
    return val;
}

__global__ void instance_norm_kernel_coalesced(
    const float* __restrict__ x,
    float* __restrict__ y,
    const float* __restrict__ weight,
    const float* __restrict__ bias,
    int N,
    int C,
    int H,
    int W,
    float eps
) {
    extern __shared__ float temp_storage[];

    int instance_id = blockIdx.x;
    int n = instance_id / C;
    int c = instance_id % C;
    int HW = H * W;

    const float* x_ptr = x + (n * C + c) * HW;
    float* y_ptr = y + (n * C + c) * HW;

    float sum = 0.0f;
    float sum_sq = 0.0f;

    for (int i = threadIdx.x; i < HW; i += blockDim.x) {
        float val = x_ptr[i];
        temp_storage[i] = val;
        sum += val;
        sum_sq += val * val;
    }

    sum = blockReduceSum(sum);
    sum_sq = blockReduceSum(sum_sq);

    __shared__ float mean;
    __shared__ float invstd;
    if (threadIdx.x == 0) {
        mean = sum / HW;
        float var = (sum_sq / HW) - (mean * mean);
        var = (var < 0.f) ? 0.f : var;
        invstd = rsqrtf(var + eps);
    }
    __syncthreads();

    float scale = (weight != nullptr) ? weight[c] : 1.0f;
    float shift = (bias != nullptr) ? bias[c] : 0.0f;

    for (int i = threadIdx.x; i < HW; i += blockDim.x) {
        float val = temp_storage[i];
        y_ptr[i] = ((val - mean) * invstd) * scale + shift;
    }
}

torch::Tensor forward(
    torch::Tensor x,
    torch::Tensor weight,
    torch::Tensor bias,
    double eps
) {
    TORCH_CHECK(x.is_cuda(), ""x must be a CUDA tensor"");
    if (weight.defined() && weight.numel() > 0)
        TORCH_CHECK(weight.is_cuda(), ""weight must be a CUDA tensor"");
    if (bias.defined() && bias.numel() > 0)
        TORCH_CHECK(bias.is_cuda(), ""bias must be a CUDA tensor"");

    auto sizes = x.sizes();
    TORCH_CHECK(sizes.size() == 4, ""Input tensor must be 4D: (N, C, H, W)"");
    int N = sizes[0];
    int C = sizes[1];
    int H = sizes[2];
    int W = sizes[3];

    auto y = torch::empty_like(x);

    int HW = H * W;
    int block_size = 256; // Fixed block size for simplicity
    int blocks = N * C;
    int shared_mem_size = HW * sizeof(float);

    instance_norm_kernel_coalesced<<<blocks, block_size, shared_mem_size>>>(
        x.data_ptr<float>(),
        y.data_ptr<float>(),
        (weight.defined() && weight.numel() > 0) ? weight.data_ptr<float>() : nullptr,
        (bias.defined() && bias.numel() > 0) ? bias.data_ptr<float>() : nullptr,
        N, C, H, W,
        static_cast<float>(eps)
    );

    return y;
}

PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
    m.def(""forward"", &forward, ""Instance Normalization forward (CUDA)"");
}
",other,,,,,1,0,0.14551212,3585
"#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>

#define TILE_DIM 16

__global__ void gemm_kernel(
    const float* __restrict__ input,
    const float* __restrict__ weight,
    const float* __restrict__ bias,
    float* output,
    const int M,
    const int N,
    const int K
) {
    __shared__ float tileA[TILE_DIM][TILE_DIM];
    __shared__ float tileB[TILE_DIM][TILE_DIM];

    int row = blockIdx.y * TILE_DIM + threadIdx.y;
    int col = blockIdx.x * TILE_DIM + threadIdx.x;

    float sum = 0.0f;
    // Loop over tiles
    for (int t = 0; t < (K + TILE_DIM - 1) / TILE_DIM; ++t) {
        int tiledCol = t * TILE_DIM + threadIdx.x;
        int tiledRow = t * TILE_DIM + threadIdx.y;

        // Load tile from input matrix
        if (row < M && tiledCol < K)
            tileA[threadIdx.y][threadIdx.x] = input[row * K + tiledCol];
        else
            tileA[threadIdx.y][threadIdx.x] = 0.0f;

        // Load tile from weight matrix. Note weight is stored as [K x N].
        if (col < N && tiledRow < K)
            tileB[threadIdx.y][threadIdx.x] = weight[tiledRow * N + col];
        else
            tileB[threadIdx.y][threadIdx.x] = 0.0f;

        __syncthreads();

        // Multiply the two tiles together
        for (int k = 0; k < TILE_DIM; ++k) {
            sum += tileA[threadIdx.y][k] * tileB[k][threadIdx.x];
        }

        __syncthreads();
    }

    if (row < M && col < N) {
        output[row * N + col] = sum + bias[col];
    }
}

__global__ void batchnorm_scale_kernel(
    float* x,
    const float* mean,
    const float* var,
    const float* weight,
    const float* bias,
    const float* scale,
    const float eps,
    const int N,
    const int D
) {
    const int tid = blockIdx.x * blockDim.x + threadIdx.x;
    const int stride = blockDim.x * gridDim.x;
    
    for (int i = tid; i < N * D; i += stride) {
        const int row = i / D;
        const int col = i % D;
        
        float val = x[i];
        val = (val - mean[col]) / sqrt(var[col] + eps);
        if (weight != nullptr && bias != nullptr) {
            val = val * weight[col] + bias[col];
        }
        val = val * scale[col];
        x[i] = val;
    }
}

__global__ void softmax_kernel(
    float* x,
    const int N,
    const int D
) {
    const int row = blockIdx.x * blockDim.x + threadIdx.x;
    
    if (row < N) {
        float max_val = -INFINITY;
        for (int i = 0; i < D; ++i) {
            max_val = max(max_val, x[row * D + i]);
        }
        
        float sum = 0.0f;
        for (int i = 0; i < D; ++i) {
            float val = exp(x[row * D + i] - max_val);
            x[row * D + i] = val;
            sum += val;
        }
        
        for (int i = 0; i < D; ++i) {
            x[row * D + i] /= sum;
        }
    }
}

torch::Tensor forward(
    torch::Tensor x,
    torch::Tensor running_mean,
    torch::Tensor running_var,
    double bn_eps,
    double bn_momentum,
    torch::Tensor weight,
    torch::Tensor bias,
    torch::Tensor scale,
    torch::Tensor gemm_weight,
    torch::Tensor gemm_bias
) {
    if (!x.is_cuda()) throw std::runtime_error(""Input must be a CUDA tensor"");
    
    const int batch_size = x.size(0);
    const int in_features = x.size(1);
    const int out_features = gemm_weight.size(0);
    
    auto options = torch::TensorOptions().dtype(x.dtype()).device(x.device());
    auto output = torch::empty({batch_size, out_features}, options);
    
    dim3 gemm_block(16, 16);
    dim3 gemm_grid(
        (out_features + gemm_block.x - 1) / gemm_block.x,
        (batch_size + gemm_block.y - 1) / gemm_block.y
    );
    
    gemm_kernel<<<gemm_grid, gemm_block>>>(
        x.data_ptr<float>(),
        gemm_weight.data_ptr<float>(),
        gemm_bias.data_ptr<float>(),
        output.data_ptr<float>(),
        batch_size,
        out_features,
        in_features
    );
    
    const int threads = 256;
    const int blocks = (batch_size * out_features + threads - 1) / threads;
    
    batchnorm_scale_kernel<<<blocks, threads>>>(
        output.data_ptr<float>(),
        running_mean.data_ptr<float>(),
        running_var.data_ptr<float>(),
        weight.defined() ? weight.data_ptr<float>() : nullptr,
        bias.defined() ? bias.data_ptr<float>() : nullptr,
        scale.data_ptr<float>(),
        bn_eps,
        batch_size,
        out_features
    );
    
    const int softmax_threads = 256;
    const int softmax_blocks = (batch_size + softmax_threads - 1) / softmax_threads;
    
    softmax_kernel<<<softmax_blocks, softmax_threads>>>(
        output.data_ptr<float>(),
        batch_size,
        out_features
    );
    
    return output;
}

PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
    m.def(""forward"", &forward, ""Custom CUDA forward function"");
}",gemm,,,,,1,0,0.14880773,4792
"#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>
#include <stdexcept>
#include <type_traits>

template <typename scalar_t>
__global__ void hardtanh_shared_vectorized(const scalar_t* __restrict__ x,
                                           scalar_t* __restrict__ out,
                                           int64_t numel,
                                           scalar_t min_val,
                                           scalar_t max_val) {
  constexpr int VEC_WIDTH = (sizeof(scalar_t) == 4) ? 4 : 2;
  using vec_t = typename std::conditional<sizeof(scalar_t) == 4, float4, double2>::type;
  
  extern __shared__ char shared_buffer[];
  vec_t* shared_vec = reinterpret_cast<vec_t*>(shared_buffer);

  const int64_t vec_num = numel / VEC_WIDTH;
  const int64_t vec_idx = blockIdx.x * blockDim.x + threadIdx.x;
  const int64_t vec_stride = blockDim.x * gridDim.x;

  // Vectorized processing with shared memory
  for (int64_t i = vec_idx; i < vec_num; i += vec_stride) {
    vec_t v = __ldg(reinterpret_cast<const vec_t*>(x) + i);
    shared_vec[threadIdx.x] = v;
    __syncthreads();

    vec_t sv = shared_vec[threadIdx.x];
    if constexpr (sizeof(scalar_t) == 4) {
      sv.x = max(min_val, min(max_val, sv.x));
      sv.y = max(min_val, min(max_val, sv.y));
      sv.z = max(min_val, min(max_val, sv.z));
      sv.w = max(min_val, min(max_val, sv.w));
    } else {
      sv.x = max(min_val, min(max_val, sv.x));
      sv.y = max(min_val, min(max_val, sv.y));
    }

    reinterpret_cast<vec_t*>(out)[i] = sv;
    __syncthreads();
  }

  // Scalar remainder processing
  const int64_t scalar_start = vec_num * VEC_WIDTH;
  const int64_t scalar_idx = scalar_start + blockIdx.x * blockDim.x + threadIdx.x;
  if (scalar_idx < numel) {
    scalar_t val = __ldg(x + scalar_idx);
    out[scalar_idx] = max(min_val, min(max_val, val));
  }
}

at::Tensor forward_cuda(const at::Tensor& x, float min_val, float max_val) {
  auto out = at::empty_like(x);
  const int64_t numel = x.numel();

  constexpr int THREADS = 256;
  const int vec_blocks = (numel / (THREADS * (sizeof(float) == 4 ? 4 : 2))) + 1;
  const int blocks = std::min(vec_blocks, 65535);
  const size_t shared_size = THREADS * sizeof(typename std::conditional<sizeof(float) == 4, float4, double2>::type);

  AT_DISPATCH_FLOATING_TYPES(x.scalar_type(), ""hardtanh_shared"", ([&] {
    hardtanh_shared_vectorized<scalar_t><<<blocks, THREADS, shared_size>>>(
      x.data_ptr<scalar_t>(),
      out.data_ptr<scalar_t>(),
      numel,
      static_cast<scalar_t>(min_val),
      static_cast<scalar_t>(max_val)
    );
  }));

  return out;
}

at::Tensor forward(const at::Tensor& x, float min_val, float max_val) {
  if (!x.is_cuda()) {
    throw std::invalid_argument(""Input tensor must be a CUDA tensor"");
  }
  return forward_cuda(x, min_val, max_val);
}

PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
  m.def(""forward"", &forward, ""HardTanh with shared memory vectorization (CUDA)"");
}",other,,,,,1,0,0.14975464,2970
"#include <torch/extension.h>
#include <ATen/cuda/CUDAContext.h>
#include <cuda.h>
#include <cuda_runtime.h>
#include <pybind11/pybind11.h>
#include <pybind11/stl.h>
#include <stdexcept>

// Define maximum number of elements that can be stored in constant memory
// For example, 16384 floats (64KB) or 16384 doubles (128KB) depending on hardware limits
#define MAX_CONST_SIZE 4096

// Declare constant memory arrays for float and double types
__constant__ float d_weight_const_float[MAX_CONST_SIZE];
__constant__ double d_weight_const_double[MAX_CONST_SIZE];

// Helper function to fetch from constant memory
// Specialization for float
template <typename scalar_t>
__device__ __forceinline__ scalar_t get_const_weight(int idx);

template <>
__device__ __forceinline__ float get_const_weight<float>(int idx) {
    return d_weight_const_float[idx];
}

// Specialization for double
template <>
__device__ __forceinline__ double get_const_weight<double>(int idx) {
    return d_weight_const_double[idx];
}

// CUDA kernel for depthwise 2D convolution using constant memory for the kernel weights
template <typename scalar_t>
__global__ void depthwiseConv2DKernelConstant(
    const scalar_t* __restrict__ x,
    // Weight is not passed as parameter; it is stored in constant memory
    const scalar_t* __restrict__ b,
    scalar_t* __restrict__ out,
    const int batch_size,
    const int in_channels,
    const int in_height,
    const int in_width,
    const int kernel_size,
    const int out_height,
    const int out_width,
    const int stride,
    const int padding
) {
    // blockIdx.z encodes combined (n, c) indices
    int bc = blockIdx.z;
    int c = bc % in_channels;
    int n = bc / in_channels;

    // Compute output spatial coordinates
    int h_out = blockIdx.y * blockDim.y + threadIdx.y;
    int w_out = blockIdx.x * blockDim.x + threadIdx.x;

    if (h_out < out_height && w_out < out_width) {
        const int batch_channel_offset = n * in_channels + c;
        scalar_t value = 0;
        // Compute top-left corner in input
        const int h_in_base = h_out * stride - padding;
        const int w_in_base = w_out * stride - padding;

        #pragma unroll
        for (int kh = 0; kh < kernel_size; ++kh) {
            int h_in = h_in_base + kh;
            if (h_in >= 0 && h_in < in_height) {
                #pragma unroll
                for (int kw = 0; kw < kernel_size; ++kw) {
                    int w_in = w_in_base + kw;
                    if (w_in >= 0 && w_in < in_width) {
                        int x_index = (batch_channel_offset * in_height + h_in) * in_width + w_in;
                        // Compute weight index: weights are stored consecutively per channel
                        int weight_index = (c * kernel_size * kernel_size) + (kh * kernel_size + kw);
                        value += x[x_index] * get_const_weight<scalar_t>(weight_index);
                    }
                }
            }
        }
        value += b[c];

        int out_index = (batch_channel_offset * out_height + h_out) * out_width + w_out;
        out[out_index] = value;
    }
}

// Forward implementation that copies the weight tensor into constant memory and launches the kernel

torch::Tensor forward_impl(
    torch::Tensor x,
    torch::Tensor weight, // Expected shape: (in_channels, 1, kernel_size, kernel_size)
    torch::Tensor bias,
    int stride,
    int padding,
    int groups
) {
    const int batch_size = x.size(0);
    const int in_channels = x.size(1);
    const int in_height = x.size(2);
    const int in_width = x.size(3);
    const int kernel_size = weight.size(2);
    const int out_height = (in_height + 2 * padding - kernel_size) / stride + 1;
    const int out_width  = (in_width  + 2 * padding - kernel_size) / stride + 1;

    // Total number of weight elements
    auto weight_elements = weight.numel();
    if (weight_elements > MAX_CONST_SIZE) {
        throw std::runtime_error(""Kernel weight size exceeds constant memory capacity."");
    }

    // Copy weight data to constant memory
    AT_DISPATCH_FLOATING_TYPES(weight.scalar_type(), ""copy_weight_to_constant"", ([&] {
        if (std::is_same<scalar_t, float>::value) {
            cudaMemcpyToSymbol(d_weight_const_float, weight.data_ptr<scalar_t>(), weight_elements * sizeof(scalar_t));
        } else if (std::is_same<scalar_t, double>::value) {
            cudaMemcpyToSymbol(d_weight_const_double, weight.data_ptr<scalar_t>(), weight_elements * sizeof(scalar_t));
        }
    }));

    auto out = torch::empty({batch_size, in_channels, out_height, out_width}, x.options());

    // Configure grid and block dimensions
    // Using 16x16 thread block for good occupancy and coalesced access
    const dim3 threads(16, 16);
    const dim3 blocks(
         (out_width + threads.x - 1) / threads.x,
         (out_height + threads.y - 1) / threads.y,
         batch_size * in_channels
    );

    AT_DISPATCH_FLOATING_TYPES(x.scalar_type(), ""depthwise_conv2d_constant"", ([&] {
       depthwiseConv2DKernelConstant<scalar_t><<<blocks, threads>>>(
           x.data_ptr<scalar_t>(),
           bias.data_ptr<scalar_t>(),
           out.data_ptr<scalar_t>(),
           batch_size, in_channels, in_height, in_width,
           kernel_size, out_height, out_width,
           stride, padding
       );
    }));

    return out;
}

// Wrap forward_impl to handle optional bias

torch::Tensor forward_wrap(
    torch::Tensor x,
    torch::Tensor weight,
    pybind11::object bias_obj,
    int stride,
    int padding,
    int groups
) {
    torch::Tensor bias;
    if (bias_obj.is_none()) {
        bias = torch::zeros({x.size(1)}, x.options());
    } else {
        bias = bias_obj.cast<torch::Tensor>();
    }
    return forward_impl(x, weight, bias, stride, padding, groups);
}

namespace py = pybind11;

PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
    m.def(
        ""forward"",
        &forward_wrap,
        ""Depthwise conv2d forward with constant memory for weights"",
        py::arg(""x""),
        py::arg(""weight""),
        py::arg(""bias"") = py::none(),
        py::arg(""stride"") = 1,
        py::arg(""padding"") = 0,
        py::arg(""groups"") = 1
    );
}
",conv2d,,,,,1,0,0.15035936,6181
"#include <torch/extension.h>
#include <cuda_runtime.h>

#define THREADS_PER_BLOCK 256

__global__ void unrolled_fused_kernel(
    const float* __restrict__ x,
    const float* __restrict__ weight,
    const float* __restrict__ conv_bias,
    float scale1,
    float scale2,
    const float* __restrict__ final_bias,
    float* __restrict__ output,
    int N, int C_in, int D_in, int H_in, int W_in,
    int C_out, int kD, int kH, int kW,
    int stride, int pad,
    int D_pool, int H_pool, int W_pool
) {
    const int tid = blockIdx.x * blockDim.x + threadIdx.x;
    const int total = N * C_out * D_pool * H_pool * W_pool;
    if (tid >= total) return;

    // Coalesced output indexing
    const int w_pool = tid % W_pool;
    const int h_pool = (tid / W_pool) % H_pool;
    const int d_pool = (tid / (W_pool * H_pool)) % D_pool;
    const int oc = (tid / (W_pool * H_pool * D_pool)) % C_out;
    const int n = tid / (W_pool * H_pool * D_pool * C_out);

    float sum = 0.0f;
    const float bias_val = conv_bias[oc];

    // Force unroll all short loops
    #pragma unroll
    for (int pd = 0; pd < 2; pd++) {
        const int d_conv = d_pool * 2 + pd;
        #pragma unroll
        for (int ph = 0; ph < 2; ph++) {
            const int h_conv = h_pool * 2 + ph;
            #pragma unroll
            for (int pw = 0; pw < 2; pw++) {
                const int w_conv = w_pool * 2 + pw;
                
                float conv_val = bias_val;
                
                #pragma unroll
                for (int ic = 0; ic < C_in; ic++) {
                    #pragma unroll
                    for (int kd = 0; kd < kD; kd++) {
                        const int d_offset = d_conv + pad - kd;
                        if (d_offset < 0 || d_offset % stride != 0) continue;
                        const int id = d_offset / stride;
                        if (id >= D_in) continue;
                        
                        #pragma unroll
                        for (int kh = 0; kh < kH; kh++) {
                            const int h_offset = h_conv + pad - kh;
                            if (h_offset < 0 || h_offset % stride != 0) continue;
                            const int ih = h_offset / stride;
                            if (ih >= H_in) continue;
                            
                            #pragma unroll
                            for (int kw = 0; kw < kW; kw++) {
                                const int w_offset = w_conv + pad - kw;
                                if (w_offset < 0 || w_offset % stride != 0) continue;
                                const int iw = w_offset / stride;
                                if (iw >= W_in) continue;
                                
                                const int x_idx = (((n * C_in + ic) * D_in + id) * H_in + ih) * W_in + iw;
                                const int w_idx = ((((ic * C_out + oc) * kD) + kd) * kH + kh) * kW + kw;
                                
                                conv_val += x[x_idx] * weight[w_idx];
                            }
                        }
                    }
                }
                sum += conv_val * scale1;
            }
        }
    }

    // Final scaling and bias
    output[tid] = (sum / 8.0f + final_bias[oc]) * scale2;
}

at::Tensor forward(
    const at::Tensor& x,
    int64_t stride,
    int64_t padding,
    const at::Tensor& conv_transpose,
    const at::Tensor& conv_transpose_bias,
    const at::Tensor& scale1,
    const at::Tensor& scale2,
    const at::Tensor& bias
) {
    const auto sizes = x.sizes();
    const int N = sizes[0];
    const int C_in = sizes[1];
    const int D_in = sizes[2];
    const int H_in = sizes[3];
    const int W_in = sizes[4];

    const auto wt_sizes = conv_transpose.sizes();
    const int C_out = wt_sizes[1];
    const int kD = wt_sizes[2];
    const int kH = wt_sizes[3];
    const int kW = wt_sizes[4];

    const int D_conv = (D_in - 1) * stride - 2 * padding + kD;
    const int H_conv = (H_in - 1) * stride - 2 * padding + kH;
    const int W_conv = (W_in - 1) * stride - 2 * padding + kW;

    const int D_pool = D_conv / 2;
    const int H_pool = H_conv / 2;
    const int W_pool = W_conv / 2;

    auto output = at::empty({N, C_out, D_pool, H_pool, W_pool}, x.options());
    
    const int total = N * C_out * D_pool * H_pool * W_pool;
    const int blocks = (total + THREADS_PER_BLOCK - 1) / THREADS_PER_BLOCK;
    
    unrolled_fused_kernel<<<blocks, THREADS_PER_BLOCK>>>(
        x.data_ptr<float>(),
        conv_transpose.data_ptr<float>(),
        conv_transpose_bias.data_ptr<float>(),
        scale1.item<float>(),
        scale2.item<float>(),
        bias.data_ptr<float>(),
        output.data_ptr<float>(),
        N, C_in, D_in, H_in, W_in,
        C_out, kD, kH, kW,
        stride, padding,
        D_pool, H_pool, W_pool
    );

    return output;
}

PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
    m.def(""forward"", &forward, ""Unrolled ConvTranspose3d with fused avgpool (CUDA)"");
}
",other,,,,,1,0,0.15249638,5033
"#include <torch/extension.h>
#include <pybind11/pybind11.h>
#include <cuda_runtime.h>
#include <cooperative_groups.h>

namespace py = pybind11;
namespace cg = cooperative_groups;

// Shared memory reduction kernel for batch norm statistics
template<int BLOCK_SIZE>
__global__ void batch_norm_stats_kernel(
    const float* input,
    float* mean,
    float* var,
    int N, int C, int H, int W
) {
    using namespace cooperative_groups;
    thread_block block = this_thread_block();
    
    __shared__ float shared_sum[BLOCK_SIZE];
    __shared__ float shared_sq_sum[BLOCK_SIZE];
    
    int tid = threadIdx.x;
    int c = blockIdx.x;
    
    float local_sum = 0.0f;
    float local_sq_sum = 0.0f;
    
    // Thread-local accumulation
    for (int n = 0; n < N; n++) {
        for (int h = 0; h < H; h++) {
            for (int w = tid; w < W; w += BLOCK_SIZE) {
                float val = input[n*C*H*W + c*H*W + h*W + w];
                local_sum += val;
                local_sq_sum += val * val;
            }
        }
    }
    
    // Store in shared memory
    shared_sum[tid] = local_sum;
    shared_sq_sum[tid] = local_sq_sum;
    block.sync();
    
    // Parallel reduction in shared memory
    for (int stride = BLOCK_SIZE/2; stride > 32; stride >>= 1) {
        if (tid < stride) {
            shared_sum[tid] += shared_sum[tid + stride];
            shared_sq_sum[tid] += shared_sq_sum[tid + stride];
        }
        block.sync();
    }
    
    // Warp-level reduction using shuffle
    thread_block_tile<32> warp = tiled_partition<32>(block);
    if (tid < 32) {
        float warp_sum = shared_sum[tid];
        float warp_sq_sum = shared_sq_sum[tid];
        
        #pragma unroll
        for (int offset = 16; offset > 0; offset >>= 1) {
            warp_sum += __shfl_down_sync(0xffffffff, warp_sum, offset);
            warp_sq_sum += __shfl_down_sync(0xffffffff, warp_sq_sum, offset);
        }
        
        if (tid == 0) {
            float inv_size = 1.0f / (N * H * W);
            mean[c] = warp_sum * inv_size;
            float mean_val = mean[c];
            var[c] = (warp_sq_sum * inv_size - mean_val * mean_val) + 1e-5f;
        }
    }
}

torch::Tensor optimized_batch_norm(
    torch::Tensor input,
    torch::Tensor weight,
    torch::Tensor bias,
    torch::Tensor running_mean,
    torch::Tensor running_var,
    bool training
) {
    const int N = input.size(0);
    const int C = input.size(1);
    const int H = input.size(2);
    const int W = input.size(3);
    
    constexpr int BLOCK_SIZE = 256;
    
    auto options = torch::TensorOptions()
        .dtype(input.dtype())
        .device(input.device());
    
    auto output = torch::empty_like(input);
    
    if (training) {
        auto mean = torch::empty({C}, options);
        auto var = torch::empty({C}, options);
        
        dim3 grid(C);
        dim3 block(BLOCK_SIZE);
        
        batch_norm_stats_kernel<BLOCK_SIZE><<<grid, block>>>(
            input.data_ptr<float>(),
            mean.data_ptr<float>(),
            var.data_ptr<float>(),
            N, C, H, W
        );
        
        // Update running stats
        running_mean.copy_(running_mean * 0.9 + mean * 0.1);
        running_var.copy_(running_var * 0.9 + var * 0.1);
    }
    
    return torch::batch_norm(
        input,
        weight,
        bias,
        running_mean,
        running_var,
        training,
        /*momentum=*/0.1,
        /*eps=*/1e-5,
        /*cudnn_enabled=*/true
    );
}

torch::Tensor basic_block_fn(
    torch::Tensor x,
    torch::Tensor conv1_w,
    torch::Tensor bn1_w,
    torch::Tensor bn1_b,
    torch::Tensor bn1_rm,
    torch::Tensor bn1_rv,
    torch::Tensor conv2_w,
    torch::Tensor bn2_w,
    torch::Tensor bn2_b,
    torch::Tensor bn2_rm,
    torch::Tensor bn2_rv,
    torch::Tensor downsample_conv_w,
    torch::Tensor downsample_bn_w,
    torch::Tensor downsample_bn_b,
    torch::Tensor downsample_bn_rm,
    torch::Tensor downsample_bn_rv,
    int64_t stride,
    bool is_training
) {
    torch::Tensor identity = x;
    
    x = torch::conv2d(x, conv1_w, /*bias=*/{}, /*stride=*/{stride, stride}, /*padding=*/{1, 1});
    x = optimized_batch_norm(x, bn1_w, bn1_b, bn1_rm, bn1_rv, is_training);
    x = torch::relu(x);
    
    x = torch::conv2d(x, conv2_w, /*bias=*/{}, /*stride=*/{1, 1}, /*padding=*/{1, 1});
    x = optimized_batch_norm(x, bn2_w, bn2_b, bn2_rm, bn2_rv, is_training);
    
    if (downsample_conv_w.defined()) {
        identity = torch::conv2d(identity, downsample_conv_w, /*bias=*/{}, /*stride=*/{stride, stride});
        identity = optimized_batch_norm(
            identity,
            downsample_bn_w,
            downsample_bn_b,
            downsample_bn_rm,
            downsample_bn_rv,
            is_training
        );
    }
    
    x += identity;
    x = torch::relu(x);
    return x;
}

torch::Tensor module_fn(torch::Tensor x, py::object params_py, bool is_training) {
    auto get_param = [&](const std::string& key) -> torch::Tensor {
        return params_py.attr(""__getitem__"")(key.c_str()).cast<torch::Tensor>();
    };
    
    // Initial layers
    auto conv1_weight = get_param(""conv1_weight"");
    auto bn1_weight = get_param(""bn1_weight"");
    auto bn1_bias = get_param(""bn1_bias"");
    auto bn1_running_mean = get_param(""bn1_running_mean"");
    auto bn1_running_var = get_param(""bn1_running_var"");

    x = torch::conv2d(x, conv1_weight, /*bias=*/{}, /*stride=*/{2, 2}, /*padding=*/{3, 3});
    x = optimized_batch_norm(x, bn1_weight, bn1_bias, bn1_running_mean, bn1_running_var, is_training);
    x = torch::relu(x);
    x = torch::max_pool2d(x, /*kernel_size=*/{3, 3}, /*stride=*/{2, 2}, /*padding=*/{1, 1});

    // Layer blocks
    for (int i = 1; i <= 4; ++i) {
        std::string layer_name = ""layer"" + std::to_string(i);
        for (int j = 0; j < 2; ++j) {
            std::string block_name = layer_name + ""_"" + std::to_string(j);
            int64_t stride = (i > 1 && j == 0) ? 2 : 1;

            auto conv1_w = get_param(block_name + ""_conv1_weight"");
            auto bn1_w = get_param(block_name + ""_bn1_weight"");
            auto bn1_b = get_param(block_name + ""_bn1_bias"");
            auto bn1_rm = get_param(block_name + ""_bn1_running_mean"");
            auto bn1_rv = get_param(block_name + ""_bn1_running_var"");

            auto conv2_w = get_param(block_name + ""_conv2_weight"");
            auto bn2_w = get_param(block_name + ""_bn2_weight"");
            auto bn2_b = get_param(block_name + ""_bn2_bias"");
            auto bn2_rm = get_param(block_name + ""_bn2_running_mean"");
            auto bn2_rv = get_param(block_name + ""_bn2_running_var"");

            std::string downsample_conv_key = block_name + ""_downsample_0_weight"";
            bool has_downsample = PyMapping_HasKeyString(params_py.ptr(), downsample_conv_key.c_str()) == 1;

            torch::Tensor downsample_conv_w, downsample_bn_w, downsample_bn_b, downsample_bn_rm, downsample_bn_rv;

            if (has_downsample) {
                downsample_conv_w = get_param(block_name + ""_downsample_0_weight"");
                downsample_bn_w = get_param(block_name + ""_downsample_1_weight"");
                downsample_bn_b = get_param(block_name + ""_downsample_1_bias"");
                downsample_bn_rm = get_param(block_name + ""_downsample_1_running_mean"");
                downsample_bn_rv = get_param(block_name + ""_downsample_1_running_var"");
            }

            x = basic_block_fn(
                x, conv1_w, bn1_w, bn1_b, bn1_rm, bn1_rv,
                conv2_w, bn2_w, bn2_b, bn2_rm, bn2_rv,
                downsample_conv_w, downsample_bn_w, downsample_bn_b,
                downsample_bn_rm, downsample_bn_rv,
                stride, is_training
            );
        }
    }

    x = torch::adaptive_avg_pool2d(x, {1, 1});
    x = x.view({x.size(0), -1});
    auto fc_weight = get_param(""fc_weight"");
    auto fc_bias = get_param(""fc_bias"");
    x = torch::linear(x, fc_weight, fc_bias);
    return x;
}

PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
    m.def(""forward"", &module_fn, ""ResNet18 forward function (CUDA)"");
}",other,,,,,1,0,0.15552709,8133
"#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>
#include <cmath>
#include <cstdio>

// Define maximum sizes for constant memory storage
// Adjust these as needed, ensuring they fit within hardware limits (typically 64KB on many NVIDIA GPUs)
#define MAX_WEIGHT_ELEMENTS 8192
#define MAX_BIAS_ELEMENTS 512

// Declare constant memory for weights and bias (read-only, cached, high bandwidth)
__constant__ float c_weight[MAX_WEIGHT_ELEMENTS];
__constant__ float c_bias[MAX_BIAS_ELEMENTS];

// CUDA kernel using constant memory for conv weights and bias
__global__ void conv_min_tanh_forward_kernel_const(
    const float* __restrict__ x,       // Input tensor: [B, C_in, H, W]
    float* __restrict__ output,          // Output tensor: [B, 1, H_out, W_out]
    const int batch,
    const int in_channels,
    const int in_height,
    const int in_width,
    const int out_channels,
    const int kernel_size,
    const int out_height,
    const int out_width)
{
    // Compute output indices using a 3D grid mapping: (b, out_y, out_x)
    int out_x = blockIdx.x * blockDim.x + threadIdx.x;
    int out_y = blockIdx.y * blockDim.y + threadIdx.y;
    int b = blockIdx.z;

    if (out_x >= out_width || out_y >= out_height || b >= batch) return;

    // Pre-compute strides for input indexing
    const int in_channel_stride = in_height * in_width;
    const int batch_stride = in_channels * in_channel_stride;
    const int weight_channel_stride = in_channels * kernel_size * kernel_size;

    int b_offset = b * batch_stride;
    int out_index = b * out_height * out_width + out_y * out_width + out_x;

    float min_val = 1e20f;

    // Loop over output channels; each convolution output uses corresponding constant bias and weights
    for (int oc = 0; oc < out_channels; ++oc) {
        float conv_sum = c_bias[oc]; // Load bias from constant memory
        int weight_oc_offset = oc * weight_channel_stride;

        // Loop over input channels
        for (int ic = 0; ic < in_channels; ++ic) {
            int ic_offset = ic * in_height * in_width;
            int weight_ic_offset = weight_oc_offset + ic * kernel_size * kernel_size;
            
            // Loop over the kernel window
            for (int ky = 0; ky < kernel_size; ++ky) {
                int in_y = out_y + ky;
                for (int kx = 0; kx < kernel_size; ++kx) {
                    int in_x = out_x + kx;
                    int x_index = b_offset + ic_offset + in_y * in_width + in_x;
                    int w_index = weight_ic_offset + ky * kernel_size + kx;
                    conv_sum += x[x_index] * c_weight[w_index]; // Load weight from constant memory
                }
            }
        }
        if (conv_sum < min_val) {
            min_val = conv_sum;
        }
    }

    // Apply double tanh activation and write output
    output[out_index] = tanhf(tanhf(min_val));
}

// Launcher function to copy constant tensors and launch the kernel
void conv_min_tanh_forward_cuda(
    at::Tensor x,
    at::Tensor conv_weight,
    at::Tensor conv_bias,
    at::Tensor output)
{
    const int batch = x.size(0);
    const int in_channels = x.size(1);
    const int in_height = x.size(2);
    const int in_width = x.size(3);
    
    const int out_channels = conv_weight.size(0);
    const int kernel_size = conv_weight.size(2); // Assumes square kernel
    const int out_height = in_height - kernel_size + 1;
    const int out_width = in_width - kernel_size + 1;

    // Copy conv_weight and conv_bias to constant memory
    int weight_numel = conv_weight.numel();
    int bias_numel = conv_bias.numel();
    TORCH_CHECK(weight_numel <= MAX_WEIGHT_ELEMENTS, ""Weight tensor too large for constant memory"");
    TORCH_CHECK(bias_numel <= MAX_BIAS_ELEMENTS, ""Bias tensor too large for constant memory"");
    
    // Copy data from GPU global memory to constant memory
    cudaMemcpyToSymbol(c_weight, conv_weight.data_ptr<float>(), weight_numel * sizeof(float));
    cudaMemcpyToSymbol(c_bias, conv_bias.data_ptr<float>(), bias_numel * sizeof(float));

    // Configure grid and block dimensions optimized for H100
    dim3 block(32, 8, 1);
    dim3 grid(
        (out_width + block.x - 1) / block.x,
        (out_height + block.y - 1) / block.y,
        batch
    );
    
    conv_min_tanh_forward_kernel_const<<<grid, block>>>(
        x.data_ptr<float>(),
        output.data_ptr<float>(),
        batch,
        in_channels,
        in_height,
        in_width,
        out_channels,
        kernel_size,
        out_height,
        out_width
    );

    cudaError_t err = cudaGetLastError();
    if (err != cudaSuccess) {
        printf(""Error in conv_min_tanh_forward_kernel_const: %s\n"", cudaGetErrorString(err));
    }
}

// C++ interface (called from Python)
at::Tensor forward(
    at::Tensor x,
    at::Tensor conv_weight,
    at::Tensor conv_bias)
{
    TORCH_CHECK(x.is_cuda(), ""x must be a CUDA tensor"");
    TORCH_CHECK(conv_weight.is_cuda(), ""conv_weight must be a CUDA tensor"");
    TORCH_CHECK(conv_bias.is_cuda(), ""conv_bias must be a CUDA tensor"");
    
    const int in_height = x.size(2);
    const int in_width = x.size(3);
    const int kernel_size = conv_weight.size(2);
    const int out_height = in_height - kernel_size + 1;
    const int out_width = in_width - kernel_size + 1;
    const int batch = x.size(0);

    // Allocate output tensor of shape [batch, 1, out_height, out_width]
    auto output = at::empty({batch, 1, out_height, out_width}, x.options());

    // Launch the CUDA kernel
    conv_min_tanh_forward_cuda(x, conv_weight, conv_bias, output);
    return output;
}

PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
    m.def(""forward"", &forward, ""Convolution, min (over channels), and double tanh activation (CUDA) with constant memory"");
}
",conv2d,,,,,1,0,0.15977134,5793
"#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>

__global__ void fused_operations_kernel(
    const float* __restrict__ input,
    const float* __restrict__ conv_weight,
    const float* __restrict__ conv_bias,
    const float* __restrict__ spatial_bias,
    float scaling_factor,
    int stride,
    int padding,
    int batch_size,
    int in_channels,
    int in_depth,
    int in_height,
    int in_width,
    int out_channels,
    int kernel_d,
    int kernel_h,
    int kernel_w,
    int out_depth,
    int out_height,
    int out_width,
    float* __restrict__ output
) {
    extern __shared__ float shared_mem[];
    float* shared_input = shared_mem;
    float* shared_weight = &shared_mem[blockDim.x * blockDim.y * blockDim.z];
    // Calculate 3D grid and block indices
    int w = blockIdx.x * blockDim.x + threadIdx.x;
    int h = blockIdx.y * blockDim.y + threadIdx.y;
    int d = blockIdx.z * blockDim.z + threadIdx.z;

    if (w >= out_width || h >= out_height || d >= out_depth) return;

    for (int b = 0; b < batch_size; ++b) {
        float total = 0.0f;

        for (int oc = 0; oc < out_channels; ++oc) {
            float channel_val = conv_bias[oc];
            
            for (int kd = 0; kd < kernel_d; ++kd) {
                for (int kh = 0; kh < kernel_h; ++kh) {
                    for (int kw = 0; kw < kernel_w; ++kw) {
                        int in_d_unclamped = (d - kd + padding) / stride;
                        int in_h_unclamped = (h - kh + padding) / stride;
                        int in_w_unclamped = (w - kw + padding) / stride;

                        bool stride_valid = ((d - kd + padding) % stride == 0) &&
                                            ((h - kh + padding) % stride == 0) &&
                                            ((w - kw + padding) % stride == 0);
                        bool in_bounds = (in_d_unclamped >= 0 && in_d_unclamped < in_depth) &&
                                         (in_h_unclamped >= 0 && in_h_unclamped < in_height) &&
                                         (in_w_unclamped >= 0 && in_w_unclamped < in_width);
                        float valid = (stride_valid && in_bounds) ? 1.0f : 0.0f;

                        int in_d = max(0, min(in_depth - 1, in_d_unclamped));
                        int in_h = max(0, min(in_height - 1, in_h_unclamped));
                        int in_w = max(0, min(in_width - 1, in_w_unclamped));

                        for (int ic = 0; ic < in_channels; ++ic) {
                            int input_idx = (((b * in_channels + ic) * in_depth + in_d)
                                              * in_height + in_h) * in_width + in_w;
                            int weight_idx = (((ic * out_channels + oc) * kernel_d + kd)
                                               * kernel_h + kh) * kernel_w + kw;
                            
                            channel_val += input[input_idx] * conv_weight[weight_idx] * valid;
                        }
                    }
                }
            }
            total += channel_val;
        }

        float mean_val = total / out_channels;
        int spatial_idx = d * out_height * out_width + h * out_width + w;
        float biased = mean_val + spatial_bias[spatial_idx];
        output[(((b * out_depth + d) * out_height + h) * out_width + w)] = tanhf(1.0f) * scaling_factor;
    }
}

torch::Tensor forward_cuda(
    const torch::Tensor& input,
    const torch::Tensor& conv_weight,
    const torch::Tensor& conv_bias,
    const torch::Tensor& spatial_bias,
    float scaling_factor,
    int stride,
    int padding
) {
    TORCH_CHECK(input.dim() == 5, ""Input must be 5D tensor"");
    TORCH_CHECK(conv_weight.dim() == 5, ""Conv weight must be 5D tensor"");

    const int batch_size = input.size(0);
    const int in_channels = input.size(1);
    const int in_depth = input.size(2);
    const int in_height = input.size(3);
    const int in_width = input.size(4);

    const int out_channels = conv_weight.size(1);
    const int kernel_d = conv_weight.size(2);
    const int kernel_h = conv_weight.size(3);
    const int kernel_w = conv_weight.size(4);

    const int out_depth = (in_depth - 1) * stride + kernel_d - 2 * padding;
    const int out_height = (in_height - 1) * stride + kernel_h - 2 * padding;
    const int out_width = (in_width - 1) * stride + kernel_w - 2 * padding;

    auto options = torch::TensorOptions()
        .dtype(input.dtype())
        .device(input.device());
    torch::Tensor output = torch::empty({batch_size, 1, out_depth, out_height, out_width}, options);

    dim3 threads(8, 8, 8);
    dim3 blocks((out_width + threads.x - 1) / threads.x,
                 (out_height + threads.y - 1) / threads.y,
                 (out_depth + threads.z - 1) / threads.z);

    fused_operations_kernel<<<blocks, threads>>>(
        input.data_ptr<float>(),
        conv_weight.data_ptr<float>(),
        conv_bias.data_ptr<float>(),
        spatial_bias.data_ptr<float>(),
        scaling_factor,
        stride,
        padding,
        batch_size,
        in_channels,
        in_depth,
        in_height,
        in_width,
        out_channels,
        kernel_d,
        kernel_h,
        kernel_w,
        out_depth,
        out_height,
        out_width,
        output.data_ptr<float>()
    );

    return output;
}

PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
    m.def(""forward"", &forward_cuda, ""Fused Transposed Conv3D Operations with Optimized Thread and Block Indexing (CUDA)"");
}
",other,,,,,1,0,0.16124311,5531
"#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>

#define BLOCK_SIZE 256

__inline__ __device__ float warpReduceSum(float val) {
    for (int offset = warpSize / 2; offset > 0; offset /= 2) {
        val += __shfl_down_sync(0xffffffff, val, offset);
    }
    return val;
}

__inline__ __device__ float blockReduceSum(float val) {
    static __shared__ float shared[32]; // Shared mem for 32 partial sums
    int lane = threadIdx.x % warpSize;
    int wid = threadIdx.x / warpSize;

    val = warpReduceSum(val); // Each warp performs partial reduction

    if (lane == 0) shared[wid] = val; // Write reduced value to shared memory
    __syncthreads(); // Wait for all partial reductions

    // Read from shared memory only if that warp existed
    val = (threadIdx.x < blockDim.x / warpSize) ? shared[lane] : 0;

    if (wid == 0) val = warpReduceSum(val); // Final reduce within first warp

    return val;
}

__global__ void fused_kernel(
    const float* __restrict__ x,
    const float* __restrict__ fc_weight,
    const float* __restrict__ fc_bias,
    float* output,
    const float* __restrict__ gn_weight,
    const float* __restrict__ gn_bias,
    int batch_size,
    int in_features,
    int out_features,
    int num_groups,
    float eps,
    float negative_slope) {

    int b = blockIdx.x;      // batch index
    int group = blockIdx.y;  // group index
    int tid = threadIdx.x;

    int channels_per_group = out_features / num_groups;
    int c_start = group * channels_per_group;

    extern __shared__ float shared_mem[];
    float *s_input = shared_mem;                          // size: in_features
    float *s_out = s_input + in_features;                 // size: channels_per_group

    // Load input row into shared memory using vectorized loads when possible
    if (in_features % 4 == 0 && tid * 4 < in_features) {
        const float4* x_vec = reinterpret_cast<const float4*>(&x[b * in_features]);
        float4* s_input_vec = reinterpret_cast<float4*>(s_input);
        for (int i = tid; i < in_features/4; i += blockDim.x) {
            s_input_vec[i] = x_vec[i];
        }
    } else {
        for (int i = tid; i < in_features; i += blockDim.x) {
            s_input[i] = x[b * in_features + i];
        }
    }
    __syncthreads();

    // Compute linear transformation
    for (int local_ch = tid; local_ch < channels_per_group; local_ch += blockDim.x) {
        int global_ch = c_start + local_ch;
        float sum = fc_bias[global_ch];
        for (int k = 0; k < in_features; ++k) {
            sum += s_input[k] * fc_weight[global_ch * in_features + k];
        }
        s_out[local_ch] = sum;
    }
    __syncthreads();

    // Compute mean and variance using warp-level reduction
    float local_sum = 0.0f;
    float local_sum_sq = 0.0f;
    for (int idx = tid; idx < channels_per_group; idx += blockDim.x) {
        float val = s_out[idx];
        local_sum += val;
        local_sum_sq += val * val;
    }

    local_sum = blockReduceSum(local_sum);
    local_sum_sq = blockReduceSum(local_sum_sq);

    if (tid == 0) {
        float mean = local_sum / channels_per_group;
        float var = local_sum_sq / channels_per_group - mean * mean;
        float inv_std = rsqrtf(var + eps);
        shared_mem[0] = mean;
        shared_mem[1] = inv_std;
    }
    __syncthreads();

    float mean = shared_mem[0];
    float inv_std = shared_mem[1];

    // Normalize, apply affine transformation, LeakyReLU, and sum
    for (int local_ch = tid; local_ch < channels_per_group; local_ch += blockDim.x) {
        float normalized = (s_out[local_ch] - mean) * inv_std;
        normalized = normalized * gn_weight[c_start + local_ch] + gn_bias[c_start + local_ch];
        normalized = fmaxf(normalized, normalized * negative_slope);
        output[b * out_features + c_start + local_ch] = normalized * 2.0f;
    }
}


torch::Tensor forward(
    torch::Tensor x,
    double eps,
    double negative_slope,
    torch::Tensor fc_weight,
    torch::Tensor fc_bias,
    torch::Tensor gn_weight,
    torch::Tensor gn_bias,
    int64_t num_groups) {

    TORCH_CHECK(x.is_cuda(), ""x must be a CUDA tensor"");

    int batch_size = x.size(0);
    int in_features = x.size(1);
    int out_features = fc_weight.size(0);
    int channels_per_group = out_features / num_groups;

    auto options = x.options();
    auto output = torch::empty({batch_size, out_features}, options);

    dim3 grid(batch_size, num_groups);
    int block_dim = channels_per_group < 128 ? channels_per_group : 128;

    size_t shared_mem_size = (in_features + channels_per_group) * sizeof(float);

    fused_kernel<<<grid, block_dim, shared_mem_size>>>(
        x.data_ptr<float>(),
        fc_weight.data_ptr<float>(),
        fc_bias.data_ptr<float>(),
        output.data_ptr<float>(),
        gn_weight.data_ptr<float>(),
        gn_bias.data_ptr<float>(),
        batch_size,
        in_features,
        out_features,
        num_groups,
        static_cast<float>(eps),
        static_cast<float>(negative_slope)
    );

    return output;
}

PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
    m.def(""forward"", &forward, ""Fused Matmul+GroupNorm+LeakyReLU+Sum with Warp-Optimized Reduction"");
}
",other,,,,,1,0,0.16183673,5224
"#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>
#include <vector>
#include <c10/util/Optional.h>

// Initialize output with bias using 2D block configuration
__global__ void initialize_output_kernel(
    float* __restrict__ output,
    const float* __restrict__ bias,
    const int batch,
    const int out_channels,
    const int out_h,
    const int out_w) {
    
    // Use 2D blocks for better spatial locality
    const int x = blockIdx.x * blockDim.x + threadIdx.x;
    const int y = blockIdx.y * blockDim.y + threadIdx.y;
    const int n = blockIdx.z;
    
    if (x >= out_w || y >= out_h || n >= batch) return;
    
    // Process all channels for this spatial location
    for (int oc = 0; oc < out_channels; oc++) {
        const int idx = n * (out_channels * out_h * out_w) +
                       oc * (out_h * out_w) +
                       y * out_w + x;
        output[idx] = bias[oc];
    }
}

// Scatter-based transposed convolution using 3D grid
__global__ void conv_transposed2d_scatter_atomic_kernel(
    const float* __restrict__ x,
    const float* __restrict__ weight,
    float* __restrict__ output,
    const int in_h,
    const int in_w,
    const int out_channels_per_group,
    const int kernel_h,
    const int kernel_w,
    const int stride_h,
    const int stride_w,
    const int pad_h,
    const int pad_w,
    const int dilation_h,
    const int dilation_w,
    const int groups,
    const int out_h,
    const int out_w,
    const int in_channels_per_group) {

    // 3D grid mapping: (spatial_x, channel, batch)
    const int spatial_idx = blockIdx.x * blockDim.x + threadIdx.x;
    const int c = blockIdx.y;
    const int n = blockIdx.z;
    
    if (c >= in_channels_per_group * groups) return;
    
    // Convert spatial_idx to ih, iw
    const int ih = spatial_idx / in_w;
    const int iw = spatial_idx % in_w;
    if (ih >= in_h) return;
    
    const int group = c / in_channels_per_group;
    const int c_within_group = c % in_channels_per_group;
    
    // Load input value once
    const int x_idx = n * (in_channels_per_group * groups * in_h * in_w) +
                     c * (in_h * in_w) +
                     ih * in_w + iw;
    const float x_val = x[x_idx];
    
    // Pre-compute base indices for weights with group offset
    const int weight_base = group * in_channels_per_group * (out_channels_per_group * kernel_h * kernel_w) + c_within_group * (out_channels_per_group * kernel_h * kernel_w);
    
    // Process output contributions
    #pragma unroll 4
    for (int kh = 0; kh < kernel_h; kh++) {
        const int oh = ih * stride_h - pad_h + kh * dilation_h;
        if (oh < 0 || oh >= out_h) continue;
        
        #pragma unroll 4
        for (int kw = 0; kw < kernel_w; kw++) {
            const int ow = iw * stride_w - pad_w + kw * dilation_w;
            if (ow < 0 || ow >= out_w) continue;
            
            const int khw_offset = kh * kernel_w + kw;
            
            // Process all output channels for this group
            #pragma unroll 4
            for (int oc_offset = 0; oc_offset < out_channels_per_group; oc_offset++) {
                const int weight_idx = weight_base +
                                     oc_offset * (kernel_h * kernel_w) +
                                     khw_offset;
                                     
                const float w_val = weight[weight_idx];
                const float contrib = x_val * w_val;
                
                if (contrib != 0.0f) {  // Skip atomic if contribution is zero
                    const int oc = group * out_channels_per_group + oc_offset;
                    const int out_idx = n * (groups * out_channels_per_group * out_h * out_w) +
                                      oc * (out_h * out_w) +
                                      oh * out_w + ow;
                    atomicAdd(&output[out_idx], contrib);
                }
            }
        }
    }
}

at::Tensor forward(
    at::Tensor x,
    at::Tensor weight,
    c10::optional<at::Tensor> bias,
    std::vector<int64_t> stride,
    std::vector<int64_t> padding,
    std::vector<int64_t> dilation,
    int groups) {
    
    x = x.contiguous();
    weight = weight.contiguous();
    
    if (!bias.has_value() || !bias.value().defined()) {
        bias = at::zeros({weight.size(1) * groups}, weight.options());
    } else {
        bias = bias.value().contiguous();
    }

    const int batch = x.size(0);
    const int in_channels = x.size(1);
    const int in_h = x.size(2);
    const int in_w = x.size(3);
    const int kernel_h = weight.size(2);
    const int kernel_w = weight.size(3);
    const int out_channels_per_group = weight.size(1);
    const int out_channels = out_channels_per_group * groups;
    
    const int stride_h = stride[0];
    const int stride_w = stride[1];
    const int pad_h = padding[0];
    const int pad_w = padding[1];
    const int dilation_h = dilation[0];
    const int dilation_w = dilation[1];
    
    const int out_h = (in_h - 1) * stride_h - 2 * pad_h + dilation_h * (kernel_h - 1) + 1;
    const int out_w = (in_w - 1) * stride_w - 2 * pad_w + dilation_w * (kernel_w - 1) + 1;

    auto output = at::empty({batch, out_channels, out_h, out_w}, x.options());

    // Initialize output with bias using 2D blocks
    dim3 threads_init(16, 16);
    dim3 blocks_init(
        (out_w + threads_init.x - 1) / threads_init.x,
        (out_h + threads_init.y - 1) / threads_init.y,
        batch
    );
    
    initialize_output_kernel<<<blocks_init, threads_init>>>(
        output.data_ptr<float>(),
        bias.value().data_ptr<float>(),
        batch,
        out_channels,
        out_h,
        out_w
    );

    // Launch scatter kernel with 3D grid
    const int THREADS_PER_BLOCK = 256;
    const int spatial_blocks = (in_h * in_w + THREADS_PER_BLOCK - 1) / THREADS_PER_BLOCK;
    
    dim3 grid(spatial_blocks, in_channels, batch);
    
    conv_transposed2d_scatter_atomic_kernel<<<grid, THREADS_PER_BLOCK>>>(
        x.data_ptr<float>(),
        weight.data_ptr<float>(),
        output.data_ptr<float>(),
        in_h,
        in_w,
        out_channels_per_group,
        kernel_h,
        kernel_w,
        stride_h,
        stride_w,
        pad_h,
        pad_w,
        dilation_h,
        dilation_w,
        groups,
        out_h,
        out_w,
        in_channels / groups
    );

    return output;
}

PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
    m.def(""forward"", &forward, ""2D Transposed Convolution (CUDA)"");
}",conv2d,,,,,1,0,0.16239497,6543
"#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>

#define TILE_DIM 32

template <typename T>
__global__ void linear_relu_kernel(
    const T* __restrict__ input, int in_dim,
    const T* __restrict__ weight, const T* __restrict__ bias,
    T* __restrict__ output, int out_dim, int batch_size) {
    
    extern __shared__ char shared_memory[];
    T* shared_input = (T*)shared_memory;
    T* shared_weight = (T*)(shared_memory + sizeof(T) * blockDim.x * TILE_DIM);
    
    const int tid = threadIdx.y * blockDim.x + threadIdx.x;
    const int row = blockIdx.x * blockDim.y + threadIdx.y;
    const int col = blockIdx.y * blockDim.x + threadIdx.x;
    
    T sum = 0;
    
    for (int t = 0; t < (in_dim + TILE_DIM - 1) / TILE_DIM; ++t) {
        if (row < batch_size && t * TILE_DIM + threadIdx.x < in_dim) {
            shared_input[tid] = __ldg(&input[row * in_dim + t * TILE_DIM + threadIdx.x]);
        } else {
            shared_input[tid] = 0;
        }
        
        if (col < out_dim && t * TILE_DIM + threadIdx.y < in_dim) {
            shared_weight[tid] = __ldg(&weight[col * in_dim + t * TILE_DIM + threadIdx.y]);
        } else {
            shared_weight[tid] = 0;
        }
        
        __syncthreads();
        
        #pragma unroll
        for (int k = 0; k < TILE_DIM; ++k) {
            sum += shared_input[threadIdx.y * TILE_DIM + k] * 
                   shared_weight[k * TILE_DIM + threadIdx.x];
        }
        
        __syncthreads();
    }
    
    if (row < batch_size && col < out_dim) {
        sum += __ldg(&bias[col]);
        output[row * out_dim + col] = sum > 0 ? sum : 0;
    }
}

template <typename T>
__global__ void linear_kernel(
    const T* __restrict__ input, int in_dim,
    const T* __restrict__ weight, const T* __restrict__ bias,
    T* __restrict__ output, int out_dim, int batch_size) {
    
    extern __shared__ char shared_memory[];
    T* shared_input = (T*)shared_memory;
    T* shared_weight = (T*)(shared_memory + sizeof(T) * blockDim.x * TILE_DIM);
    
    const int tid = threadIdx.y * blockDim.x + threadIdx.x;
    const int row = blockIdx.x * blockDim.y + threadIdx.y;
    const int col = blockIdx.y * blockDim.x + threadIdx.x;
    
    T sum = 0;
    
    for (int t = 0; t < (in_dim + TILE_DIM - 1) / TILE_DIM; ++t) {
        if (row < batch_size && t * TILE_DIM + threadIdx.x < in_dim) {
            shared_input[tid] = __ldg(&input[row * in_dim + t * TILE_DIM + threadIdx.x]);
        } else {
            shared_input[tid] = 0;
        }
        
        if (col < out_dim && t * TILE_DIM + threadIdx.y < in_dim) {
            shared_weight[tid] = __ldg(&weight[col * in_dim + t * TILE_DIM + threadIdx.y]);
        } else {
            shared_weight[tid] = 0;
        }
        
        __syncthreads();
        
        #pragma unroll
        for (int k = 0; k < TILE_DIM; ++k) {
            sum += shared_input[threadIdx.y * TILE_DIM + k] * 
                   shared_weight[k * TILE_DIM + threadIdx.x];
        }
        
        __syncthreads();
    }
    
    if (row < batch_size && col < out_dim) {
        sum += __ldg(&bias[col]);
        output[row * out_dim + col] = sum;
    }
}

torch::Tensor forward(
    torch::Tensor x,
    std::vector<torch::Tensor> weights,
    std::vector<torch::Tensor> biases) {
    
    TORCH_CHECK(weights.size() == biases.size(), ""Weights and biases count mismatch"");
    TORCH_CHECK(x.size(1) == weights[0].size(1), ""Input dimension mismatch"");
    
    torch::Tensor current_input = x;
    
    for(size_t i=0; i<weights.size()-1; i++) {
        auto weight = weights[i];
        auto bias = biases[i];
        int in_dim = weight.size(1);
        int out_dim = weight.size(0);
        int batch_size = current_input.size(0);
        
        auto output = torch::zeros({batch_size, out_dim}, 
            torch::device(torch::kCUDA).dtype(current_input.dtype()));
        
        dim3 block(TILE_DIM, TILE_DIM);
        dim3 grid((batch_size + TILE_DIM - 1)/TILE_DIM, 
                 (out_dim + TILE_DIM - 1)/TILE_DIM);
        
        int shared_mem_size = 2 * TILE_DIM * TILE_DIM * sizeof(float);
        
        if(current_input.dtype() == torch::kFloat32) {
            linear_relu_kernel<float><<<grid, block, shared_mem_size>>>(
                current_input.data_ptr<float>(),
                in_dim,
                weight.data_ptr<float>(),
                bias.data_ptr<float>(),
                output.data_ptr<float>(),
                out_dim,
                batch_size
            );
        } else {
            TORCH_CHECK(false, ""Unsupported dtype"");
        }
        
        current_input = output;
    }
    
    auto weight = weights.back();
    auto bias = biases.back();
    int in_dim = weight.size(1);
    int out_dim = weight.size(0);
    int batch_size = current_input.size(0);
    
    auto output = torch::zeros({batch_size, out_dim}, 
        torch::device(torch::kCUDA).dtype(current_input.dtype()));
    
    dim3 block(TILE_DIM, TILE_DIM);
    dim3 grid((batch_size + TILE_DIM - 1)/TILE_DIM, 
             (out_dim + TILE_DIM - 1)/TILE_DIM);
    
    int shared_mem_size = 2 * TILE_DIM * TILE_DIM * sizeof(float);
    
    if(current_input.dtype() == torch::kFloat32) {
        linear_kernel<float><<<grid, block, shared_mem_size>>>(
            current_input.data_ptr<float>(),
            in_dim,
            weight.data_ptr<float>(),
            bias.data_ptr<float>(),
            output.data_ptr<float>(),
            out_dim,
            batch_size
        );
    } else {
        TORCH_CHECK(false, ""Unsupported dtype"");
    }
    
    return output;
}

PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
    m.def(""forward"", &forward, ""MLP forward (CUDA)"");
}",other,,,,,1,0,0.16443107,5747
"#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>
#include <vector>
#include <cfloat>

// Constants in GPU constant memory
__constant__ float hswish_offset = 3.0f;
__constant__ float hswish_scale = 6.0f;
__constant__ float hswish_inv_scale = 1.0f/6.0f;

// Fused HardSwish+ReLU activation
__device__ inline float hardswish_relu(float x) {
    float activated = fminf(fmaxf(x + hswish_offset, 0.0f), hswish_scale);
    return fmaxf(x * activated * hswish_inv_scale, 0.0f);
}

__global__ void activation_kernel(float* input, float* output, int64_t size) {
    int index = blockIdx.x * blockDim.x + threadIdx.x;
    if (index < size) {
        output[index] = hardswish_relu(input[index]);
    }
}

// Parallel reduction for max value
__device__ float channel_max_reduce(float val, float* shared) {
    int tid = threadIdx.x;
    shared[tid] = val;
    __syncthreads();

    for (int stride = blockDim.x/2; stride > 0; stride >>= 1) {
        if (tid < stride && shared[tid + stride] > shared[tid])
            shared[tid] = shared[tid + stride];
        __syncthreads();
    }
    return shared[0];
}

// Parallel reduction for sum
__device__ float channel_sum_reduce(float val, float* shared) {
    int tid = threadIdx.x;
    shared[tid] = val;
    __syncthreads();

    for (int stride = blockDim.x/2; stride > 0; stride >>= 1) {
        if (tid < stride)
            shared[tid] += shared[tid + stride];
        __syncthreads();
    }
    return shared[0];
}

__global__ void optimized_softmax_kernel(float* input, float* output,
                                        int batch_size, int channels, int spatial_size) {
    extern __shared__ float shared[];
    int spatial_idx = blockIdx.x % spatial_size;
    int batch_idx = blockIdx.x / spatial_size;
    int channel_idx = threadIdx.x;

    if (batch_idx >= batch_size || channel_idx >= channels) return;

    int input_idx = batch_idx * channels * spatial_size +
                  channel_idx * spatial_size + spatial_idx;

    // Load value and find max
    float val = input[input_idx];
    float max_val = channel_max_reduce(val, shared);

    // Compute exp normalized by max
    float exp_val = expf(val - max_val);
    float sum_exp = channel_sum_reduce(exp_val, shared);

    // Write normalized value
    output[input_idx] = exp_val / sum_exp;
}

torch::Tensor module_forward(
    torch::Tensor x,
    torch::Tensor conv_weight,
    torch::Tensor conv_bias)
{
    x = x.contiguous().cuda();
    conv_weight = conv_weight.contiguous().cuda();
    conv_bias = conv_bias.contiguous().cuda();

    x = torch::conv3d(x, conv_weight, conv_bias);

    // Activation phase
    int64_t total_elements = x.numel();
    torch::Tensor activated = torch::empty_like(x);
    const int threads = 256;
    int blocks = (total_elements + threads - 1) / threads;
    activation_kernel<<<blocks, threads>>>(x.data_ptr<float>(), activated.data_ptr<float>(), total_elements);

    // Softmax phase
    auto sizes = x.sizes();
    int batch_size = sizes[0];
    int channels = sizes[1];
    int spatial_size = sizes[2] * sizes[3] * sizes[4];
    torch::Tensor softmax_out = torch::empty_like(activated);

    dim3 softmax_blocks(batch_size * spatial_size);
    optimized_softmax_kernel<<<softmax_blocks, channels, channels*sizeof(float)>>>(
        activated.data_ptr<float>(),
        softmax_out.data_ptr<float>(),
        batch_size,
        channels,
        spatial_size
    );

    // Mean reduction
    return softmax_out.view(sizes).mean({2, 3, 4});
}

PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
    m.def(""forward"", &module_forward, ""Optimized fused forward"");
}",conv3d,,,,,1,0,0.16908845,3635
"#include <torch/extension.h>
#include <ATen/cuda/CUDAContext.h>
#include <vector>

__global__ void module_fn_forward_kernel(
    const float* __restrict__ x,
    const float* __restrict__ weight,
    const float* __restrict__ bias,
    float* __restrict__ out,
    const int batch_size,
    const int in_features,
    const int out_features,
    const float scaling_factor)
{
    const unsigned int warp_size = 32;
    const unsigned int lane_id = threadIdx.x % warp_size;
    const unsigned int warp_id = threadIdx.x / warp_size;
    
    int row = blockIdx.x;
    int col = blockIdx.y * blockDim.y + threadIdx.y;
    
    if (row < batch_size && col < out_features) {
        float val = 0.0f;
        
        // Each warp handles a portion of the reduction
        for (int k = lane_id; k < in_features; k += warp_size) {
            val += x[row * in_features + k] * weight[col * in_features + k];
        }
        
        // Warp-level reduction using shuffle operations
        #pragma unroll
        for (int offset = warp_size/2; offset > 0; offset /= 2) {
            val += __shfl_down_sync(0xffffffff, val, offset);
        }
        
        // First thread in warp has final sum
        if (lane_id == 0) {
            val += bias[col];
            float original_val = val;
            val *= scaling_factor;
            val += original_val;
            out[row * out_features + col] = val;
        }
    }
}

torch::Tensor forward(
    torch::Tensor x,
    const float scaling_factor,
    torch::Tensor weight,
    torch::Tensor bias)
{
    TORCH_CHECK(x.is_cuda(), ""x must be a CUDA tensor"");
    TORCH_CHECK(weight.is_cuda(), ""weight must be a CUDA tensor"");
    TORCH_CHECK(bias.is_cuda(), ""bias must be a CUDA tensor"");
    
    auto x_ = x.contiguous();
    auto w_ = weight.contiguous();
    auto b_ = bias.contiguous();

    const int batch_size = x_.size(0);
    const int in_features = x_.size(1);
    const int out_features = w_.size(0);

    auto out = torch::empty({batch_size, out_features}, x_.options());

    // Configure block and grid sizes optimized for warp operations
    dim3 block(32, 16); // 32 threads per warp
    dim3 grid(batch_size, (out_features + block.y - 1) / block.y);

    module_fn_forward_kernel<<<grid, block>>>(
        x_.data_ptr<float>(),
        w_.data_ptr<float>(),
        b_.data_ptr<float>(),
        out.data_ptr<float>(),
        batch_size,
        in_features,
        out_features,
        scaling_factor
    );

    auto err = cudaGetLastError();
    if (err != cudaSuccess) {
        throw std::runtime_error(cudaGetErrorString(err));
    }

    return out;
}

PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
    m.def(""forward"", &forward, ""module_fn forward (CUDA)"");
}",other,,,,,1,0,0.17073761,2738
"#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>
#include <cstdio>

__global__ void conv2d_kernel(
    const float* __restrict__ input,
    const float* __restrict__ weight,
    const float* __restrict__ bias,
    float* __restrict__ output,
    int batch_size,
    int in_channels,
    int input_height,
    int input_width,
    int out_channels,
    int kernel_h,
    int kernel_w,
    int output_height,
    int output_width) {

  const int idx = blockIdx.x * blockDim.x + threadIdx.x;
  const int stride = blockDim.x * gridDim.x;
  const int total = batch_size * out_channels * output_height * output_width;

  for (int index = idx; index < total; index += stride) {
    const int w_out = index % output_width;
    int temp = index / output_width;
    const int h_out = temp % output_height;
    temp /= output_height;
    const int oc = temp % out_channels;
    const int n = temp / out_channels;

    float sum = bias[oc];

    #pragma unroll 4
    for (int ic = 0; ic < in_channels; ++ic) {
      const int input_base = (n * in_channels + ic) * input_height * input_width;
      const int weight_base = (oc * in_channels + ic) * kernel_h * kernel_w;

      #pragma unroll 4
      for (int i = 0; i < kernel_h; ++i) {
        const int in_h = h_out + i;
        const int input_offset = (input_base + in_h * input_width) + w_out;
        const int weight_row = weight_base + i * kernel_w;

        #pragma unroll 4
        for (int j = 0; j < kernel_w; ++j) {
          sum += input[input_offset + j] * weight[weight_row + j];
        }
      }
    }
    output[index] = sum;
  }
}

__global__ void instance_norm_kernel(
    float* __restrict__ x,
    const float* __restrict__ scale,
    const float* __restrict__ shift,
    float divide_by,
    int batch_size,
    int channels,
    int height,
    int width) {

  const int n = blockIdx.x;
  const int c = blockIdx.y;
  const int tid = threadIdx.x;
  const int num_pixels = height * width;

  extern __shared__ float shared[];
  float* s_sum = shared;
  float* s_sum_sq = shared + blockDim.x;

  float sum = 0, sum_sq = 0;
  for (int i = tid; i < num_pixels; i += blockDim.x) {
    const int idx = ((n * channels + c) * height * width) + i;
    float val = x[idx];
    sum += val;
    sum_sq += val * val;
  }

  s_sum[tid] = sum;
  s_sum_sq[tid] = sum_sq;
  __syncthreads();

  #pragma unroll
  for (int s = blockDim.x/2; s > 0; s >>= 1) {
    if (tid < s) {
      s_sum[tid] += s_sum[tid + s];
      s_sum_sq[tid] += s_sum_sq[tid + s];
    }
    __syncthreads();
  }

  const float mean = s_sum[0] / num_pixels;
  const float inv_std = rsqrtf(s_sum_sq[0]/num_pixels - mean*mean + 1e-5f);

  #pragma unroll 4
  for (int i = tid; i < num_pixels; i += blockDim.x) {
    const int idx = ((n * channels + c) * height * width) + i;
    float val = (x[idx] - mean) * inv_std;
    x[idx] = (scale[c] * val + shift[c]) / divide_by;
  }
}

torch::Tensor forward_cuda(
    torch::Tensor input,
    torch::Tensor conv_weight,
    torch::Tensor conv_bias,
    c10::optional<torch::Tensor> instance_norm_weight_opt,
    c10::optional<torch::Tensor> instance_norm_bias_opt,
    float divide_by) {

  input = input.contiguous();
  conv_weight = conv_weight.contiguous();
  conv_bias = conv_bias.contiguous();

  int batch_size = input.size(0);
  int in_channels = input.size(1);
  int input_height = input.size(2);
  int input_width = input.size(3);
  int out_channels = conv_weight.size(0);
  int kernel_h = conv_weight.size(2);
  int kernel_w = conv_weight.size(3);
  int output_height = input_height - kernel_h + 1;
  int output_width = input_width - kernel_w + 1;

  auto output = torch::empty({batch_size, out_channels, output_height, output_width}, input.options());

  // Launch conv kernel
  int threads = 256;
  int blocks = (batch_size * out_channels * output_height * output_width + threads - 1) / threads;
  conv2d_kernel<<<blocks, threads>>>(
      input.data_ptr<float>(),
      conv_weight.data_ptr<float>(),
      conv_bias.data_ptr<float>(),
      output.data_ptr<float>(),
      batch_size, in_channels,
      input_height, input_width,
      out_channels, kernel_h, kernel_w,
      output_height, output_width);

  // Handle instance norm params
  auto instance_norm_weight = instance_norm_weight_opt.has_value() ? 
      instance_norm_weight_opt->contiguous() : torch::ones({out_channels}, output.options());
  auto instance_norm_bias = instance_norm_bias_opt.has_value() ?
      instance_norm_bias_opt->contiguous() : torch::zeros({out_channels}, output.options());

  // Launch instance norm kernel
  dim3 grid(batch_size, out_channels);
  int shmem = 2 * 256 * sizeof(float);
  instance_norm_kernel<<<grid, 256, shmem>>>(
      output.data_ptr<float>(),
      instance_norm_weight.data_ptr<float>(),
      instance_norm_bias.data_ptr<float>(),
      divide_by,
      batch_size, out_channels,
      output_height, output_width);

  return output;
}

PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
  m.def(""forward"", &forward_cuda, ""Conv2D + InstanceNorm + Divide (CUDA)"");
}
",conv2d,,,,,1,0,0.17336684,5063
"#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>

__global__ void gemm_kernel(
    const float* input,
    const float* weight,
    const float* bias,
    float* output,
    const int M,
    const int N,
    const int K
) {
    const int row = blockIdx.y * blockDim.y + threadIdx.y;
    const int col = blockIdx.x * blockDim.x + threadIdx.x;
    
    if (row < M && col < N) {
        float sum = 0.0f;
        for (int k = 0; k < K; ++k) {
            sum += input[row * K + k] * weight[k * N + col];
        }
        output[row * N + col] = sum + bias[col];
    }
}

__global__ void batchnorm_scale_kernel(
    float* x,
    const float* mean,
    const float* var,
    const float* weight,
    const float* bias,
    const float* scale,
    const float eps,
    const int N,
    const int D
) {
    const int tid = blockIdx.x * blockDim.x + threadIdx.x;
    const int stride = blockDim.x * gridDim.x;
    
    for (int i = tid; i < N * D; i += stride) {
        const int row = i / D;
        const int col = i % D;
        
        float val = x[i];
        val = (val - mean[col]) / sqrt(var[col] + eps);
        if (weight != nullptr && bias != nullptr) {
            val = val * weight[col] + bias[col];
        }
        val = val * scale[col];
        x[i] = val;
    }
}

__global__ void softmax_kernel(
    float* x,
    const int N,
    const int D
) {
    const int row = blockIdx.x * blockDim.x + threadIdx.x;
    
    if (row < N) {
        float max_val = -INFINITY;
        for (int i = 0; i < D; ++i) {
            max_val = max(max_val, x[row * D + i]);
        }
        
        float sum = 0.0f;
        for (int i = 0; i < D; ++i) {
            float val = exp(x[row * D + i] - max_val);
            x[row * D + i] = val;
            sum += val;
        }
        
        for (int i = 0; i < D; ++i) {
            x[row * D + i] /= sum;
        }
    }
}

torch::Tensor forward(
    torch::Tensor x,
    torch::Tensor running_mean,
    torch::Tensor running_var,
    double bn_eps,
    double bn_momentum,
    torch::Tensor weight,
    torch::Tensor bias,
    torch::Tensor scale,
    torch::Tensor gemm_weight,
    torch::Tensor gemm_bias
) {
    if (!x.is_cuda()) throw std::runtime_error(""Input must be a CUDA tensor"");
    
    const int batch_size = x.size(0);
    const int in_features = x.size(1);
    const int out_features = gemm_weight.size(0);
    
    auto options = torch::TensorOptions().dtype(x.dtype()).device(x.device());
    auto output = torch::empty({batch_size, out_features}, options);
    
    dim3 gemm_block(16, 16);
    dim3 gemm_grid(
        (out_features + gemm_block.x - 1) / gemm_block.x,
        (batch_size + gemm_block.y - 1) / gemm_block.y
    );
    
    gemm_kernel<<<gemm_grid, gemm_block>>>(
        x.data_ptr<float>(),
        gemm_weight.data_ptr<float>(),
        gemm_bias.data_ptr<float>(),
        output.data_ptr<float>(),
        batch_size,
        out_features,
        in_features
    );
    
    const int threads = 256;
    const int blocks = (batch_size * out_features + threads - 1) / threads;
    
    batchnorm_scale_kernel<<<blocks, threads>>>(
        output.data_ptr<float>(),
        running_mean.data_ptr<float>(),
        running_var.data_ptr<float>(),
        weight.defined() ? weight.data_ptr<float>() : nullptr,
        bias.defined() ? bias.data_ptr<float>() : nullptr,
        scale.data_ptr<float>(),
        bn_eps,
        batch_size,
        out_features
    );
    
    const int softmax_threads = 256;
    const int softmax_blocks = (batch_size + softmax_threads - 1) / softmax_threads;
    
    softmax_kernel<<<softmax_blocks, softmax_threads>>>(
        output.data_ptr<float>(),
        batch_size,
        out_features
    );
    
    return output;
}

PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
    m.def(""forward"", &forward, ""Custom CUDA forward function"");
}",gemm,,,,,1,0,0.17909671,3890
"#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>

#define BLOCK_SIZE 256

template <typename scalar_t>
__global__ void masked_cumsum_kernel(
    const scalar_t* __restrict__ x,
    const bool* __restrict__ mask,
    scalar_t* __restrict__ output,
    int64_t N,
    int64_t L) {

    extern __shared__ char shared_mem[];
    scalar_t* s_data = (scalar_t*)shared_mem;
    bool* s_mask = (bool*)(s_data + L);

    const int row = blockIdx.x;
    const int tid = threadIdx.x;
    
    if (row >= N) return;

    const scalar_t* x_row = x + row * L;
    const bool* mask_row = mask + row * L;
    scalar_t* output_row = output + row * L;

    // Cooperatively load data into shared memory
    for (int i = tid; i < L; i += BLOCK_SIZE) {
        s_data[i] = x_row[i];
        s_mask[i] = mask_row[i];
    }
    __syncthreads();

    // Compute cumulative sum
    scalar_t sum = 0;
    for (int i = 0; i < L; i++) {
        if (s_mask[i]) {
            sum += s_data[i];
        }
        if (tid == 0) {
            output_row[i] = sum;
        }
    }
}

#define CHECK_CUDA(x) TORCH_CHECK(x.is_cuda(), #x "" must be a CUDA tensor"")
#define CHECK_CONTIGUOUS(x) TORCH_CHECK(x.is_contiguous(), #x "" must be contiguous"")
#define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)

torch::Tensor masked_cumsum(
    const torch::Tensor& x,
    const torch::Tensor& mask,
    int64_t dim) {

    CHECK_INPUT(x);
    CHECK_INPUT(mask);
    TORCH_CHECK(x.sizes() == mask.sizes(), ""x and mask must have the same shape"");
    TORCH_CHECK(mask.scalar_type() == torch::kBool, ""mask must be a boolean tensor"");

    if (dim < 0) {
        dim += x.dim();
    }
    TORCH_CHECK(dim >= 0 && dim < x.dim(), ""Invalid dimension"");

    std::vector<int64_t> perm;
    for (int64_t i = 0; i < x.dim(); ++i) {
        if (i != dim)
            perm.push_back(i);
    }
    perm.push_back(dim);

    auto x_permuted = x.permute(perm).contiguous();
    auto mask_permuted = mask.permute(perm).contiguous();

    int64_t N = x_permuted.numel() / x_permuted.size(-1);
    int64_t L = x_permuted.size(-1);

    auto x_flat = x_permuted.view({N, L});
    auto mask_flat = mask_permuted.view({N, L});
    auto output_flat = torch::empty_like(x_flat);

    const int threads = BLOCK_SIZE;
    const int blocks = N;
    const size_t shared_mem_size = L * (x.element_size() + sizeof(bool));

    AT_DISPATCH_FLOATING_TYPES(x.scalar_type(), ""masked_cumsum_cuda"", ([&] {
        masked_cumsum_kernel<scalar_t><<<blocks, threads, shared_mem_size>>>(
            x_flat.data_ptr<scalar_t>(),
            mask_flat.data_ptr<bool>(),
            output_flat.data_ptr<scalar_t>(),
            N,
            L
        );
    }));

    auto output_permuted = output_flat.view(x_permuted.sizes());
    std::vector<int64_t> inv_perm(perm.size());
    for (size_t i = 0; i < perm.size(); ++i) {
        inv_perm[perm[i]] = i;
    }
    auto output = output_permuted.permute(inv_perm);

    return output;
}

PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
    m.def(""forward"", &masked_cumsum, ""Masked Cumulative Sum (CUDA)"");
}",other,,,,,1,0,0.18004827,3097
"#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>
#include <math.h>

// Optimized kernel for Triplet Margin Loss using __ldg() for read-only load and 128-bit aligned vectorized loads (float4).
// Each block handles one batch element and uses shared memory reduction for performance.

__global__ void triplet_margin_loss_kernel_optimized(
    const float* __restrict__ anchor,
    const float* __restrict__ positive,
    const float* __restrict__ negative,
    float* __restrict__ output,
    const float margin,
    const int batch_size,
    const int feat_size) {

    int batch_idx = blockIdx.x;
    if (batch_idx >= batch_size) return;

    // Allocate shared memory for reduction: two arrays for positive and negative sums
    extern __shared__ float shared_mem[];
    float* sh_sum_pos = shared_mem;
    float* sh_sum_neg = shared_mem + blockDim.x;

    float sum_pos = 0.0f;
    float sum_neg = 0.0f;
    int offset = batch_idx * feat_size;

    // Use vectorized loads if possible by processing 4 floats (128-bit) at a time
    int vectorized_end = (feat_size / 4) * 4;
    const float4* anchor_vec = reinterpret_cast<const float4*>(anchor + offset);
    const float4* positive_vec = reinterpret_cast<const float4*>(positive + offset);
    const float4* negative_vec = reinterpret_cast<const float4*>(negative + offset);
    int num_vec = vectorized_end / 4;

    for (int i = threadIdx.x; i < num_vec; i += blockDim.x) {
        float4 a4 = __ldg(&anchor_vec[i]);
        float4 p4 = __ldg(&positive_vec[i]);
        float4 n4 = __ldg(&negative_vec[i]);

        // Compute squared differences for anchor-positive
        float d0 = a4.x - p4.x;
        float d1 = a4.y - p4.y;
        float d2 = a4.z - p4.z;
        float d3 = a4.w - p4.w;
        sum_pos += d0 * d0 + d1 * d1 + d2 * d2 + d3 * d3;

        // Compute squared differences for anchor-negative
        d0 = a4.x - n4.x;
        d1 = a4.y - n4.y;
        d2 = a4.z - n4.z;
        d3 = a4.w - n4.w;
        sum_neg += d0 * d0 + d1 * d1 + d2 * d2 + d3 * d3;
    }

    // Process any remaining elements that don't fit into a float4
    for (int i = vectorized_end + threadIdx.x; i < feat_size; i += blockDim.x) {
        float a = __ldg(anchor + offset + i);
        float p = __ldg(positive + offset + i);
        float n = __ldg(negative + offset + i);
        float d_pos = a - p;
        float d_neg = a - n;
        sum_pos += d_pos * d_pos;
        sum_neg += d_neg * d_neg;
    }

    // Store partial sums in shared memory
    sh_sum_pos[threadIdx.x] = sum_pos;
    sh_sum_neg[threadIdx.x] = sum_neg;
    __syncthreads();

    // Perform reduction within the block to get total sum for positive and negative distances
    for (unsigned int s = blockDim.x / 2; s > 0; s >>= 1) {
        if (threadIdx.x < s) {
            sh_sum_pos[threadIdx.x] += sh_sum_pos[threadIdx.x + s];
            sh_sum_neg[threadIdx.x] += sh_sum_neg[threadIdx.x + s];
        }
        __syncthreads();
    }

    if (threadIdx.x == 0) {
        float total_pos = sh_sum_pos[0];
        float total_neg = sh_sum_neg[0];
        // Compute final loss: max(0, sqrt(total_pos) - sqrt(total_neg) + margin)
        float loss = sqrtf(total_pos) - sqrtf(total_neg) + margin;
        output[batch_idx] = (loss > 0.0f) ? loss : 0.0f;
    }
}

// CUDA launcher for the optimized kernel
torch::Tensor triplet_margin_loss_cuda_optimized(
    torch::Tensor anchor,
    torch::Tensor positive,
    torch::Tensor negative,
    float margin) {

    TORCH_CHECK(anchor.device().is_cuda(), ""anchor must be a CUDA tensor"");
    TORCH_CHECK(positive.device().is_cuda(), ""positive must be a CUDA tensor"");
    TORCH_CHECK(negative.device().is_cuda(), ""negative must be a CUDA tensor"");

    const int batch_size = anchor.size(0);
    const int feat_size = anchor.size(1);
    auto output = torch::empty({batch_size}, anchor.options());

    int threads = 256;
    // Allocate shared memory: 2 arrays of 'threads' floats (for positive and negative sums)
    int shared_mem_size = 2 * threads * sizeof(float);
    
    // Launch one block per batch element
    triplet_margin_loss_kernel_optimized<<<batch_size, threads, shared_mem_size>>>(
        anchor.data_ptr<float>(),
        positive.data_ptr<float>(),
        negative.data_ptr<float>(),
        output.data_ptr<float>(),
        margin,
        batch_size,
        feat_size);
    
    // Return the mean loss over the batch
    return output.mean();
}

PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
    m.def(""forward"", &triplet_margin_loss_cuda_optimized, ""Triplet margin loss forward optimized (CUDA)"");
}
",other,,,,,1,0,0.18178354,4624
"#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>

#define BLOCK_SIZE 32
#define CHUNK_SIZE 8

// Constant memory for frequently accessed parameters
__constant__ int d_N;
__constant__ int d_num_chunks;
__constant__ int d_chunk_sizes[256];  // For storing chunk size information

__global__ void triangular_mm_kernel(const float* __restrict__ A,
                                   const float* __restrict__ B,
                                   float* __restrict__ C) {
    __shared__ float s_A[BLOCK_SIZE][BLOCK_SIZE];
    __shared__ float s_B[BLOCK_SIZE][BLOCK_SIZE];
    
    const int row = blockIdx.y * BLOCK_SIZE + threadIdx.y;
    const int col = blockIdx.x * BLOCK_SIZE + threadIdx.x;
    
    // Register array for accumulation
    float reg_C[CHUNK_SIZE] = {0.0f};
    
    // Calculate number of iterations based on constant memory value
    const int num_iterations = (row / BLOCK_SIZE) + 1;
    
    // Loop over block-level tiles
    for (int bk = 0; bk < num_iterations; bk++) {
        const int block_start = bk * BLOCK_SIZE;
        
        // Collaborative loading with vectorized memory access
        if (row < d_N && (block_start + threadIdx.x) < d_N) {
            s_A[threadIdx.y][threadIdx.x] = A[row * d_N + block_start + threadIdx.x];
        } else {
            s_A[threadIdx.y][threadIdx.x] = 0.0f;
        }
        
        if ((block_start + threadIdx.y) < d_N && col < d_N) {
            s_B[threadIdx.y][threadIdx.x] = B[(block_start + threadIdx.y) * d_N + col];
        } else {
            s_B[threadIdx.y][threadIdx.x] = 0.0f;
        }
        
        __syncthreads();
        
        // Register-level tiling for computation
        if (row < d_N && col < d_N && row >= col) {
            const int k_start = max(block_start, col);
            const int k_end = min(block_start + BLOCK_SIZE, row + 1);
            
            #pragma unroll
            for (int k = k_start; k < k_end; k += CHUNK_SIZE) {
                #pragma unroll
                for (int c = 0; c < CHUNK_SIZE && (k + c) < k_end; c++) {
                    reg_C[c] += s_A[threadIdx.y][k - block_start + c] * 
                               s_B[k - block_start + c][threadIdx.x];
                }
            }
        }
        
        __syncthreads();
    }
    
    // Reduction and writing results
    if (row < d_N && col < d_N) {
        if (row >= col) {
            float sum = 0.0f;
            #pragma unroll
            for (int i = 0; i < CHUNK_SIZE; i++) {
                sum += reg_C[i];
            }
            C[row * d_N + col] = sum;
        } else {
            C[row * d_N + col] = 0.0f;
        }
    }
}

at::Tensor forward(at::Tensor A, at::Tensor B) {
    TORCH_CHECK(A.is_cuda(), ""A must be a CUDA tensor"");
    TORCH_CHECK(B.is_cuda(), ""B must be a CUDA tensor"");
    TORCH_CHECK(A.dim() == 2, ""A must be a 2D tensor"");
    TORCH_CHECK(B.dim() == 2, ""B must be a 2D tensor"");
    TORCH_CHECK(A.size(0) == A.size(1), ""A must be square"");
    TORCH_CHECK(B.size(0) == B.size(1), ""B must be square"");
    TORCH_CHECK(A.size(0) == B.size(0), ""A and B must be the same size"");

    const int N = A.size(0);
    auto C = torch::empty_like(A);

    // Copy constants to constant memory
    cudaMemcpyToSymbol(d_N, &N, sizeof(int));
    
    // Calculate and store chunk sizes in constant memory
    int num_chunks = (N + BLOCK_SIZE - 1) / BLOCK_SIZE;
    cudaMemcpyToSymbol(d_num_chunks, &num_chunks, sizeof(int));
    
    int chunk_sizes[256];  // Assuming max 256 chunks
    for (int i = 0; i < num_chunks; i++) {
        chunk_sizes[i] = min(BLOCK_SIZE, N - i * BLOCK_SIZE);
    }
    cudaMemcpyToSymbol(d_chunk_sizes, chunk_sizes, num_chunks * sizeof(int));

    dim3 threadsPerBlock(BLOCK_SIZE, BLOCK_SIZE);
    dim3 numBlocks((N + BLOCK_SIZE - 1) / BLOCK_SIZE,
                   (N + BLOCK_SIZE - 1) / BLOCK_SIZE);

    triangular_mm_kernel<<<numBlocks, threadsPerBlock>>>(
        A.data_ptr<float>(),
        B.data_ptr<float>(),
        C.data_ptr<float>()
    );

    return C;
}

PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
    m.def(""forward"", &forward, ""Triangular matrix multiplication (CUDA)"");
}",other,,,,,1,0,0.18448474,4175
"#include <torch/extension.h>
#include <ATen/ATen.h>
#include <vector>
#include <cuda_runtime.h>

#define BLOCK_SIZE 128
#define TILE_SIZE 16
#define VECTOR_SIZE 4  // For vectorized loads

__device__ __forceinline__ float mish(float x) {
    return x * tanh(logf(1.0f + expf(x)));
}

__global__ void matmul_scale_residual_kernel(
    const float* __restrict__ input,
    const float* __restrict__ weight,
    const float* __restrict__ bias,
    float* __restrict__ output,
    const int batch_size,
    const int input_size,
    const int hidden_size,
    const float scale_factor,
    const float clamp_min,
    const float clamp_max
) {
    __shared__ float weight_shared[TILE_SIZE][TILE_SIZE];
    __shared__ float input_shared[TILE_SIZE][TILE_SIZE];
    
    const int tx = threadIdx.x;
    const int ty = threadIdx.y;
    const int bx = blockIdx.x;
    const int by = blockIdx.y;
    
    const int row = by * TILE_SIZE + ty;
    const int col = bx * TILE_SIZE + tx;
    
    float sum = 0.0f;
    
    // Main matrix multiplication loop
    #pragma unroll 4
    for (int t = 0; t < (input_size + TILE_SIZE - 1) / TILE_SIZE; ++t) {
        // Load input tile with vectorized loads when possible
        if (row < batch_size && (t * TILE_SIZE + tx) < input_size) {
            if ((tx & 3) == 0 && (t * TILE_SIZE + tx + 3) < input_size) {
                float4 tmp = *reinterpret_cast<const float4*>(&input[row * input_size + t * TILE_SIZE + tx]);
                input_shared[ty][tx] = tmp.x;
                input_shared[ty][tx + 1] = tmp.y;
                input_shared[ty][tx + 2] = tmp.z;
                input_shared[ty][tx + 3] = tmp.w;
            } else {
                input_shared[ty][tx] = input[row * input_size + t * TILE_SIZE + tx];
            }
        } else {
            input_shared[ty][tx] = 0.0f;
        }
        
        // Load weight tile with vectorized loads when possible
        if (col < hidden_size && (t * TILE_SIZE + ty) < input_size) {
            if ((ty & 3) == 0 && (t * TILE_SIZE + ty + 3) < input_size) {
                float4 tmp = *reinterpret_cast<const float4*>(&weight[col * input_size + t * TILE_SIZE + ty]);
                weight_shared[ty][tx] = tmp.x;
                weight_shared[ty + 1][tx] = tmp.y;
                weight_shared[ty + 2][tx] = tmp.z;
                weight_shared[ty + 3][tx] = tmp.w;
            } else {
                weight_shared[ty][tx] = weight[col * input_size + t * TILE_SIZE + ty];
            }
        } else {
            weight_shared[ty][tx] = 0.0f;
        }
        
        __syncthreads();
        
        // Manually unrolled inner product computation
        #pragma unroll
        for (int k = 0; k < TILE_SIZE; k += 4) {
            sum += input_shared[ty][k] * weight_shared[k][tx];
            sum += input_shared[ty][k + 1] * weight_shared[k + 1][tx];
            sum += input_shared[ty][k + 2] * weight_shared[k + 2][tx];
            sum += input_shared[ty][k + 3] * weight_shared[k + 3][tx];
        }
        
        __syncthreads();
    }
    
    if (row < batch_size && col < hidden_size) {
        // Add bias and apply transformations
        sum += bias[col];
        sum *= scale_factor;
        sum += sum;
        sum = fmaxf(fminf(sum, clamp_max), clamp_min);
        
        output[row * hidden_size + col] = sum;
    }
}

torch::Tensor module_fn_forward(
    torch::Tensor x,
    float scale_factor,
    float clamp_min,
    float clamp_max,
    torch::Tensor weight,
    torch::Tensor bias
) {
    const int batch_size = x.size(0);
    const int input_size = x.size(1);
    const int hidden_size = weight.size(0);
    
    auto options = torch::TensorOptions()
        .dtype(x.dtype())
        .device(x.device());
    
    auto output = torch::empty({batch_size, hidden_size}, options);
    
    dim3 block(TILE_SIZE, TILE_SIZE);
    dim3 grid(
        (hidden_size + TILE_SIZE - 1) / TILE_SIZE,
        (batch_size + TILE_SIZE - 1) / TILE_SIZE
    );
    
    matmul_scale_residual_kernel<<<grid, block>>>(
        x.data_ptr<float>(),
        weight.data_ptr<float>(),
        bias.data_ptr<float>(),
        output.data_ptr<float>(),
        batch_size,
        input_size,
        hidden_size,
        scale_factor,
        clamp_min,
        clamp_max
    );
    
    output = output.logsumexp(1, true);
    auto mish_val = output * torch::tanh(torch::softplus(output));
    output.mul_(mish_val);
    
    return output;
}

PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
    m.def(""forward"", &module_fn_forward, ""Forward pass for module_fn (CUDA)"");
}",other,,,,,1,0,0.18682344,4581
"#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>
#include <vector>
#include <algorithm>

// Define tile sizes for shared memory
#define TILE_WIDTH 16
#define TILE_HEIGHT 16

__global__ void shared_conv2d_relu_bias_kernel(
    const float* __restrict__ x,
    const float* __restrict__ conv_weight,
    const float* __restrict__ conv_bias,
    const float* __restrict__ bias,
    float* __restrict__ out,
    const int N,
    const int C_in,
    const int H_in,
    const int W_in,
    const int C_out,
    const int K_h,
    const int K_w,
    const int H_out,
    const int W_out) {
    
    __shared__ float shared_input[TILE_HEIGHT + 2][TILE_WIDTH + 2];
    __shared__ float shared_weight[TILE_WIDTH][TILE_WIDTH];

    // Calculate output position
    const int tx = threadIdx.x;
    const int ty = threadIdx.y;
    const int bx = blockIdx.x;
    const int by = blockIdx.y;
    const int bz = blockIdx.z;

    const int h_out_start = by * TILE_HEIGHT;
    const int w_out_start = bx * TILE_WIDTH;
    const int n = bz / C_out;
    const int co = bz % C_out;

    // Pre-compute bias value
    float val = conv_bias[co];

    // Loop over input channels
    for (int ci = 0; ci < C_in; ci++) {
        // Load input tile into shared memory with padding for convolution
        for (int i = ty; i < TILE_HEIGHT + K_h - 1; i += blockDim.y) {
            for (int j = tx; j < TILE_WIDTH + K_w - 1; j += blockDim.x) {
                const int h_in = h_out_start + i;
                const int w_in = w_out_start + j;
                if (h_in < H_in && w_in < W_in) {
                    shared_input[i][j] = x[((n * C_in + ci) * H_in + h_in) * W_in + w_in];
                } else {
                    shared_input[i][j] = 0.0f;
                }
            }
        }

        // Load weight tile into shared memory
        for (int i = ty; i < K_h; i += blockDim.y) {
            for (int j = tx; j < K_w; j += blockDim.x) {
                shared_weight[i][j] = conv_weight[(((co * C_in) + ci) * K_h + i) * K_w + j];
            }
        }

        __syncthreads();

        // Compute convolution for this thread's output pixel
        const int h_out = h_out_start + ty;
        const int w_out = w_out_start + tx;
        
        if (h_out < H_out && w_out < W_out) {
            for (int kh = 0; kh < K_h; kh++) {
                for (int kw = 0; kw < K_w; kw++) {
                    val += shared_input[ty + kh][tx + kw] * shared_weight[kh][kw];
                }
            }
        }

        __syncthreads();
    }

    // Apply ReLU and bias, then write output
    if (ty < H_out && tx < W_out) {
        const int out_idx = ((n * C_out + co) * H_out + h_out_start + ty) * W_out + w_out_start + tx;
        if (h_out_start + ty < H_out && w_out_start + tx < W_out) {
            val = fmaxf(val, 0.0f);  // ReLU
            val += bias[co];         // Add bias
            out[out_idx] = val;
        }
    }
}

torch::Tensor conv2d_relu_bias_forward(
    torch::Tensor x,
    torch::Tensor conv_weight,
    torch::Tensor conv_bias,
    torch::Tensor bias) {

    TORCH_CHECK(x.is_cuda(), ""x must be a CUDA tensor"");
    TORCH_CHECK(conv_weight.is_cuda(), ""conv_weight must be a CUDA tensor"");
    TORCH_CHECK(conv_bias.is_cuda(), ""conv_bias must be a CUDA tensor"");
    TORCH_CHECK(bias.is_cuda(), ""bias must be a CUDA tensor"");
    
    const auto N = x.size(0);
    const auto C_in = x.size(1);
    const auto H_in = x.size(2);
    const auto W_in = x.size(3);
    const auto C_out = conv_weight.size(0);
    const auto K_h = conv_weight.size(2);
    const auto K_w = conv_weight.size(3);
    
    const auto H_out = H_in - K_h + 1;
    const auto W_out = W_in - K_w + 1;

    // Ensure contiguous memory layout
    x = x.contiguous();
    conv_weight = conv_weight.contiguous();
    conv_bias = conv_bias.contiguous();
    bias = bias.contiguous();

    auto out = torch::empty({N, C_out, H_out, W_out}, x.options());

    // Calculate grid and block dimensions
    dim3 threadsPerBlock(TILE_WIDTH, TILE_HEIGHT);
    dim3 numBlocks(
        (W_out + TILE_WIDTH - 1) / TILE_WIDTH,
        (H_out + TILE_HEIGHT - 1) / TILE_HEIGHT,
        N * C_out
    );

    shared_conv2d_relu_bias_kernel<<<numBlocks, threadsPerBlock>>>(
        x.data_ptr<float>(),
        conv_weight.data_ptr<float>(),
        conv_bias.data_ptr<float>(),
        bias.data_ptr<float>(),
        out.data_ptr<float>(),
        N, C_in, H_in, W_in,
        C_out, K_h, K_w,
        H_out, W_out
    );

    return out;
}

PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
    m.def(""forward"", &conv2d_relu_bias_forward, ""Shared memory convolution + ReLU + bias (CUDA)"");
}",conv2d,,,,,1,0,0.19018716,4683
"#include <torch/extension.h>
#include <pybind11/pybind11.h>

namespace py = pybind11;

torch::Tensor optimized_forward(torch::Tensor x, py::object params_py, bool is_training) {
    auto get_param = [&](const std::string& key) -> torch::Tensor {
        return params_py.attr(""__getitem__"")(key.c_str()).cast<torch::Tensor>();
    };

    // Convert input to channels_last format for optimal memory access
    x = x.to(torch::MemoryFormat::ChannelsLast);

    // Initial convolution with optimized memory layout
    auto conv1_w = get_param(""conv1_weight"").contiguous().to(torch::MemoryFormat::ChannelsLast);
    x = torch::conv2d(x, conv1_w, {}, {2, 2}, {3, 3});
    x = torch::batch_norm(x, 
                         get_param(""bn1_weight""),
                         get_param(""bn1_bias""),
                         get_param(""bn1_running_mean""),
                         get_param(""bn1_running_var""),
                         is_training, 0.0, 1e-5, true);
    x = torch::relu(x);
    x = torch::max_pool2d(x, {3, 3}, {2, 2}, {1, 1}).contiguous().to(torch::MemoryFormat::ChannelsLast);

    auto process_block = [&](const std::string& block_name, int64_t stride) {
        auto identity = x;
        
        // First convolution
        auto conv1_w = get_param(block_name + ""_conv1_weight"").contiguous().to(torch::MemoryFormat::ChannelsLast);
        x = torch::conv2d(x, conv1_w, {}, {stride, stride}, {1, 1}).contiguous().to(torch::MemoryFormat::ChannelsLast);
        x = torch::batch_norm(x,
                             get_param(block_name + ""_bn1_weight""),
                             get_param(block_name + ""_bn1_bias""),
                             get_param(block_name + ""_bn1_running_mean""),
                             get_param(block_name + ""_bn1_running_var""),
                             is_training, 0.0, 1e-5, true);
        x = torch::relu(x);

        // Second convolution
        auto conv2_w = get_param(block_name + ""_conv2_weight"").contiguous().to(torch::MemoryFormat::ChannelsLast);
        x = torch::conv2d(x, conv2_w, {}, {1, 1}, {1, 1}).contiguous().to(torch::MemoryFormat::ChannelsLast);
        x = torch::batch_norm(x,
                             get_param(block_name + ""_bn2_weight""),
                             get_param(block_name + ""_bn2_bias""),
                             get_param(block_name + ""_bn2_running_mean""),
                             get_param(block_name + ""_bn2_running_var""),
                             is_training, 0.0, 1e-5, true);

        // Downsample path
        std::string ds_key = block_name + ""_downsample_0_weight"";
        if (PyMapping_HasKeyString(params_py.ptr(), ds_key.c_str())) {
            auto ds_conv_w = get_param(block_name + ""_downsample_0_weight"").contiguous().to(torch::MemoryFormat::ChannelsLast);
            identity = torch::conv2d(identity, ds_conv_w, {}, {stride, stride});
            identity = torch::batch_norm(identity,
                                        get_param(block_name + ""_downsample_1_weight""),
                                        get_param(block_name + ""_downsample_1_bias""),
                                        get_param(block_name + ""_downsample_1_running_mean""),
                                        get_param(block_name + ""_downsample_1_running_var""),
                                        is_training, 0.0, 1e-5, true);
        }

        x = torch::relu(x + identity).contiguous().to(torch::MemoryFormat::ChannelsLast);
    };

    // Process all layers with explicit memory layout management
    for (int layer = 1; layer <= 4; ++layer) {
        std::string layer_name = ""layer"" + std::to_string(layer);
        process_block(layer_name + ""_0"", (layer > 1) ? 2 : 1);
        process_block(layer_name + ""_1"", 1);
    }

    x = torch::adaptive_avg_pool2d(x, {1, 1});
    x = x.view({x.size(0), -1});
    return torch::linear(x, get_param(""fc_weight""), get_param(""fc_bias""));
}

PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
    m.def(""forward"", &optimized_forward, ""Channels-last optimized ResNet18"");
}",other,,,,,1,0,0.1904903,4038
"#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>
#include <vector>
#include <cmath>
#include <stdio.h>

__device__ inline float gelu(float x) {
    const float k0 = 0.7978845608028654f;
    return 0.5f * x * (1.0f + tanhf(k0 * (x + 0.044715f * x * x * x)));
}

__global__ void conv_forward_kernel(
    const float* __restrict__ input,
    const float* __restrict__ weight,
    const float* __restrict__ bias,
    const float* __restrict__ multiplier,
    float* __restrict__ output,
    int batch_size,
    int in_channels,
    int input_h,
    int input_w,
    int out_channels,
    int kernel_size,
    int output_h,
    int output_w
) {
    const unsigned int warp_size = 32;
    const unsigned int lane_id = threadIdx.x % warp_size;
    const unsigned int warp_id = threadIdx.x / warp_size;
    
    int idx = (blockIdx.x * blockDim.x + threadIdx.x) / warp_size;
    int total = batch_size * out_channels * output_h * output_w;
    
    while (idx < total) {
        int ow = idx % output_w;
        int tmp = idx / output_w;
        int oh = tmp % output_h;
        tmp = tmp / output_h;
        int oc = tmp % out_channels;
        int n = tmp / out_channels;
        
        float partial_sum = (lane_id == 0) ? bias[oc] : 0.0f;
        
        for (int ic = 0; ic < in_channels; ic++) {
            for (int i = lane_id; i < kernel_size * kernel_size; i += warp_size) {
                int kh = i / kernel_size;
                int kw = i % kernel_size;
                int in_h = oh + kh;
                int in_w = ow + kw;
                
                int input_index = ((n * in_channels + ic) * input_h + in_h) * input_w + in_w;
                int weight_index = ((oc * in_channels + ic) * kernel_size + kh) * kernel_size + kw;
                
                partial_sum += input[input_index] * weight[weight_index];
            }
        }
        
        // Warp reduction using shuffle
        #pragma unroll
        for (int offset = warp_size/2; offset > 0; offset /= 2) {
            partial_sum += __shfl_down_sync(0xffffffff, partial_sum, offset);
        }
        
        if (lane_id == 0) {
            float sum = partial_sum;
            sum *= multiplier[oc];
            sum = (sum > 0.0f) ? sum : 0.01f * sum; // LeakyReLU
            output[idx] = gelu(sum);
        }
        
        idx += (blockDim.x * gridDim.x) / warp_size;
    }
}

torch::Tensor forward_cuda(
    torch::Tensor input,
    torch::Tensor conv_weight,
    torch::Tensor conv_bias,
    torch::Tensor multiplier
) {
    const auto batch_size = input.size(0);
    const auto in_channels = input.size(1);
    const auto input_h = input.size(2);
    const auto input_w = input.size(3);
    const auto out_channels = conv_weight.size(0);
    const auto kernel_size = conv_weight.size(2);
    const auto output_h = input_h - kernel_size + 1;
    const auto output_w = input_w - kernel_size + 1;
    
    auto output = torch::empty({batch_size, out_channels, output_h, output_w}, input.options());
    
    const int threads = 256;
    const int total_elements = batch_size * out_channels * output_h * output_w;
    const int blocks = (total_elements + (threads/32) - 1) / (threads/32);
    
    conv_forward_kernel<<<blocks, threads>>>(
        input.data_ptr<float>(),
        conv_weight.data_ptr<float>(),
        conv_bias.data_ptr<float>(),
        multiplier.data_ptr<float>(),
        output.data_ptr<float>(),
        batch_size,
        in_channels,
        input_h,
        input_w,
        out_channels,
        kernel_size,
        output_h,
        output_w
    );
    
    return output;
}

PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
    m.def(""forward"", &forward_cuda, ""Convolution, scalar multiplication, LeakyReLU and GELU (CUDA)"");
}",conv2d,,,,,1,0,0.19190201,3780
"#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>
#include <vector>

#define CHECK_CUDA(x) TORCH_CHECK(x.is_cuda(), #x "" must be a CUDA tensor"")
#define CHECK_CONTIGUOUS(x) TORCH_CHECK(x.is_contiguous(), #x "" must be contiguous"")
#define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)

#define TILE_SIZE 256
#define WARP_SIZE 32

__global__ void sync_optimized_prod_reduce_kernel(const float* __restrict__ input,
                                                  float* __restrict__ output,
                                                  const int dim_size,
                                                  const int stride) {
    __shared__ float shared_data[TILE_SIZE];
    const int tid = threadIdx.x;
    const int bid = blockIdx.x;
    const int lane_id = tid % WARP_SIZE;
    const int warp_id = tid / WARP_SIZE;
    
    // Initialize partial product
    float thread_prod = 1.0f;
    
    // Process input in tiles to minimize unnecessary synchronizations
    for (int tile_start = 0; tile_start < dim_size; tile_start += TILE_SIZE) {
        // Coalesce loads into shared memory
        if (tid + tile_start < dim_size) {
            shared_data[tid] = input[bid + (tid + tile_start) * stride];
        } else {
            shared_data[tid] = 1.0f;
        }
        __syncthreads(); // Synchronize after loading to shared memory
        
        // Reduce within tile using the threads
        for (int i = tid; i < TILE_SIZE && (i + tile_start < dim_size); i += blockDim.x) {
            thread_prod *= shared_data[i];
        }
        __syncthreads(); // Ensure all reductions are complete before loading next tile
    }
    
    // Warp-level reduction
    for (int offset = WARP_SIZE / 2; offset > 0; offset >>= 1) {
        thread_prod *= __shfl_down_sync(0xffffffff, thread_prod, offset);
    }
    
    // Only one thread per warp writes to shared memory
    if (lane_id == 0) {
        shared_data[warp_id] = thread_prod;
    }
    __syncthreads(); // Synchronize before final reduction

    // Reduce within first warp
    if (warp_id == 0) {
        float final_prod = (lane_id < blockDim.x / WARP_SIZE) ? shared_data[lane_id] : 1.0f;
        for (int offset = WARP_SIZE / 2; offset > 0; offset >>= 1) {
            final_prod *= __shfl_down_sync(0xffffffff, final_prod, offset);
        }
        if (lane_id == 0) {
            output[bid] = final_prod;
        }
    }
}

torch::Tensor forward(torch::Tensor x, int dim) {
    CHECK_INPUT(x);
    
    auto sizes = x.sizes().vec();
    int dim_size = sizes[dim];
    sizes.erase(sizes.begin() + dim);
    torch::Tensor output = torch::empty(sizes, x.options());
    
    int num_elements = output.numel();
    int stride = x.stride(dim);
    
    const float* input_ptr = x.data_ptr<float>();
    float* output_ptr = output.data_ptr<float>();
    
    // Launch configuration optimized for fewer synchronizations
    int threads = TILE_SIZE;
    int blocks = num_elements;
    
    sync_optimized_prod_reduce_kernel<<<blocks, threads>>>(input_ptr, output_ptr, dim_size, stride);
    
    return output;
}

PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
    m.def(""forward"", &forward, ""Synchronization optimized product reduction (CUDA)"");
}
",other,,,,,1,0,0.19282928,3240
"#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>

// Define maximum sizes for constant memory (in floats)
#define MAX_WEIGHT_SIZE 15360  // 15360 * 4 bytes = 60KB
#define MAX_BIAS_SIZE 1024    // 1024 * 4 bytes = 4KB

// Declare weight and bias in constant memory
__constant__ float c_weight[MAX_WEIGHT_SIZE];
__constant__ float c_bias[MAX_BIAS_SIZE];

// Helper to compute output length
inline int compute_output_length(int input_length, int stride, int padding, int dilation, int kernel_size) {
    return (input_length - 1) * stride - 2 * padding + dilation * (kernel_size - 1) + 1;
}

// CUDA kernel using constant memory for weight and bias
__global__ void conv_transpose1d_kernel(
    const float* x_ptr,
    float* output_ptr,
    int batch_size,
    int in_channels,
    int out_channels,
    int input_length,
    int output_length,
    int kernel_size,
    int stride,
    int padding,
    int dilation,
    int has_bias
) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx >= batch_size * out_channels * output_length) return;

    int b = idx / (out_channels * output_length);
    int rem = idx % (out_channels * output_length);
    int oc = rem / output_length;
    int o = rem % output_length;

    float sum = 0.0f;

    // Loop over kernel positions
    for (int k = 0; k < kernel_size; ++k) {
        int i_pos = o + padding - k * dilation;
        if (i_pos % stride != 0) continue;
        int i = i_pos / stride;
        if (i < 0 || i >= input_length) continue;

        // Loop over input channels
        for (int ic = 0; ic < in_channels; ++ic) {
            int x_idx = b * in_channels * input_length + ic * input_length + i;
            int weight_idx = ic * (out_channels * kernel_size) + oc * kernel_size + k;
            sum += x_ptr[x_idx] * c_weight[weight_idx];
        }
    }
    if (has_bias) {
        sum += c_bias[oc];
    }
    int output_idx = b * out_channels * output_length + oc * output_length + o;
    output_ptr[output_idx] = sum;
}

// Forward function that copies weight (and bias) to constant memory and launches the kernel
torch::Tensor forward_cuda(
    torch::Tensor x,
    torch::Tensor weight,
    torch::optional<torch::Tensor> bias,
    int stride,
    int padding,
    int dilation
) {
    TORCH_CHECK(x.device().is_cuda(), ""x must be a CUDA tensor"");
    TORCH_CHECK(weight.device().is_cuda(), ""weight must be a CUDA tensor"");
    TORCH_CHECK(x.dim() == 3, ""x must be 3D (batch, in_channels, input_length)"");
    TORCH_CHECK(weight.dim() == 3, ""weight must be 3D (in_channels, out_channels, kernel_size)"");

    x = x.contiguous();
    weight = weight.contiguous();

    int weight_numel = weight.numel();
    TORCH_CHECK(weight_numel <= MAX_WEIGHT_SIZE, ""Weight tensor exceeds constant memory capacity"");
    
    bool bias_flag = false;
    torch::Tensor bias_contig;
    if (bias.has_value()) {
        bias_contig = bias->contiguous();
        TORCH_CHECK(bias_contig.device().is_cuda(), ""bias must be a CUDA tensor"");
        TORCH_CHECK(bias_contig.dim() == 1, ""bias must be 1D"");
        int out_channels = weight.size(1);
        TORCH_CHECK(bias_contig.size(0) == out_channels, ""bias size must match out_channels"");
        TORCH_CHECK(out_channels <= MAX_BIAS_SIZE, ""Bias tensor exceeds constant memory capacity"");
        bias_flag = true;
    }

    // Copy weight (and bias if present) to constant memory
    cudaMemcpyToSymbol(c_weight, weight.data_ptr<float>(), weight_numel * sizeof(float));
    if (bias_flag) {
        int out_channels = weight.size(1);
        cudaMemcpyToSymbol(c_bias, bias_contig.data_ptr<float>(), out_channels * sizeof(float));
    }

    int batch_size = x.size(0);
    int in_channels = x.size(1);
    int input_length = x.size(2);
    int out_channels = weight.size(1);
    int kernel_size = weight.size(2);

    int output_length = compute_output_length(input_length, stride, padding, dilation, kernel_size);
    auto output = torch::zeros({batch_size, out_channels, output_length}, x.options());

    int num_output_elements = batch_size * out_channels * output_length;
    int threads_per_block = 512;
    int num_blocks = (num_output_elements + threads_per_block - 1) / threads_per_block;

    conv_transpose1d_kernel<<<num_blocks, threads_per_block>>>(
        x.data_ptr<float>(),
        output.data_ptr<float>(),
        batch_size,
        in_channels,
        out_channels,
        input_length,
        output_length,
        kernel_size,
        stride,
        padding,
        dilation,
        bias_flag ? 1 : 0
    );

    return output;
}

PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
    m.def(""forward"", &forward_cuda, ""ConvTranspose1D forward (CUDA) with constant memory"",
          py::arg(""x""), py::arg(""weight""), py::arg(""bias"") = py::none(),
          py::arg(""stride""), py::arg(""padding""), py::arg(""dilation""));
}
",conv2d,,,,,1,0,0.19379056,4879
"#include <torch/extension.h>
#include <vector>
#include <cmath>
#include <limits>

// Optimized block sizes based on profiling
constexpr int BLOCK_SIZE_X = 128;
constexpr int BLOCK_SIZE_Y = 8;
constexpr int WARP_SIZE = 32;
constexpr int SHARED_MEM_BANK_SIZE = 32;

// Constant memory for frequently accessed parameters
__constant__ int64_t d_n_head;
__constant__ int64_t d_n_embd;
__constant__ float d_c_attn_bias[4096];
__constant__ float d_c_proj_bias[4096];

// Shared memory with padding to avoid bank conflicts
__shared__ float shared_buffer[BLOCK_SIZE_X][BLOCK_SIZE_Y + 2];

void setup_constants(
    int64_t n_head,
    int64_t n_embd,
    const torch::Tensor& c_attn_bias,
    const torch::Tensor& c_proj_bias
) {
    cudaMemcpyToSymbol(d_n_head, &n_head, sizeof(int64_t));
    cudaMemcpyToSymbol(d_n_embd, &n_embd, sizeof(int64_t));
    cudaMemcpyToSymbol(d_c_attn_bias, c_attn_bias.data_ptr<float>(), 
                       c_attn_bias.numel() * sizeof(float));
    cudaMemcpyToSymbol(d_c_proj_bias, c_proj_bias.data_ptr<float>(), 
                       c_proj_bias.numel() * sizeof(float));
}

torch::Tensor forward(
    torch::Tensor x,
    torch::Tensor c_attn_weight,
    torch::Tensor c_attn_bias,
    torch::Tensor c_proj_weight,
    torch::Tensor c_proj_bias,
    torch::Tensor bias,
    int64_t n_head,
    int64_t n_embd,
    bool is_training
) {
    using namespace torch::indexing;
    
    // Setup constants and create streams
    setup_constants(n_head, n_embd, c_attn_bias, c_proj_bias);
    cudaStream_t compute_stream, transfer_stream;
    cudaStreamCreate(&compute_stream);
    cudaStreamCreate(&transfer_stream);
    
    auto B = x.size(0);
    auto T = x.size(1);
    auto C = x.size(2);
    
    // Ensure aligned memory access
    auto x_contig = x.contiguous();
    auto c_attn_weight_contig = c_attn_weight.contiguous();
    
    // QKV projection with optimized block configuration
    dim3 qkv_block(BLOCK_SIZE_X, BLOCK_SIZE_Y);
    dim3 qkv_grid((B * T + BLOCK_SIZE_X - 1) / BLOCK_SIZE_X,
                  (C + BLOCK_SIZE_Y - 1) / BLOCK_SIZE_Y);
    
    auto qkv = torch::empty({B, T, 3 * C}, x.options());
    auto qkv_2d = torch::addmm(
        c_attn_bias,
        x_contig.reshape({B * T, C}),
        c_attn_weight_contig.transpose(0, 1),
        1.0, 1.0
    );
    qkv = qkv_2d.reshape({B, T, 3 * C});
    
    // Split QKV with optimized memory access
    auto head_size = C / n_head;
    auto q = qkv.slice(2, 0, n_embd)
        .reshape({B, T, n_head, head_size})
        .permute({0, 2, 1, 3})
        .contiguous();
    auto k = qkv.slice(2, n_embd, 2 * n_embd)
        .reshape({B, T, n_head, head_size})
        .permute({0, 2, 1, 3})
        .contiguous();
    auto v = qkv.slice(2, 2 * n_embd, 3 * n_embd)
        .reshape({B, T, n_head, head_size})
        .permute({0, 2, 1, 3})
        .contiguous();
    
    // Attention computation with optimized block sizes
    float scale = 1.0f / std::sqrt(static_cast<float>(head_size));
    auto q_2d = q.reshape({B * n_head, T, head_size});
    auto k_2d = k.reshape({B * n_head, T, head_size});
    
    dim3 att_block(BLOCK_SIZE_X, BLOCK_SIZE_Y);
    dim3 att_grid((T + BLOCK_SIZE_X - 1) / BLOCK_SIZE_X,
                  (T + BLOCK_SIZE_Y - 1) / BLOCK_SIZE_Y);
    
    auto att = torch::bmm(q_2d, k_2d.transpose(1, 2)) * scale;
    
    // Mask and softmax with optimized blocking
    att = att.reshape({B, n_head, T, T});
    auto mask = bias.index({Slice(), Slice(), Slice(None, T), Slice(None, T)});
    att = att.masked_fill(mask.eq(0), -std::numeric_limits<float>::infinity());
    att = torch::softmax(att.reshape({-1, T}), -1).reshape({B * n_head, T, T});
    
    // Final attention computation
    auto v_2d = v.reshape({B * n_head, T, head_size});
    auto y = torch::bmm(att, v_2d);
    
    // Output projection with optimized memory access
    y = y.reshape({B, n_head, T, head_size})
         .permute({0, 2, 1, 3})
         .contiguous()
         .reshape({B * T, C});
    
    auto out = torch::addmm(
        c_proj_bias,
        y,
        c_proj_weight.transpose(0, 1),
        1.0, 1.0
    );
    
    // Cleanup
    cudaStreamDestroy(compute_stream);
    cudaStreamDestroy(transfer_stream);
    
    return out.reshape({B, T, C});
}

PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
    m.def(""forward"", &forward, ""Optimized block-tuned Causal Attention forward (CUDA)"");
}",other,,,,,1,0,0.19565834,4396
"#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>
#include <pybind11/pybind11.h>

namespace py = pybind11;

#define THREADS_PER_BLOCK 256
#define ELEMENTS_PER_THREAD 4

template <typename scalar_t>
__global__ void depthwise_conv2d_kernel(
    const scalar_t* __restrict__ input,
    const scalar_t* __restrict__ weight,
    const scalar_t* __restrict__ bias,
    scalar_t* __restrict__ output,
    int batch,
    int channels,
    int in_h, int in_w,
    int out_h, int out_w,
    int k,
    int stride,
    int padding,
    int dilation) {

  int tid = blockIdx.x * blockDim.x + threadIdx.x;
  int stride_x = blockDim.x * gridDim.x;
  int total = batch * channels * out_h * out_w;

  // Process multiple elements per thread
  for (int index = tid; index < total; index += stride_x) {
    int ow = index % out_w;
    int tmp = index / out_w;
    int oh = tmp % out_h;
    tmp = tmp / out_h;
    int c = tmp % channels;
    int n = tmp / channels;

    scalar_t sum = 0;
    #pragma unroll
    for (int i = 0; i < k; ++i) {
      #pragma unroll
      for (int j = 0; j < k; ++j) {
        int ih = oh * stride - padding + i * dilation;
        int iw = ow * stride - padding + j * dilation;
        if (ih >= 0 && ih < in_h && iw >= 0 && iw < in_w) {
          int input_idx = n * channels * in_h * in_w + c * in_h * in_w + ih * in_w + iw;
          int weight_idx = c * k * k + i * k + j;
          sum += input[input_idx] * weight[weight_idx];
        }
      }
    }
    if (bias != nullptr)
      sum += bias[c];
    output[index] = sum;
  }
}

// Shared memory tile size for pointwise convolution
#define TILE_DIM 32

template <typename scalar_t>
__global__ void pointwise_conv2d_kernel(
    const scalar_t* __restrict__ input,
    const scalar_t* __restrict__ weight,
    const scalar_t* __restrict__ bias,
    scalar_t* __restrict__ output,
    int batch,
    int in_channels,
    int out_channels,
    int h,
    int w) {

  __shared__ scalar_t shared_input[TILE_DIM][TILE_DIM];  // Removed +1 to avoid bank conflicts, ensure proper access patterns

  int bx = blockIdx.x * TILE_DIM;
  int by = blockIdx.y;
  int tx = threadIdx.x;
  int ty = threadIdx.y;

  int h_idx = bx + tx;
  int batch_channel = by;
  int n = batch_channel / out_channels;
  int oc = batch_channel % out_channels;

  scalar_t sum = 0;
  
  // Loop over input channel tiles
  for (int tile = 0; tile < (in_channels + TILE_DIM - 1) / TILE_DIM; ++tile) {
    // Load input tile into shared memory
    int ic = tile * TILE_DIM + ty;
    if (h_idx < h * w && ic < in_channels) {
      shared_input[ty][tx] = input[n * in_channels * h * w + ic * h * w + h_idx];
    } else {
      shared_input[ty][tx] = 0;
    }
    __syncthreads();

    // Compute partial sums
    #pragma unroll
    for (int k = 0; k < TILE_DIM && (tile * TILE_DIM + k) < in_channels; ++k) {
      sum += shared_input[k][tx] * weight[oc * in_channels + tile * TILE_DIM + k];
    }
    __syncthreads();
  }

  if (h_idx < h * w) {
    if (bias != nullptr)
      sum += bias[oc];
    output[n * out_channels * h * w + oc * h * w + h_idx] = sum;
  }
}

torch::Tensor forward_cuda(
    const torch::Tensor& x,
    const torch::Tensor& depthwise_weight,
    const torch::Tensor& pointwise_weight,
    const torch::Tensor& depthwise_bias,
    const torch::Tensor& pointwise_bias,
    int stride,
    int padding,
    int dilation) {

  TORCH_CHECK(x.is_cuda(), ""Input tensor must be a CUDA tensor"");

  int batch = x.size(0);
  int in_channels = x.size(1);
  int in_h = x.size(2);
  int in_w = x.size(3);
  int k = depthwise_weight.size(2);
  int out_h = (in_h + 2 * padding - dilation * (k - 1) - 1) / stride + 1;
  int out_w = (in_w + 2 * padding - dilation * (k - 1) - 1) / stride + 1;

  auto depthwise_output = torch::empty({batch, in_channels, out_h, out_w}, x.options());

  int total_depthwise = batch * in_channels * out_h * out_w;
  int threads = THREADS_PER_BLOCK;
  int blocks = (total_depthwise + threads * ELEMENTS_PER_THREAD - 1) / (threads * ELEMENTS_PER_THREAD);

  const void* depthwise_bias_ptr = (depthwise_bias.defined() && depthwise_bias.numel() > 0)
                                     ? depthwise_bias.data_ptr()
                                     : nullptr;

  AT_DISPATCH_FLOATING_TYPES(x.scalar_type(), ""depthwise_conv2d_cuda"", ([&] {
    depthwise_conv2d_kernel<scalar_t><<<blocks, threads>>>(
        x.data_ptr<scalar_t>(),
        depthwise_weight.data_ptr<scalar_t>(),
        reinterpret_cast<const scalar_t*>(depthwise_bias_ptr),
        depthwise_output.data_ptr<scalar_t>(),
        batch,
        in_channels,
        in_h, in_w,
        out_h, out_w,
        k,
        stride,
        padding,
        dilation);
  }));

  int out_channels = pointwise_weight.size(0);
  auto output = torch::empty({batch, out_channels, out_h, out_w}, x.options());

  dim3 threadsPoint(TILE_DIM, TILE_DIM);
  dim3 blocksPoint((out_h * out_w + TILE_DIM - 1) / TILE_DIM,
                   batch * out_channels);

  const void* pointwise_bias_ptr = (pointwise_bias.defined() && pointwise_bias.numel() > 0)
                                     ? pointwise_bias.data_ptr()
                                     : nullptr;

  AT_DISPATCH_FLOATING_TYPES(x.scalar_type(), ""pointwise_conv2d_cuda"", ([&] {
    pointwise_conv2d_kernel<scalar_t><<<blocksPoint, threadsPoint>>>(
        depthwise_output.data_ptr<scalar_t>(),
        pointwise_weight.data_ptr<scalar_t>(),
        reinterpret_cast<const scalar_t*>(pointwise_bias_ptr),
        output.data_ptr<scalar_t>(),
        batch,
        in_channels,
        out_channels,
        out_h, out_w);
  }));

  return output;
}

at::Tensor toTensor(const py::object& obj) {
  if (obj.is_none()) {
    return at::Tensor();
  }
  try {
    return obj.cast<at::Tensor>();
  } catch (const py::cast_error& e) {
    if (py::hasattr(obj, ""data"")) {
      return obj.attr(""data"").cast<at::Tensor>();
    }
    throw std::runtime_error(""Expected a torch Tensor or Parameter."");
  }
}

at::Tensor forward_wrapper(py::object x_obj,
                           py::object depthwise_weight_obj,
                           py::object pointwise_weight_obj,
                           py::object depthwise_bias_obj,
                           py::object pointwise_bias_obj,
                           int stride,
                           int padding,
                           int dilation) {

  auto x = toTensor(x_obj);
  auto depthwise_weight = toTensor(depthwise_weight_obj);
  auto pointwise_weight = toTensor(pointwise_weight_obj);
  auto depthwise_bias = toTensor(depthwise_bias_obj);
  auto pointwise_bias = toTensor(pointwise_bias_obj);

  return forward_cuda(x, depthwise_weight, pointwise_weight,
                      depthwise_bias, pointwise_bias,
                      stride, padding, dilation);
}

PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
  m.def(""forward"", &forward_wrapper, ""CUDA depthwise separable convolution forward"");
}",conv2d,,,,,1,0,0.19664249,6957
"#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>
#include <vector>
#include <limits>
#include <cmath>

// This kernel uses shared memory and warp shuffle reduction to optimize performance.

#include <cooperative_groups.h>

template <typename scalar_t>
__global__ void log_softmax_forward_kernel_shared_mem(
    const scalar_t* __restrict__ input,
    scalar_t* __restrict__ output,
    const int dim_size) {

    namespace cg = cooperative_groups;
    cg::thread_block block = cg::this_thread_block();
    cg::thread_block_tile<32> warp = cg::tiled_partition<32>(block);

    // Each block processes one row (batch element)
    int batch_idx = blockIdx.x;
    const scalar_t* input_row = input + batch_idx * dim_size;
    scalar_t* output_row = output + batch_idx * dim_size;

    const int tid = threadIdx.x;
    const int blockSize = blockDim.x;
    const int warpSize = 32;
    const int warpId = tid / warpSize;
    const int laneId = tid % warpSize;

    // Shared memory allocation with double buffering
    extern __shared__ __align__(sizeof(scalar_t)) unsigned char smem[];
    scalar_t* shared_data = reinterpret_cast<scalar_t*>(smem);

    // Step 1: Compute the maximum value in the row using shared memory for intra-block communication
    scalar_t local_max = -std::numeric_limits<scalar_t>::infinity();
    for (int i = tid; i < dim_size; i += blockSize) {
        scalar_t val = input_row[i];
        local_max = (val > local_max) ? val : local_max;
    }

    // Store local max in shared memory
    shared_data[tid] = local_max;
    __syncthreads();

    // Perform reduction in shared memory
    for (int stride = blockSize / 2; stride > 0; stride /= 2) {
        if (tid < stride) {
            shared_data[tid] = max(shared_data[tid], shared_data[tid + stride]);
        }
        __syncthreads();
    }
    scalar_t global_max = shared_data[0];

    // Step 2: Compute the sum of exp(val - global_max) along the row
    scalar_t local_sum = 0;
    for (int i = tid; i < dim_size; i += blockSize) {
        scalar_t exp_val = exp(input_row[i] - global_max);
        local_sum += exp_val;
        output_row[i] = exp_val;  // store intermediate result
    }

    // Store local sums in shared memory
    shared_data[tid] = local_sum;
    __syncthreads();

    // Reduce sum in shared memory
    for (int stride = blockSize / 2; stride > 0; stride /= 2) {
        if (tid < stride) {
            shared_data[tid] += shared_data[tid + stride];
        }
        __syncthreads();
    }
    scalar_t global_sum = shared_data[0];

    scalar_t log_sum = log(global_sum);

    // Step 3: Compute final log softmax output
    for (int i = tid; i < dim_size; i += blockSize) {
        output_row[i] = (input_row[i] - global_max) - log_sum;
    }
}

// Host function launching the kernel
torch::Tensor log_softmax_cuda_forward(torch::Tensor input, int64_t dim) {
    TORCH_CHECK(input.is_cuda(), ""input must be a CUDA tensor"");
    TORCH_CHECK(
        input.scalar_type() == torch::kFloat32 || input.scalar_type() == torch::kFloat64,
        ""input must be float32 or float64"");

    int64_t ndim = input.dim();
    TORCH_CHECK(dim >= -ndim && dim < ndim, ""dim out of range"");
    dim = (dim >= 0) ? dim : dim + ndim;

    // Permute input to bring 'dim' to the last dimension
    std::vector<int64_t> permute_dims;
    for (int64_t i = 0; i < ndim; ++i) {
        if (i != dim) {
            permute_dims.push_back(i);
        }
    }
    permute_dims.push_back(dim);
    input = input.permute(permute_dims).contiguous();
    
    int64_t batch_size = input.numel() / input.size(-1);
    int64_t dim_size = input.size(-1);
    auto output = torch::empty_like(input);

    // Choose number of threads: next power of two of dim_size, capped at 1024
    int threads = 1;
    while (threads < dim_size) threads <<= 1;
    if (threads > 1024) threads = 1024;

    // Compute required shared memory
    size_t shared_mem_size = threads * sizeof(float);  // temporary using float size

    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), ""log_softmax_forward_cuda_shared_mem"", ([&] {
        shared_mem_size = threads * sizeof(scalar_t);
        log_softmax_forward_kernel_shared_mem<scalar_t><<<batch_size, threads, shared_mem_size>>>(
            input.data_ptr<scalar_t>(),
            output.data_ptr<scalar_t>(),
            dim_size);
    }));

    // Inverse permutation to restore original shape
    std::vector<int64_t> inverse_permute_dims(ndim);
    for (size_t i = 0; i < permute_dims.size(); ++i) {
        inverse_permute_dims[permute_dims[i]] = i;
    }
    output = output.permute(inverse_permute_dims);
    return output;
}

PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
    m.def(""forward"", &log_softmax_cuda_forward, ""LogSoftmax forward shared memory (CUDA)"");
}
",other,,,,,1,0,0.19924852,4808
"#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>
#include <math.h>

// Define a device inline function for tanh that uses fast math for float types
// and standard tanh for other types.

template<typename scalar_t>
__device__ __forceinline__ scalar_t fast_tanh(scalar_t x) {
    return tanh(x);
}

template<>
__device__ __forceinline__ float fast_tanh<float>(float x) {
    return __tanhf(x);
}


#define CHECK_CUDA(x) AT_ASSERTM(x.is_cuda(), #x "" must be a CUDA tensor"")
#define UNROLL_FACTOR 4

template <typename scalar_t>
__global__ void bias_subtract_tanh_kernel_unrolled(
    scalar_t* __restrict__ output,
    const scalar_t* __restrict__ bias,
    int64_t N, int64_t C_out, int64_t H_out, int64_t W_out
) {
    __shared__ scalar_t shared_bias[256];  // Assuming C_out <= 256
    
    // Load bias into shared memory
    if (threadIdx.x < C_out) {
        shared_bias[threadIdx.x] = bias[threadIdx.x];
    }
    __syncthreads();

    const int64_t size = N * C_out * H_out * W_out;
    const int64_t tid = blockIdx.x * blockDim.x + threadIdx.x;
    const int64_t stride = blockDim.x * gridDim.x;
    const int64_t HW = H_out * W_out;

    #pragma unroll
    for (int64_t base_idx = tid; base_idx < size; base_idx += stride * UNROLL_FACTOR) {
        scalar_t vals[UNROLL_FACTOR];
        int64_t c_vals[UNROLL_FACTOR];
        
        #pragma unroll
        for (int i = 0; i < UNROLL_FACTOR; i++) {
            if (base_idx + i * stride < size) {
                const int64_t idx = base_idx + i * stride;
                c_vals[i] = (idx / HW) % C_out;
                vals[i] = output[idx] - shared_bias[c_vals[i]];
            }
        }

        #pragma unroll
        for (int i = 0; i < UNROLL_FACTOR; i++) {
            if (base_idx + i * stride < size) {
                output[base_idx + i * stride] = tanh(vals[i]);
            }
        }
    }
}

torch::Tensor forward(
    torch::Tensor x,
    int64_t stride,
    int64_t padding,
    int64_t output_padding,
    torch::Tensor conv_transpose,
    torch::Tensor conv_transpose_bias,
    torch::Tensor bias
) {
    CHECK_CUDA(x);
    CHECK_CUDA(conv_transpose);
    CHECK_CUDA(conv_transpose_bias);
    CHECK_CUDA(bias);

    torch::DeviceGuard device_guard(x.device());

    auto output = at::conv_transpose2d(
        x,
        conv_transpose,
        conv_transpose_bias,
        {stride, stride},
        {padding, padding},
        {output_padding, output_padding},
        1
    );

    int64_t N = output.size(0);
    int64_t C_out = output.size(1);
    int64_t H_out = output.size(2);
    int64_t W_out = output.size(3);
    
    const int threads = 256;
    const int blocks = std::min(65535, static_cast<int>((N * C_out * H_out * W_out + threads - 1) / threads));

    AT_DISPATCH_FLOATING_TYPES(output.scalar_type(), ""bias_subtract_tanh_cuda_unrolled"", ([&] {
        bias_subtract_tanh_kernel_unrolled<scalar_t><<<blocks, threads>>>(
            output.data_ptr<scalar_t>(),
            bias.data_ptr<scalar_t>(),
            N, C_out, H_out, W_out
        );
    }));

    return output;
}

PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
    m.def(""forward"", &forward, ""CUDA forward function with unrolled loops"");
}",other,,,,,1,0,0.20047943,3221
"#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>

#define TILE_WIDTH 32  // Increased tile width for better occupancy
#define NUM_STREAMS 4
#define CHUNKS_PER_STREAM 2  // Process multiple chunks per stream for better overlap

template <typename scalar_t>
__global__ void optimized_matmul_kernel(
    const scalar_t* __restrict__ A,
    const scalar_t* __restrict__ B,
    scalar_t* __restrict__ C,
    const int M, const int K, const int N,
    const int row_offset) {
    
    // Increased shared memory for larger tiles
    __shared__ scalar_t sA[TILE_WIDTH][TILE_WIDTH];
    __shared__ scalar_t sB[TILE_WIDTH][TILE_WIDTH];
    
    const int row = row_offset + blockIdx.y * TILE_WIDTH + threadIdx.y;
    const int col = blockIdx.x * TILE_WIDTH + threadIdx.x;
    
    // Use register for accumulation
    scalar_t value = 0;
    
    // Prefetch first tile coordinates
    int curr_tile_idx = 0;
    int tiledA_col = threadIdx.x;
    int tiledB_row = threadIdx.y;
    
    const int num_tiles = (K + TILE_WIDTH - 1) / TILE_WIDTH;
    
    #pragma unroll 4
    for (int t = 0; t < num_tiles; ++t) {
        // Load current tiles using vectorized loads where possible
        if (row < (row_offset + M) && tiledA_col < K)
            sA[threadIdx.y][threadIdx.x] = __ldg(&A[row * K + tiledA_col]);
        else
            sA[threadIdx.y][threadIdx.x] = 0;
            
        if (col < N && tiledB_row < K)
            sB[threadIdx.y][threadIdx.x] = __ldg(&B[tiledB_row * N + col]);
        else
            sB[threadIdx.y][threadIdx.x] = 0;
            
        __syncthreads();
        
        // Compute using registers and unrolled loop
        #pragma unroll
        for (int i = 0; i < TILE_WIDTH; ++i) {
            value = fma(sA[threadIdx.y][i], sB[i][threadIdx.x], value);
        }
        
        __syncthreads();
        
        // Update tile coordinates
        tiledA_col += TILE_WIDTH;
        tiledB_row += TILE_WIDTH;
    }
    
    // Write result using coalesced access
    if (row < (row_offset + M) && col < N) {
        C[row * N + col] = value;
    }
}

torch::Tensor module_fn(torch::Tensor A, torch::Tensor B) {
    TORCH_CHECK(A.is_cuda() && B.is_cuda(), ""Inputs must be CUDA tensors"");
    
    const int64_t M = A.size(0);
    const int64_t K = A.size(1);
    const int64_t N = B.size(1);
    TORCH_CHECK(K == B.size(0), ""Dimension mismatch"");
    
    // Ensure 128-bit alignment
    TORCH_CHECK((reinterpret_cast<uintptr_t>(A.data_ptr()) & 15) == 0, ""Input A must be 128-bit aligned"");
    TORCH_CHECK((reinterpret_cast<uintptr_t>(B.data_ptr()) & 15) == 0, ""Input B must be 128-bit aligned"");
    
    auto C = torch::empty({M, N}, A.options());
    
    // Create and store streams
    std::vector<cudaStream_t> streams(NUM_STREAMS);
    for (int i = 0; i < NUM_STREAMS; ++i) {
        cudaStreamCreateWithFlags(&streams[i], cudaStreamNonBlocking);
    }
    
    const int chunk_size = (M + (NUM_STREAMS * CHUNKS_PER_STREAM) - 1) / (NUM_STREAMS * CHUNKS_PER_STREAM);
    const dim3 threads(TILE_WIDTH, TILE_WIDTH);
    
    AT_DISPATCH_FLOATING_TYPES(A.scalar_type(), ""optimized_matmul_kernel"", [&] {
        for (int chunk = 0; chunk < NUM_STREAMS * CHUNKS_PER_STREAM; ++chunk) {
            const int stream_idx = chunk % NUM_STREAMS;
            const int row_start = chunk * chunk_size;
            const int valid_rows = std::min(chunk_size, static_cast<int>(M - row_start));
            
            if (valid_rows <= 0) break;
            
            const dim3 blocks((N + TILE_WIDTH - 1) / TILE_WIDTH,
                            (valid_rows + TILE_WIDTH - 1) / TILE_WIDTH);
                            
            optimized_matmul_kernel<scalar_t><<<blocks, threads, 0, streams[stream_idx]>>>(
                A.data_ptr<scalar_t>(),
                B.data_ptr<scalar_t>(),
                C.data_ptr<scalar_t>(),
                valid_rows, K, N,
                row_start
            );
        }
    });
    
    // Cleanup streams
    for (auto& stream : streams) {
        cudaStreamSynchronize(stream);
        cudaStreamDestroy(stream);
    }
    
    return C;
}

PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
    m.def(""forward"", &module_fn, ""Optimized multi-stream tiled matrix multiplication"");
}",other,,,,,1,0,0.20063338,4281
"#include <torch/extension.h>
#include <ATen/ATen.h>
#include <ATen/cuda/CUDAContext.h>
#include <pybind11/pybind11.h>
#include <pybind11/stl.h>
#include <vector>
#include <cuda_runtime.h>

namespace py = pybind11;

// Vectorized kernel that copies a contiguous block of new features from src to dst.
// It assumes that each batch's block (of size block_size = growth_rate * spatial_size) is 128-bit aligned
// (i.e. block_size is a multiple of 4 floats).

__global__ void copy_feature_kernel_batch(const float* src, float* dst,
                                            int total_channels, int current_channels,
                                            int block_size, int spatial_size, int batch) {
  // Each batch has block_size elements to copy (growth_rate * spatial_size).
  // We'll copy in units of 4 floats (16 bytes).
  int num_vec_per_batch = block_size / 4; // assuming block_size is divisible by 4
  int idx = blockIdx.x * blockDim.x + threadIdx.x;
  int total_vec = batch * num_vec_per_batch;
  if (idx < total_vec) {
    int b = idx / num_vec_per_batch;       // batch index
    int vec_idx = idx % num_vec_per_batch;   // index within the batch block, counting in float4 units
    // For each batch, src is contiguous.
    const float4* src_vec = reinterpret_cast<const float4*>(src + b * block_size);
    // For dst, we compute the offset: each batch occupies (total_channels * spatial_size) elements.
    // The new feature should be placed starting at channel index current_channels.
    float4* dst_vec = reinterpret_cast<float4*>(dst + b * (total_channels * spatial_size) + current_channels * spatial_size);
    dst_vec[vec_idx] = __ldg(&src_vec[vec_idx]);
  }
}

// Fallback kernel for when the block size is not divisible by 4.
__global__ void copy_feature_kernel_batch_tail(const float* src, float* dst,
                                                 int total_channels, int current_channels,
                                                 int block_size, int spatial_size, int batch) {
  int idx = blockIdx.x * blockDim.x + threadIdx.x;
  int total_elems = batch * block_size;
  if (idx < total_elems) {
    int b = idx / block_size;
    int pos = idx % block_size;
    // Compute destination index manually:
    int dst_index = b * (total_channels * spatial_size) + current_channels * spatial_size + pos;
    dst[dst_index] = __ldg(&src[b * block_size + pos]);
  }
}

// The layer function remains the same as before, applying batch norm, relu, conv2d, and dropout.

torch::Tensor layer_fn(
    torch::Tensor x,
    torch::Tensor bn_weight,
    torch::Tensor bn_bias,
    torch::Tensor bn_mean,
    torch::Tensor bn_var,
    torch::Tensor conv_weight,
    bool is_training
) {
    const double momentum = 0.1;
    const double eps = 1e-5;

    x = at::batch_norm(
        x,
        bn_weight,
        bn_bias,
        bn_mean,
        bn_var,
        is_training,
        momentum,
        eps,
        true
    );

    x = at::relu(x);

    x = at::conv2d(
        x,
        conv_weight,
        torch::Tensor(),
        {1, 1},
        {1, 1}
    );

    x = at::dropout(x, 0.0, is_training);

    return x;
}

// The forward function pre-allocates the entire output and, for each layer, computes the new feature
// and copies it into the appropriate location in the output using the custom CUDA kernel
// that leverages __ldg() and 128-bit aligned (float4) loads/stores when possible.

torch::Tensor forward(
    torch::Tensor x,
    py::object params,
    bool is_training
) {
    // Retrieve parameter lists from the ParameterDict
    py::list bn_weights = params.attr(""__getitem__"")(""bn_weights"");
    py::list bn_biases = params.attr(""__getitem__"")(""bn_biases"");
    py::list bn_means = params.attr(""__getitem__"")(""bn_means"");
    py::list bn_vars = params.attr(""__getitem__"")(""bn_vars"");
    py::list conv_weights = params.attr(""__getitem__"")(""conv_weights"");

    size_t num_layers = bn_weights.size();

    // Pre-calculate dimensions
    int64_t batch = x.size(0);
    int64_t input_channels = x.size(1);
    int64_t height = x.size(2);
    int64_t width = x.size(3);
    int64_t spatial_size = height * width;

    // Assume growth_rate from first conv weight's output channels
    int64_t growth_rate = conv_weights[0].cast<torch::Tensor>().size(0);
    int64_t total_channels = input_channels + growth_rate * num_layers;

    auto options = x.options();
    torch::Tensor output = torch::empty({batch, total_channels, height, width}, options);

    // Copy initial input to output
    output.narrow(1, 0, input_channels).copy_(x);

    int64_t current_channels = input_channels;
    torch::Tensor current_input = x;

    // Get the current CUDA stream
    auto stream = at::cuda::getCurrentCUDAStream();

    for (size_t i = 0; i < num_layers; ++i) {
        // Compute the new feature using the layer function
        torch::Tensor new_feature = layer_fn(
            current_input,
            bn_weights[i].cast<torch::Tensor>(),
            bn_biases[i].cast<torch::Tensor>(),
            bn_means[i].cast<torch::Tensor>(),
            bn_vars[i].cast<torch::Tensor>(),
            conv_weights[i].cast<torch::Tensor>(),
            is_training
        );

        // new_feature dimensions: [batch, growth_rate, height, width]
        new_feature = new_feature.contiguous();  // ensure contiguous memory

        // Instead of using at::copy_, we use a custom CUDA kernel to optimize global memory loads/stores
        // Calculate the number of elements per batch in new_feature
        int block_size = growth_rate * spatial_size; // number of floats per batch for new_feature
        const float* src = new_feature.data_ptr<float>();
        // Calculate destination pointer offset in the pre-allocated output using manual pointer arithmetic
        // For each batch, destination block starts at offset: current_channels * spatial_size within that batch
        float* dst = output.data_ptr<float>();

        // Check if we can use 128-bit (float4) vectorized copy
        if ((block_size % 4) == 0) {
            int num_vec_per_batch = block_size / 4; // number of float4 elements per batch
            int total_vec = batch * num_vec_per_batch;
            int threads = 256;
            int blocks = (total_vec + threads - 1) / threads;
            copy_feature_kernel_batch<<<blocks, threads, 0, stream.stream()>>>(
                src,
                dst,
                total_channels,
                current_channels,
                block_size,
                spatial_size,
                batch
            );
        } else {
            int total_elems = batch * block_size;
            int threads = 256;
            int blocks = (total_elems + threads - 1) / threads;
            copy_feature_kernel_batch_tail<<<blocks, threads, 0, stream.stream()>>>(
                src,
                dst,
                total_channels,
                current_channels,
                block_size,
                spatial_size,
                batch
            );
        }

        // Update current_channels and current_input to include the new feature.
        current_channels += growth_rate;
        // Obtain a view on output that contains all features computed so far.
        current_input = output.narrow(1, 0, current_channels);
    }

    return output;
}

PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
    m.def(""forward"", &forward, ""DenseNet121 dense block forward function with optimized global memory access"");
}
",other,,,,,1,0,0.20076963,7495
"#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>
#include <vector>

// Constants for the first convolution layer (Block 1, first conv)
#define CONV1_OUT_CHANNELS 64
#define CONV1_IN_CHANNELS 3
#define CONV1_KERNEL_SIZE 3
#define CONV1_WEIGHTS_SIZE (CONV1_OUT_CHANNELS * CONV1_IN_CHANNELS * CONV1_KERNEL_SIZE * CONV1_KERNEL_SIZE)

// Store first conv layer weights and biases in constant memory
__constant__ float const_conv1_weights[CONV1_WEIGHTS_SIZE];
__constant__ float const_conv1_bias[CONV1_OUT_CHANNELS];

// Optimized kernel for the first convolution layer with loop unrolling
// Input shape: [N, CONV1_IN_CHANNELS, H, W]
// Output shape: [N, CONV1_OUT_CHANNELS, H, W]
__global__ void conv1_optimized_kernel(const float* __restrict__ input,
                                        float* __restrict__ output,
                                        int N, int H, int W) {
    // Determine sample index and output channel from gridDim.z
    int n = blockIdx.z / CONV1_OUT_CHANNELS;
    int oc = blockIdx.z % CONV1_OUT_CHANNELS;

    int x = blockIdx.x * blockDim.x + threadIdx.x;
    int y = blockIdx.y * blockDim.y + threadIdx.y;

    if (x < W && y < H) {
        float sum = 0.0f;
        // Base index for the current sample
        int base_input = n * (CONV1_IN_CHANNELS * H * W);

        // Loop over input channels (cannot unroll because there are 3 channels but loop is small)
        for (int c = 0; c < CONV1_IN_CHANNELS; c++) {
            int input_channel_base = base_input + c * H * W;

            // Unrolled 3x3 convolution with padding = 1
            // Row -1
            int y0 = y - 1;
            int x0 = x - 1;
            float in0 = (y0 >= 0 && y0 < H && x0 >= 0 && x0 < W) ? input[input_channel_base + y0 * W + x0] : 0.0f;
            int x1 = x;
            float in1 = (y0 >= 0 && y0 < H && x1 >= 0 && x1 < W) ? input[input_channel_base + y0 * W + x1] : 0.0f;
            int x2 = x + 1;
            float in2 = (y0 >= 0 && y0 < H && x2 >= 0 && x2 < W) ? input[input_channel_base + y0 * W + x2] : 0.0f;

            // Row 0
            int y1 = y;
            int x3 = x - 1;
            float in3 = (y1 >= 0 && y1 < H && x3 >= 0 && x3 < W) ? input[input_channel_base + y1 * W + x3] : 0.0f;
            int x4 = x;
            float in4 = (y1 >= 0 && y1 < H && x4 >= 0 && x4 < W) ? input[input_channel_base + y1 * W + x4] : 0.0f;
            int x5 = x + 1;
            float in5 = (y1 >= 0 && y1 < H && x5 >= 0 && x5 < W) ? input[input_channel_base + y1 * W + x5] : 0.0f;

            // Row +1
            int y2 = y + 1;
            int x6 = x - 1;
            float in6 = (y2 >= 0 && y2 < H && x6 >= 0 && x6 < W) ? input[input_channel_base + y2 * W + x6] : 0.0f;
            int x7 = x;
            float in7 = (y2 >= 0 && y2 < H && x7 >= 0 && x7 < W) ? input[input_channel_base + y2 * W + x7] : 0.0f;
            int x8 = x + 1;
            float in8 = (y2 >= 0 && y2 < H && x8 >= 0 && x8 < W) ? input[input_channel_base + y2 * W + x8] : 0.0f;

            // Load constant weights for this input channel and output channel
            int weight_base = oc * (CONV1_IN_CHANNELS * 9) + c * 9;
            float w0 = const_conv1_weights[weight_base + 0];
            float w1 = const_conv1_weights[weight_base + 1];
            float w2 = const_conv1_weights[weight_base + 2];
            float w3 = const_conv1_weights[weight_base + 3];
            float w4 = const_conv1_weights[weight_base + 4];
            float w5 = const_conv1_weights[weight_base + 5];
            float w6 = const_conv1_weights[weight_base + 6];
            float w7 = const_conv1_weights[weight_base + 7];
            float w8 = const_conv1_weights[weight_base + 8];

            // Accumulate contributions
            sum += in0 * w0 + in1 * w1 + in2 * w2 +
                   in3 * w3 + in4 * w4 + in5 * w5 +
                   in6 * w6 + in7 * w7 + in8 * w8;
        }
        // Add bias from constant memory
        sum += const_conv1_bias[oc];

        // Write result to output tensor [N, CONV1_OUT_CHANNELS, H, W]
        int out_index = n * (CONV1_OUT_CHANNELS * H * W) + oc * (H * W) + y * W + x;
        output[out_index] = sum;
    }
}

// Kernel launcher for conv1_optimized_kernel
void conv1_optimized_launcher(const float* input, float* output, int N, int H, int W, cudaStream_t stream = 0) {
    dim3 block(16, 16);
    dim3 grid((W + block.x - 1) / block.x,
              (H + block.y - 1) / block.y,
              N * CONV1_OUT_CHANNELS);
    conv1_optimized_kernel<<<grid, block, 0, stream>>>(input, output, N, H, W);
}

// Helper function for the first convolution layer optimized with constant memory and loop unrolling
// Expects input of shape [N, CONV1_IN_CHANNELS, H, W] and weight of shape [CONV1_OUT_CHANNELS, CONV1_IN_CHANNELS, 3, 3]
// Bias of shape [CONV1_OUT_CHANNELS].
// Returns output of shape [N, CONV1_OUT_CHANNELS, H, W]

torch::Tensor conv1_optimized(torch::Tensor input, torch::Tensor weight, torch::Tensor bias) {
    int N = input.size(0);
    int H = input.size(2);
    int W = input.size(3);
    auto output = torch::empty({N, CONV1_OUT_CHANNELS, H, W}, input.options());
    
    // Copy weight and bias to constant memory
    cudaMemcpyToSymbol(const_conv1_weights, weight.data_ptr<float>(), CONV1_WEIGHTS_SIZE * sizeof(float));
    cudaMemcpyToSymbol(const_conv1_bias, bias.data_ptr<float>(), CONV1_OUT_CHANNELS * sizeof(float));

    conv1_optimized_launcher(input.data_ptr<float>(), output.data_ptr<float>(), N, H, W);
    cudaDeviceSynchronize();
    return output;
}

// VGG16 forward pass using an optimized first convolution layer that leverages constant memory and loop unrolling.
// The remainder of the network uses standard torch operations to ensure correctness.

torch::Tensor vgg16_forward_cuda(
    torch::Tensor x,
    std::vector<torch::Tensor> conv_weights,
    std::vector<torch::Tensor> conv_biases,
    std::vector<torch::Tensor> fc_weights,
    std::vector<torch::Tensor> fc_biases,
    bool is_training
) {
    auto current = x;
    // Block 1: Use optimized conv for the first layer
    // Assumes conv_weights[0]: [64, 3, 3, 3] and conv_biases[0]: [64]
    current = conv1_optimized(current, conv_weights[0], conv_biases[0]);
    current = torch::relu(current);
    // Continue Block 1 with second conv using torch ops
    current = torch::conv2d(current, conv_weights[1], conv_biases[1], /*stride=*/1, /*padding=*/1);
    current = torch::relu(current);
    current = torch::max_pool2d(current, /*kernel_size=*/2, /*stride=*/2);

    // Block 2
    current = torch::conv2d(current, conv_weights[2], conv_biases[2], /*stride=*/1, /*padding=*/1);
    current = torch::relu(current);
    current = torch::conv2d(current, conv_weights[3], conv_biases[3], /*stride=*/1, /*padding=*/1);
    current = torch::relu(current);
    current = torch::max_pool2d(current, /*kernel_size=*/2, /*stride=*/2);

    // Block 3
    current = torch::conv2d(current, conv_weights[4], conv_biases[4], /*stride=*/1, /*padding=*/1);
    current = torch::relu(current);
    current = torch::conv2d(current, conv_weights[5], conv_biases[5], /*stride=*/1, /*padding=*/1);
    current = torch::relu(current);
    current = torch::conv2d(current, conv_weights[6], conv_biases[6], /*stride=*/1, /*padding=*/1);
    current = torch::relu(current);
    current = torch::max_pool2d(current, /*kernel_size=*/2, /*stride=*/2);

    // Block 4
    current = torch::conv2d(current, conv_weights[7], conv_biases[7], /*stride=*/1, /*padding=*/1);
    current = torch::relu(current);
    current = torch::conv2d(current, conv_weights[8], conv_biases[8], /*stride=*/1, /*padding=*/1);
    current = torch::relu(current);
    current = torch::conv2d(current, conv_weights[9], conv_biases[9], /*stride=*/1, /*padding=*/1);
    current = torch::relu(current);
    current = torch::max_pool2d(current, /*kernel_size=*/2, /*stride=*/2);

    // Block 5
    current = torch::conv2d(current, conv_weights[10], conv_biases[10], /*stride=*/1, /*padding=*/1);
    current = torch::relu(current);
    current = torch::conv2d(current, conv_weights[11], conv_biases[11], /*stride=*/1, /*padding=*/1);
    current = torch::relu(current);
    current = torch::conv2d(current, conv_weights[12], conv_biases[12], /*stride=*/1, /*padding=*/1);
    current = torch::relu(current);
    current = torch::max_pool2d(current, /*kernel_size=*/2, /*stride=*/2);

    // Classifier
    current = current.flatten(1);
    current = torch::linear(current, fc_weights[0], fc_biases[0]);
    current = torch::relu(current);
    if (is_training) {
        current = torch::dropout(current, /*p=*/0.0, /*train=*/true);
    }
    current = torch::linear(current, fc_weights[1], fc_biases[1]);
    current = torch::relu(current);
    if (is_training) {
        current = torch::dropout(current, /*p=*/0.0, /*train=*/true);
    }
    current = torch::linear(current, fc_weights[2], fc_biases[2]);

    return current;
}

PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
    m.def(""forward"", &vgg16_forward_cuda, ""VGG16 forward with optimized constant memory and unrolling (CUDA)"");
}
",other,,,,,1,0,0.20451656,9114
"#include <torch/extension.h>
#include <ATen/ATen.h>
#include <cuda.h>
#include <cuda_runtime.h>
#include <math.h>

// Kernel 1: Fuse tanh activation with max pooling
// Instead of applying tanh for every element then pooling, we use the monotonicity of tanh
// to compute max pooling on the raw input then apply tanh once. This kernel ensures that global
// memory accesses are coalesced by mapping each output element (n, c, h_out, w_out) to a thread.

__global__ void fused_tanh_maxpool_kernel(
    const float* __restrict__ input,  // input tensor of shape [N, C, H, W]
    float* __restrict__ output,       // output tensor of shape [N, C, H/2, W/2]
    int N, int C, int H, int W,
    int H_out, int W_out
) {
    int index = blockIdx.x * blockDim.x + threadIdx.x;
    int total = N * C * H_out * W_out;
    if (index >= total) return;

    // Decode flattened index into (n, c, h_out, w_out)
    int w_out = index % W_out;
    int tmp = index / W_out;
    int h_out = tmp % H_out;
    tmp = tmp / H_out;
    int c = tmp % C;
    int n = tmp / C;

    // Compute corresponding top-left index for the 2x2 pooling window in input
    int h_in = h_out * 2;
    int w_in = w_out * 2;

    // Compute base offset for input element [n, c, 0, 0]
    int channel_stride = H * W;
    int n_stride = C * channel_stride;
    int base = n * n_stride + c * channel_stride;

    // Read the 2x2 window elements (assume H and W are even)
    float a = input[base + h_in * W + w_in];
    float b = input[base + h_in * W + (w_in + 1)];
    float c_val = input[base + (h_in + 1) * W + w_in];
    float d = input[base + (h_in + 1) * W + (w_in + 1)];

    // Compute max of the window
    float max_val = fmaxf(fmaxf(a, b), fmaxf(c_val, d));
    // Apply tanh activation on the pooled max
    float activated = tanhf(max_val);

    output[index] = activated;
}

// Kernel 2: Fuse group normalization over the pooled output
// Each block processes one (n, group) pair. Threads cooperatively compute the mean and variance
// for the group's elements and then apply normalization with the provided gamma (weight) and beta (bias).

__global__ void fused_group_norm_kernel(
    float* __restrict__ data, // in/out tensor of shape [N, C, H_out, W_out]
    int N, int C, int H_out, int W_out,
    const float* __restrict__ gamma, // scale, shape [C]
    const float* __restrict__ beta,  // bias, shape [C]
    int num_groups,
    float eps
) {
    // Identify sample n and group g from blockIdx.x (one block per (n, g))
    int blockId = blockIdx.x; // total blocks = N * num_groups
    int n = blockId / num_groups;
    int g = blockId % num_groups;
    int channels_per_group = C / num_groups;
    int group_channel_start = g * channels_per_group;
    int num_elements = channels_per_group * H_out * W_out;

    // Allocate shared memory for block-level reduction: two arrays for sum and sum of squares
    extern __shared__ float shared_data[]; // size = 2 * blockDim.x * sizeof(float)
    float* s_sum = shared_data;
    float* s_sum_sq = &shared_data[blockDim.x];

    float local_sum = 0.0f;
    float local_sum_sq = 0.0f;

    // Each thread processes a chunk of the group's elements
    for (int idx = threadIdx.x; idx < num_elements; idx += blockDim.x) {
        // Decode idx into (channel offset, h, w) within the group
        int w = idx % W_out;
        int temp = idx / W_out;
        int h = temp % H_out;
        int ch_offset = temp / H_out;  // channel within the group
        int c = group_channel_start + ch_offset;

        int global_index = n * (C * H_out * W_out) + c * (H_out * W_out) + h * W_out + w;
        float val = data[global_index];
        local_sum += val;
        local_sum_sq += val * val;
    }
    s_sum[threadIdx.x] = local_sum;
    s_sum_sq[threadIdx.x] = local_sum_sq;
    __syncthreads();

    // Perform reduction in shared memory
    for (int stride = blockDim.x / 2; stride > 0; stride /= 2) {
        if (threadIdx.x < stride) {
            s_sum[threadIdx.x] += s_sum[threadIdx.x + stride];
            s_sum_sq[threadIdx.x] += s_sum_sq[threadIdx.x + stride];
        }
        __syncthreads();
    }

    // Compute mean and variance for the group
    float mean = s_sum[0] / num_elements;
    float var = s_sum_sq[0] / num_elements - mean * mean;
    float inv_std = rsqrtf(var + eps);

    // Apply group normalization for each element in the group
    for (int idx = threadIdx.x; idx < num_elements; idx += blockDim.x) {
        int w = idx % W_out;
        int temp = idx / W_out;
        int h = temp % H_out;
        int ch_offset = temp / H_out;
        int c = group_channel_start + ch_offset;

        int global_index = n * (C * H_out * W_out) + c * (H_out * W_out) + h * W_out + w;
        float val = data[global_index];
        float norm_val = (val - mean) * inv_std;
        norm_val = gamma[c] * norm_val + beta[c];
        data[global_index] = norm_val;
    }
}

// Forward function: applies ConvTranspose2d and BatchNorm using ATen kernels,
// then fuses tanh activation with max pooling and group normalization using custom CUDA kernels.

at::Tensor forward(
    at::Tensor x,
    int64_t stride,
    int64_t padding,
    at::Tensor conv_transpose,
    at::Tensor conv_transpose_bias,
    at::Tensor batch_norm_weight,
    at::Tensor batch_norm_bias,
    at::Tensor batch_norm_running_mean,
    at::Tensor batch_norm_running_var,
    at::Tensor group_norm_weight,
    at::Tensor group_norm_bias,
    int64_t num_groups
) {
    // Step 1: Transposed Convolution (handled by highly-optimized ATen kernel)
    x = at::conv_transpose2d(
            x,
            conv_transpose,
            conv_transpose_bias,
            {stride, stride},
            {padding, padding}
        );

    // Step 2: Batch Normalization (handled by ATen kernel)
    x = at::batch_norm(
            x,
            batch_norm_weight,
            batch_norm_bias,
            batch_norm_running_mean,
            batch_norm_running_var,
            /*training=*/true,
            /*momentum=*/0.1,
            /*eps=*/1e-5,
            /*cudnn_enabled=*/true
        );

    // Get dimensions after conv transpose and batch norm, input is assumed to be in NCHW order
    int N = x.size(0);
    int C = x.size(1);
    int H = x.size(2);
    int W = x.size(3);

    // For max pooling with kernel size 2 and stride 2, output dimensions
    int H_out = H / 2;
    int W_out = W / 2;

    // Allocate tensor for the fused tanh + max pooling + group norm output
    auto pooled = at::empty({N, C, H_out, W_out}, x.options());

    // Launch Kernel 1: Fused Tanh and Max Pooling
    int total_elements = N * C * H_out * W_out;
    int threads = 256;
    int blocks = (total_elements + threads - 1) / threads;

    fused_tanh_maxpool_kernel<<<blocks, threads>>>(
        x.data_ptr<float>(),
        pooled.data_ptr<float>(),
        N, C, H, W,
        H_out, W_out
    );
    cudaDeviceSynchronize();

    // Launch Kernel 2: Fused Group Normalization
    // We launch one block per sample and group (gridDim.x = N * num_groups)
    int gn_blocks = N * num_groups;
    int gn_threads = 256;
    size_t shared_mem_size = 2 * gn_threads * sizeof(float); // For reduction

    fused_group_norm_kernel<<<gn_blocks, gn_threads, shared_mem_size>>>(
        pooled.data_ptr<float>(),
        N, C, H_out, W_out,
        group_norm_weight.data_ptr<float>(),
        group_norm_bias.data_ptr<float>(),
        num_groups, 1e-5
    );
    cudaDeviceSynchronize();

    return pooled;
}

PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
    m.def(""forward"", &forward, ""Fused 11_ConvTranspose2d_BatchNorm_Tanh_MaxPool_GroupNorm with memory coalescing"");
}
",other,,,,,1,0,0.20500107,7659
"#include <torch/extension.h>
#include <ATen/cuda/CUDAContext.h>
#include <cuda.h>
#include <cuda_runtime.h>
#include <pybind11/pybind11.h>
#include <pybind11/stl.h>

// Optimized depthwise 2D convolution kernel using atomic operations only where necessary.
template <typename scalar_t>
__global__ void depthwiseConv2DKernelOptimized(
    const scalar_t* __restrict__ x,
    const scalar_t* __restrict__ w,
    const scalar_t* __restrict__ b,
    scalar_t* __restrict__ out,
    const int batch_size,
    const int in_channels,
    const int in_height,
    const int in_width,
    const int kernel_size,
    const int out_height,
    const int out_width,
    const int stride,
    const int padding)
{
    __shared__ scalar_t shared_out[256];
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    int total = batch_size * in_channels * out_height * out_width;
    if (idx >= total) {
        return;
    }

    // Decompose idx into (n, c, h_out, w_out).
    int w_out_idx = idx % out_width;
    int tmp = idx / out_width;
    int h_out_idx = tmp % out_height;
    tmp /= out_height;
    int c = tmp % in_channels;
    int n = tmp / in_channels;

    // Initialize shared memory for this block
    int shared_idx = threadIdx.x;
    shared_out[shared_idx] = 0;
    __syncthreads();

    // Accumulate over the kernel.
    scalar_t value = 0;
    for (int kh = 0; kh < kernel_size; kh++) {
        for (int kw = 0; kw < kernel_size; kw++) {
            int h_in = h_out_idx * stride - padding + kh;
            int w_in = w_out_idx * stride - padding + kw;
            // Boundary check.
            if (h_in >= 0 && h_in < in_height && w_in >= 0 && w_in < in_width) {
                int x_index = ((n * in_channels + c) * in_height + h_in) * in_width + w_in;
                int w_index = ((c * 1 + 0) * kernel_size + kh) * kernel_size + kw;
                value += x[x_index] * w[w_index];
            }
        }
    }
    // Add bias for this channel.
    value += b[c];

    // Use atomic operation to update shared memory
    atomicAdd(&shared_out[shared_idx], value);
    __syncthreads();

    // Write to output from shared memory
    if (threadIdx.x == 0) {
        for (int i = 0; i < blockDim.x; i++) {
            atomicAdd(&out[idx - threadIdx.x + i], shared_out[i]);
        }
    }
}

// The actual implementation of depthwise Conv2D in CUDA.
torch::Tensor forward_impl(
    torch::Tensor x,
    torch::Tensor weight,
    torch::Tensor bias,
    int stride,
    int padding,
    int groups)
{
    // For depthwise conv: groups == in_channels typically.
    // Compute output dimensions.
    const int batch_size = x.size(0);
    const int in_channels = x.size(1);
    const int in_height = x.size(2);
    const int in_width = x.size(3);

    const int kernel_size = weight.size(2);  // weight is (in_channels, 1, K, K)
    // Output height/width formula for convolution.
    const int out_height = (in_height + 2 * padding - kernel_size) / stride + 1;
    const int out_width  = (in_width  + 2 * padding - kernel_size) / stride + 1;

    // Create output tensor.
    auto out = torch::empty({batch_size, in_channels, out_height, out_width}, x.options());

    // Launch kernel.
    const int total = batch_size * in_channels * out_height * out_width;
    const int threads = 256;
    const int blocks = (total + threads - 1) / threads;

    AT_DISPATCH_FLOATING_TYPES(x.scalar_type(), ""depthwise_conv2d_forward_optimized"", ([&] {
        depthwiseConv2DKernelOptimized<scalar_t><<<blocks, threads, threads * sizeof(scalar_t)>>>(
            x.data_ptr<scalar_t>(),
            weight.data_ptr<scalar_t>(),
            bias.data_ptr<scalar_t>(),
            out.data_ptr<scalar_t>(),
            batch_size, in_channels, in_height, in_width,
            kernel_size, out_height, out_width,
            stride, padding
        );
    }));

    return out;
}

namespace py = pybind11;

// We wrap forward_impl to allow passing None as bias from Python.
// If bias is None, we create a zero-initialized bias tensor.
torch::Tensor forward_wrap(
    torch::Tensor x,
    torch::Tensor weight,
    py::object bias_obj,
    int stride,
    int padding,
    int groups)
{
    // If Python code passes ""None"" for bias, make a zero bias.
    torch::Tensor bias;
    if (bias_obj.is_none()) {
        bias = torch::zeros({x.size(1)}, x.options());
    } else {
        bias = bias_obj.cast<torch::Tensor>();
    }
    return forward_impl(x, weight, bias, stride, padding, groups);
}

// Bind to PyTorch.
PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
    m.def(
        ""forward"",
        &forward_wrap,
        ""Depthwise conv2d forward optimized (handles optional bias)"",
        py::arg(""x""),
        py::arg(""weight""),
        py::arg(""bias"") = py::none(),
        py::arg(""stride"") = 1,
        py::arg(""padding"") = 0,
        py::arg(""groups"") = 1
    );
}",conv2d,,,,,1,0,0.20579097,4863
